{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f2bedf81edd438db1e6c9a2aa41f232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_433c50d1f40442e39ab4e89756633ed2",
              "IPY_MODEL_f7abca3d5824472e8efe63668cee5f8f",
              "IPY_MODEL_ec133885fe2b419286ae609f52efbf59",
              "IPY_MODEL_6b0f7d81472b4eaabb0325861c3e17d8",
              "IPY_MODEL_29495f23ed974a9a9f54c06aee42708e"
            ],
            "layout": "IPY_MODEL_b526e765ebfa410184392667c4ff7262"
          }
        },
        "433c50d1f40442e39ab4e89756633ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6950c1233bb4bb588ccc86961e2e907",
            "placeholder": "​",
            "style": "IPY_MODEL_bf355ee0edf24dc18831d76ac9d39842",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "f7abca3d5824472e8efe63668cee5f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9b1ecd205f02453b9a66535130d66185",
            "placeholder": "​",
            "style": "IPY_MODEL_35e12a72c3d140aab8a45a289db88155",
            "value": ""
          }
        },
        "ec133885fe2b419286ae609f52efbf59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_4d55d82c57c445739214a38419f638a5",
            "style": "IPY_MODEL_9ffd8b1684444acd94294959214c3c28",
            "value": true
          }
        },
        "6b0f7d81472b4eaabb0325861c3e17d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_12b14e88bc09472897076dd6b8b62945",
            "style": "IPY_MODEL_81941dffdc6e4c90b77fc6cb6506a626",
            "tooltip": ""
          }
        },
        "29495f23ed974a9a9f54c06aee42708e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9f5552a9c0e42b09c96cb11bbad8b35",
            "placeholder": "​",
            "style": "IPY_MODEL_9bda68f535524c5997e0d11d81fae635",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b526e765ebfa410184392667c4ff7262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "a6950c1233bb4bb588ccc86961e2e907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf355ee0edf24dc18831d76ac9d39842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b1ecd205f02453b9a66535130d66185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e12a72c3d140aab8a45a289db88155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d55d82c57c445739214a38419f638a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ffd8b1684444acd94294959214c3c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12b14e88bc09472897076dd6b8b62945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81941dffdc6e4c90b77fc6cb6506a626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d9f5552a9c0e42b09c96cb11bbad8b35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bda68f535524c5997e0d11d81fae635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajveer43/medusa_task/blob/master/Medusa_1__Preparing_GGUF_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task is to implement a FastAPI service that serves a Language Model (LLM) with a [medusa](https://github.com/FasterDecoding/Medusa) head (using `lmsys/vicuna-7b`). The goal is to optimize the inference speed using a model compilation library (e.g., `llama.cpp`) and enhance performance via speculative decoding with the medusa head. Additionally, you are required to implement dynamic batching to handle multiple concurrent requests efficiently.\n",
        "\n",
        "### **Key Deliverables:**\n",
        "\n",
        "1. **Model Compilation:**\n",
        "    - Use a model compilation library (e.g., [llama.cpp](https://github.com/ggerganov/llama.cpp)) to optimize the inference of the base model.\n",
        "    - Provide an explanation of your choice of compilation library and its impact on performance.\n",
        "2. **Medusa Head Implementation:**\n",
        "    - Implement the medusa head on top of the base model to improve performance via speculative decoding. Avoid using existing implementations.\n",
        "    - Include a brief explanation of how speculative decoding is implemented and its advantages.\n",
        "3. **Dynamic Batching:**\n",
        "    - Implement dynamic batching to efficiently manage multiple concurrent requests.\n",
        "    - Explain your approach to dynamic batching and its benefits in serving LLMs.\n",
        "4. **Service Implementation:**\n",
        "    - Use [FastAPI](https://fastapi.tiangolo.com/) to create a service that serves the LLM with the medusa head.\n",
        "    - Ensure the service can handle concurrent requests with low latency.\n",
        "5. **Testing & Validation:**\n",
        "    - Provide test cases to validate the correctness and efficiency of your implementation.\n",
        "    - Include performance benchmarks or metrics comparing different configurations (e.g., with and without the medusa head, with and without dynamic batching).\n",
        "\n",
        "### **Grading Criteria:**\n",
        "\n",
        "1. **Correctness (40%):**\n",
        "    - Functional service that correctly serves the LLM.\n",
        "    - Proper implementation of the medusa head with enhanced performance.\n",
        "2. **Optimization & Performance (30%):**\n",
        "    - Effective use of the model compilation library for inference optimization.\n",
        "    - Performance improvement through speculative decoding with the medusa head.\n",
        "    - Efficient handling of requests with dynamic batching.\n",
        "3. **Code Quality & Documentation (20%):**\n",
        "    - Clean, readable, and maintainable code.\n",
        "    - Clear and concise documentation explaining implementation choices.\n",
        "4. **Testing & Validation (10%):**\n",
        "    - Comprehensive test cases covering key functionalities.\n",
        "    - Inclusion of performance benchmarks or metrics to demonstrate optimizations.\n",
        "\n",
        "### **Partial Credit:**\n",
        "\n",
        "Partial implementations will still be evaluated based on relevant criteria. For instance:\n",
        "\n",
        "- **Model Optimization Only:** Focusing on base model optimization without medusa head or dynamic batching.\n",
        "- **Medusa Head Implementation:** Implementing speculative decoding without dynamic batching.\n",
        "- **Dynamic Batching:** Focusing on request handling efficiency without medusa head.\n",
        "\n",
        "---\n",
        "\n",
        "### **Additional Notes:**\n",
        "\n",
        "- **Free GPU Access:** If you need access to GPUs, consider using services like [Google Colab](https://colab.research.google.com/) or [Kaggle Notebooks](https://www.kaggle.com/kernels), which provide free access to GPU resources.\n",
        "- **Submission:** Please submit your code, along with a brief report (Markdown or PDF) explaining your implementation, testing, and any performance metrics.\n",
        "\n",
        "This assignment is designed to test your understanding of model optimization, complex inference strategies, and the ability to build scalable services. Partial implementations are welcome and will be graded accordingly."
      ],
      "metadata": {
        "id": "ZLqPkodx8i3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Medusa Paper (2024) - Brief Summary**\n",
        "\n",
        "The *Medusa* paper introduces a novel framework to enhance the decoding speed of large language models (LLMs) without compromising output quality. Traditionally, decoding (predicting the next token step-by-step) is the bottleneck in LLMs. Medusa tackles this by allowing the model to predict **multiple future tokens in parallel**, not just one at a time.\n",
        "\n",
        "It does this by attaching lightweight \"Medusa heads\" (small prediction modules) on top of the base model. These heads jointly predict the next **M** tokens in one shot. A verification step then checks these predicted tokens against the base model to ensure correctness. If they pass, they are accepted; if not, the model falls back to standard autoregressive decoding.\n",
        "\n",
        "In simple terms:  \n",
        "Medusa is like giving the model a shortcut to \"guess\" multiple words ahead, and then double-checking its guesses, resulting in **2x to 2.5x decoding speed-up** while maintaining similar accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Speculative Decoding - Brief Explanation**\n",
        "\n",
        "*Speculative Decoding* is a general strategy to speed up autoregressive generation (token-by-token prediction) in LLMs. Instead of predicting one token at a time, speculative decoding generates **a batch of future tokens** in parallel using a smaller, faster model (called the draft model).\n",
        "\n",
        "Here's how it works:\n",
        "1. The draft model predicts several tokens ahead quickly.\n",
        "2. The main (larger and more accurate) model then verifies or adjusts these tokens.\n",
        "3. Validated tokens are accepted directly; incorrect ones trigger normal token-by-token generation.\n",
        "\n",
        "By letting a lightweight model \"speculate\" multiple tokens and only asking the big model to verify, this method can significantly **reduce the number of expensive forward passes** through the large model."
      ],
      "metadata": {
        "id": "dklc29DOqoyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary libraries"
      ],
      "metadata": {
        "id": "EE_-hYiqr__R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mbw1dd6NjtG",
        "outputId": "e577cabe-acb9-43b1-8b06-145fded5b267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,892 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,737 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,684 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,784 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,045 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,049 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 29.9 MB in 6s (4,855 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt update && apt install -y cmake git curl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok  # Required to expose FastAPI publicly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIUWJz1XNkWO",
        "outputId": "0f962a8e-ac01-4dc1-d109-b2d4db2e87a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn transformers huggingface_hub torch accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWgkJ-gENpKQ",
        "outputId": "c24832fe-8967-4438-c4a6-cdddc678a098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastapi\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fastapi-0.115.12 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 starlette-0.46.1 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FasterDecoding/Medusa.git\n",
        "%cd Medusa\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tun5_m6PuP0T",
        "outputId": "c00555e0-002a-43aa-f916-44c194c718e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Medusa'...\n",
            "remote: Enumerating objects: 353, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 353 (delta 118), reused 99 (delta 99), pack-reused 191 (from 1)\u001b[K\n",
            "Receiving objects: 100% (353/353), 4.89 MiB | 22.85 MiB/s, done.\n",
            "Resolving deltas: 100% (193/193), done.\n",
            "/content/Medusa\n",
            "Obtaining file:///content/Medusa\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fschat (from medusa-llm==1.0)\n",
            "  Downloading fschat-0.2.36-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medusa-llm==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.34 in /usr/local/lib/python3.11/dist-packages (from medusa-llm==1.0) (4.50.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from medusa-llm==1.0) (1.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from medusa-llm==1.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from medusa-llm==1.0) (5.29.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34->medusa-llm==1.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->medusa-llm==1.0) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medusa-llm==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medusa-llm==1.0) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from fschat->medusa-llm==1.0) (3.11.14)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from fschat->medusa-llm==1.0) (0.115.12)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from fschat->medusa-llm==1.0) (0.28.1)\n",
            "Collecting markdown2[all] (from fschat->medusa-llm==1.0)\n",
            "  Downloading markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting nh3 (from fschat->medusa-llm==1.0)\n",
            "  Downloading nh3-0.2.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from fschat->medusa-llm==1.0) (3.0.50)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from fschat->medusa-llm==1.0) (2.10.6)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from fschat->medusa-llm==1.0) (13.9.4)\n",
            "Collecting shortuuid (from fschat->medusa-llm==1.0)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tiktoken (from fschat->medusa-llm==1.0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from fschat->medusa-llm==1.0) (0.34.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit>=3.0.0->fschat->medusa-llm==1.0) (0.2.13)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.0.0->fschat->medusa-llm==1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.0.0->fschat->medusa-llm==1.0) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat->medusa-llm==1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat->medusa-llm==1.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat->medusa-llm==1.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat->medusa-llm==1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat->medusa-llm==1.0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat->medusa-llm==1.0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat->medusa-llm==1.0) (1.18.3)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->fschat->medusa-llm==1.0) (0.46.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->fschat->medusa-llm==1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->fschat->medusa-llm==1.0) (2.27.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->fschat->medusa-llm==1.0) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->fschat->medusa-llm==1.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->fschat->medusa-llm==1.0) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->fschat->medusa-llm==1.0) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->fschat->medusa-llm==1.0) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medusa-llm==1.0) (3.0.2)\n",
            "Collecting wavedrom (from markdown2[all]->fschat->medusa-llm==1.0)\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting latex2mathml (from markdown2[all]->fschat->medusa-llm==1.0)\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.34->medusa-llm==1.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.34->medusa-llm==1.0) (2.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->fschat->medusa-llm==1.0) (8.1.8)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat->medusa-llm==1.0) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->fschat->medusa-llm==1.0) (1.3.1)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->fschat->medusa-llm==1.0)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from wavedrom->markdown2[all]->fschat->medusa-llm==1.0) (1.17.0)\n",
            "Downloading fschat-0.2.36-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nh3-0.2.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.0/739.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: medusa-llm, wavedrom\n",
            "  Building editable for medusa-llm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medusa-llm: filename=medusa_llm-1.0-0.editable-py3-none-any.whl size=12218 sha256=3927ec0cd197ef39f654e436dc50525a64f04e19b9cc56487647adb7ef6e3e3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wx2haq1l/wheels/33/31/cd/62bad3b58737812646779fb1d5827debf04c7f9a46527776d4\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30082 sha256=ad4be84bb04135bd16a70b6690d6b0cffde73ffb6693e10b65acd5e76a1a4ae5\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/3b/4dcf6b22fa41c5ece715fa5f4e05afd683e7b0ce0f2fcc7bb6\n",
            "Successfully built medusa-llm wavedrom\n",
            "Installing collected packages: svgwrite, shortuuid, nh3, markdown2, latex2mathml, wavedrom, tiktoken, fschat, medusa-llm\n",
            "Successfully installed fschat-0.2.36 latex2mathml-3.77.0 markdown2-2.5.3 medusa-llm-1.0 nh3-0.2.21 shortuuid-1.0.13 svgwrite-1.4.3 tiktoken-0.9.0 wavedrom-2.0.3.post3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_pvgGlUvCTo",
        "outputId": "058d0671-4da2-490f-d7ac-95dd57965c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok sentencepiece\n",
        "# !pip install medusa-llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVhj8n8qhAtY",
        "outputId": "3c192740-e7f2-4e3f-ec4a-4978f3e257fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "%cd llama.cpp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8ta3f63NtvU",
        "outputId": "302f7a3a-e001-46cd-e270-13a38c39a176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 47216, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 47216 (delta 119), reused 66 (delta 64), pack-reused 47062 (from 4)\u001b[K\n",
            "Receiving objects: 100% (47216/47216), 99.50 MiB | 12.52 MiB/s, done.\n",
            "Resolving deltas: 100% (33891/33891), done.\n",
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build LlamaCPP"
      ],
      "metadata": {
        "id": "wZEfwGXPsIx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -B build\n",
        "!cmake --build build --config Release -j 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEy6V_IhvXAt",
        "outputId": "1c22bce5-a58b-436a-cc4b-1bee48d696d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Configuring done (4.1s)\n",
            "-- Generating done (0.6s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  3%] Built target ggml-base\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[  8%] Built target ggml-cpu\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[  9%] Built target ggml\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 19%] Built target llama\n",
            "[ 19%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[ 19%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 19%] Built target build_info\n",
            "[ 20%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 25%] Built target common\n",
            "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 26%] Built target test-tokenizer-0\n",
            "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 27%] Built target test-sampling\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 29%] Built target test-grammar-parser\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 30%] Built target test-grammar-integration\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 32%] Built target test-llama-grammar\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 33%] Built target test-chat\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 34%] Built target test-json-schema-to-grammar\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 35%] Built target test-tokenizer-1-bpe\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 36%] Built target test-tokenizer-1-spm\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 37%] Built target test-log\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 39%] Built target test-arg-parser\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 41%] Built target test-chat-template\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 42%] Built target test-gguf\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 43%] Built target test-backend-ops\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 45%] Built target test-model-load-cancel\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 46%] Built target test-autorelease\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 48%] Built target test-barrier\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 49%] Built target test-quantize-fns\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 50%] Built target test-quantize-perf\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 52%] Built target test-rope\n",
            "[ 52%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 53%] Built target test-c\n",
            "[ 54%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 54%] Built target llama-batched-bench\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 55%] Built target llama-batched\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 56%] Built target llama-embedding\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 57%] Built target llama-eval-callback\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
            "[ 58%] Built target llama-gbnf-validator\n",
            "[ 58%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 58%] Built target sha256\n",
            "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 59%] Built target xxhash\n",
            "[ 60%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 60%] Built target sha1\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 61%] Built target llama-gguf-hash\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 62%] Built target llama-gguf-split\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 63%] Built target llama-gguf\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 64%] Built target llama-gritlm\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 64%] Built target llama-imatrix\n",
            "[ 65%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
            "[ 65%] Built target llama-infill\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 66%] Built target llama-bench\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 67%] Built target llama-lookahead\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 68%] Built target llama-lookup\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 69%] Built target llama-lookup-create\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 70%] Built target llama-lookup-merge\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 71%] Built target llama-lookup-stats\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 72%] Built target llama-cli\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 73%] Built target llama-parallel\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 74%] Built target llama-passkey\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 75%] Built target llama-perplexity\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 76%] Built target llama-quantize\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 77%] Built target llama-retrieval\n",
            "[ 78%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 78%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 79%] Built target llama-server\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 80%] Built target llama-save-load-state\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 81%] Built target llama-run\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 82%] Built target llama-simple\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 83%] Built target llama-simple-chat\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 84%] Built target llama-speculative\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 85%] Built target llama-speculative-simple\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 85%] Built target llama-tokenize\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 86%] Built target llama-tts\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 87%] Built target llama-gen-docs\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 88%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 89%] Built target llama-cvector-generator\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 90%] Built target llama-export-lora\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
            "[ 91%] Built target llama-quantize-stats\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 92%] Built target llava\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 92%] Built target llava_static\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
            "[ 93%] Built target llava_shared\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 94%] Built target llama-llava-cli\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 95%] Built target llama-minicpmv-cli\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 96%] Built target llama-qwen2vl-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 97%] Built target llama-gemma3-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
            "[ 98%] Built target llama-llava-clip-quantize-cli\n",
            "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 99%] Built target llama-vdot\n",
            "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[100%] Built target llama-q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "4f2bedf81edd438db1e6c9a2aa41f232",
            "433c50d1f40442e39ab4e89756633ed2",
            "f7abca3d5824472e8efe63668cee5f8f",
            "ec133885fe2b419286ae609f52efbf59",
            "6b0f7d81472b4eaabb0325861c3e17d8",
            "29495f23ed974a9a9f54c06aee42708e",
            "b526e765ebfa410184392667c4ff7262",
            "a6950c1233bb4bb588ccc86961e2e907",
            "bf355ee0edf24dc18831d76ac9d39842",
            "9b1ecd205f02453b9a66535130d66185",
            "35e12a72c3d140aab8a45a289db88155",
            "4d55d82c57c445739214a38419f638a5",
            "9ffd8b1684444acd94294959214c3c28",
            "12b14e88bc09472897076dd6b8b62945",
            "81941dffdc6e4c90b77fc6cb6506a626",
            "d9f5552a9c0e42b09c96cb11bbad8b35",
            "9bda68f535524c5997e0d11d81fae635"
          ]
        },
        "id": "qY2ULRBMOM0P",
        "outputId": "b70a9f57-e639-41f6-ceaf-a8e19faa7d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f2bedf81edd438db1e6c9a2aa41f232"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clone Model form HF"
      ],
      "metadata": {
        "id": "N8f2WIDRsOOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/lmsys/vicuna-7b-v1.3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jWhQbFmOWQ2",
        "outputId": "82362f26-5abf-4d5f-9c1f-bb2eb65ac9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vicuna-7b-v1.3'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Total 43 (delta 0), reused 0 (delta 0), pack-reused 43 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (43/43), 6.81 KiB | 435.00 KiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.55 GiB | 5.54 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && mkdir optimized_model"
      ],
      "metadata": {
        "id": "MM--8KqQ9pqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Per5ErHq90UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert to GGUF"
      ],
      "metadata": {
        "id": "rDGr7YKBsRQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert_hf_to_gguf.py vicuna-7b-v1.3 --outtype f16 --outfile ../optimized_model/vicuna-7b-v1.3-F16.gguf\n"
      ],
      "metadata": {
        "id": "bJq8yYSVOdDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9867498-87ea-4041-c30e-f587e21372e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: vicuna-7b-v1.3\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 32000}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 32000}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 2048\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:../optimized_model/vicuna-7b-v1.3-F16.gguf: n_tensors = 291, total_size = 13.5G\n",
            "Writing: 100% 13.5G/13.5G [02:13<00:00, 101Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to ../optimized_model/vicuna-7b-v1.3-F16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .."
      ],
      "metadata": {
        "id": "z4Jz2bFi_2Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qauntize using llama CPP"
      ],
      "metadata": {
        "id": "BJvYQpZasXKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/llama.cpp/build/bin && ./llama-quantize /content/optimized_model/vicuna-7b-v1.3-F16.gguf /content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf q4_K_M"
      ],
      "metadata": {
        "id": "TA_jcLwug-Yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf3c2de-b3d6-40e1-b83b-21f36f411e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 4974 (029c693f)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/optimized_model/vicuna-7b-v1.3-F16.gguf' to '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /content/optimized_model/vicuna-7b-v1.3-F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Vicuna 7b v1.3\n",
            "llama_model_loader: - kv   3:                            general.version str              = v1.3\n",
            "llama_model_loader: - kv   4:                           general.basename str              = vicuna\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   7:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_K .. size =   250.00 MiB ->    70.31 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "llama_model_quantize_impl: model size  = 12853.02 MB\n",
            "llama_model_quantize_impl: quant size  =  3891.24 MB\n",
            "\n",
            "main: quantize time = 880361.91 ms\n",
            "main:    total time = 880361.91 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Model Size Reduction: From 14GB to 4GB with GGUF Conversion\n",
        "\n",
        "The dramatic reduction in model size from 14GB to 4GB through GGUF conversion represents a significant optimization that's worth understanding in detail.\n",
        "\n",
        "## Original Model Format vs GGUF\n",
        "\n",
        "The original 14GB model was likely stored in one of these formats:\n",
        "- **PyTorch format** (.pt/.pth) - Typically uses FP16 (16-bit) or FP32 (32-bit) floating point precision\n",
        "- **Hugging Face format** - Similar to PyTorch, using full precision weights\n",
        "- **Safetensors format** - A safer alternative to PyTorch's pickle-based format\n",
        "\n",
        "These formats prioritize accuracy over size, storing model weights in high-precision floating-point format.\n",
        "\n",
        "## The GGUF Format\n",
        "\n",
        "GGUF (GPT-Generated Unified Format) is the successor to GGML, designed specifically for efficient inference of large language models. Key aspects:\n",
        "\n",
        "- **Improved architecture**: Better organized metadata and weight layout compared to the older GGML format\n",
        "- **Self-contained**: Includes tokenizer data, model parameters, and quantization information\n",
        "- **Optimized memory layout**: Designed for faster loading and reduced memory fragmentation\n",
        "- **Cross-platform compatibility**: Works across different hardware architectures\n",
        "\n",
        "## Quantization Process\n",
        "\n",
        "The size reduction from 14GB to 4GB (approximately 71% reduction) was achieved through:\n",
        "\n",
        "1. **Precision reduction**: Converting from FP16/FP32 to a more compressed numerical representation\n",
        "\n",
        "2. **Weight quantization**: The model likely used one of these quantization methods:\n",
        "   - **8-bit quantization** (Q8_0): Each weight stored in 8 bits instead of 16/32 bits\n",
        "   - **4-bit quantization** (Q4_K_M): Extremely compressed format using just 4 bits per weight\n",
        "   - **Mixed precision**: Some layers kept at higher precision (critical layers) while others use lower precision\n",
        "\n",
        "3. **KV quantization**: Quantized key-value cache for more efficient inference\n",
        "\n",
        "## Technical Implementation\n",
        "\n",
        "\n",
        "The \"F16_KM\" in your filename indicates:\n",
        "- **F16**: Base precision is Float16\n",
        "- **K**: K-quant method used (blockwise quantization)\n",
        "- **M**: Mixed precision approach\n",
        "\n",
        "## Performance Implications\n",
        "\n",
        "This 4GB GGUF model offers significant advantages:\n",
        "\n",
        "1. **Memory efficiency**: Runs on consumer hardware with limited VRAM\n",
        "2. **Loading speed**: Smaller file loads faster into memory\n",
        "3. **Inference performance**: Often 2-4x faster than full-precision models\n",
        "4. **Disk space**: 71% reduction in storage requirements\n",
        "\n",
        "The trade-off is typically a very small reduction in output quality that's barely noticeable in most applications. Modern quantization techniques like K-quant have minimized this quality loss significantly compared to earlier methods.\n",
        "\n",
        "This optimization is particularly valuable for deployment in resource-constrained environments like edge devices, consumer GPUs, or when serving multiple model instances concurrently.\n"
      ],
      "metadata": {
        "id": "ITjTwzCb7z8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama.cpp/vicuna-7b-v1.3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G_zX7LThF4G",
        "outputId": "33fd218e-74ce-43a9-b3ba-f0f58e34f79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/vicuna-7b-v1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install python helper for llama cpp"
      ],
      "metadata": {
        "id": "mDbo1A4zscoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python==0.2.85"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MUE7jF_MZK5",
        "outputId": "e3c70327-bd45-4e53-8a7e-08a886790887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python==0.2.85 in /usr/local/lib/python3.11/dist-packages (0.2.85)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.85) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.85) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.85) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.85) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.85) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "QgLsYyKRM5Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf\""
      ],
      "metadata": {
        "id": "K4u04_z7NXKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load model for interfence testing"
      ],
      "metadata": {
        "id": "CMbGjAN6siQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path=model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJXnBsnwNn2c",
        "outputId": "47eeb6c2-3806-40a8-adc8-908b824e044e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Vicuna 7b v1.3\n",
            "llama_model_loader: - kv   3:                            general.version str              = v1.3\n",
            "llama_model_loader: - kv   4:                           general.basename str              = vicuna\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   7:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  12:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  13:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  25:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = Vicuna 7b v1.3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.block_count': '32', 'tokenizer.ggml.padding_token_id': '0', 'general.basename': 'vicuna', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '2048', 'general.name': 'Vicuna 7b v1.3', 'llama.rope.dimension_count': '128', 'general.version': 'v1.3', 'general.type': 'model', 'general.size_label': '7B', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.vocab_size': '32000'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### inference test 1"
      ],
      "metadata": {
        "id": "GB7dA3bosnpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_kwargs = {\n",
        "    \"max_tokens\":200,\n",
        "    \"echo\":False,\n",
        "    \"top_k\":1\n",
        "}\n",
        "\n",
        "prompt = \"Which country hosted 2018 fifa world cup?\"\n",
        "res = llm(prompt, **generation_kwargs)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xLMhixENw1z",
        "outputId": "86891487-47f5-4cdd-ce92-3f876aebed07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    7264.88 ms\n",
            "llama_print_timings:      sample time =       7.45 ms /   200 runs   (    0.04 ms per token, 26838.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =  142857.42 ms /   200 runs   (  714.29 ms per token,     1.40 tokens per second)\n",
            "llama_print_timings:       total time =  143047.91 ms /   200 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-a9abefdd-d0a7-4247-8571-a44bf2facdc1',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1743072997,\n",
              " 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf',\n",
              " 'choices': [{'text': \"\\nThe 2018 FIFA World Cup was held in Russia from June 14 to July 15, 2 The 2018 FIFA World Cup was the 21st FIFA World Cup, an international football tournament contested by the men's national teams of the member associations of FIFA once every four years. It took place in Russia from 14 June to 15 July 2018. It was the first World Cup to be held in Eastern Europe, and the 11th time that it had been held in Europe.\\nThe tournament consisted of 32 teams, which were selected from qualifying matches held between March 2015 and October 2017. A total of 64 matches were played in 12 venues located in 11 cities across Russia. It was the first World Cup to use the video assistant referee (VAR) system.\\nFrance won the tournament\",\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 14, 'completion_tokens': 200, 'total_tokens': 214}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### inference test 2"
      ],
      "metadata": {
        "id": "QyMg1vuZsug7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_kwargs = {\n",
        "    \"max_tokens\":200,\n",
        "    \"echo\":False,\n",
        "    \"top_k\":1\n",
        "}\n",
        "\n",
        "prompt = \"who is MS dhoni?\"\n",
        "res = llm(prompt, **generation_kwargs)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYLejfxAP0FM",
        "outputId": "031d457c-87f1-46a4-d795-94e6c9dc891f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    7264.88 ms\n",
            "llama_print_timings:      sample time =       4.43 ms /   113 runs   (    0.04 ms per token, 25519.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2805.45 ms /     7 tokens (  400.78 ms per token,     2.50 tokens per second)\n",
            "llama_print_timings:        eval time =   78864.45 ms /   112 runs   (  704.15 ms per token,     1.42 tokens per second)\n",
            "llama_print_timings:       total time =   81763.70 ms /   119 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-c7243b7d-c6ed-48f3-baf3-c6397b446338',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1743073140,\n",
              " 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf',\n",
              " 'choices': [{'text': '\\nMS Dhoni is a former Indian cricketer and the current captain of the Indian national cricket team. He is a wicketkeeper-batsman and is widely regarded as one of the greatest finishers in limited-overs cricket. Dhoni made his international debut in 2004 and has since played in over 300 ODIs and 100 Test matches for India. He is also the captain of the Chennai Super Kings franchise in the Indian Premier League (IPL).',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 8, 'completion_tokens': 112, 'total_tokens': 120}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save quantized model to drive"
      ],
      "metadata": {
        "id": "bVC9REdAsykS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaiQVhf_QBJe",
        "outputId": "4583d594-63b5-480c-c796-6e4fe2c4e733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/drive/My Drive/quantized_models\""
      ],
      "metadata": {
        "id": "zzbEmjiZQ7bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_file_path = '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf'\n",
        "destination_file_path = '/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf'\n",
        "\n",
        "shutil.copy(source_file_path, destination_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w5YZJgfRQwUR",
        "outputId": "fc396c5e-611e-4ae3-cb5e-fc42d945367c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.36.0 accelerate==0.25.0 huggingface_hub==0.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ud3Ha0-rVFM-",
        "outputId": "31444739-6321-45a9-c04c-1a2af4f7c632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.36.0\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/126.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.25.0\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting huggingface_hub==0.20.0\n",
            "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.20.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.20.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (3.0.2)\n",
            "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.20.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub, tokenizers, transformers, accelerate\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.29.3\n",
            "    Uninstalling huggingface-hub-0.29.3:\n",
            "      Successfully uninstalled huggingface-hub-0.29.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.0\n",
            "    Uninstalling transformers-4.50.0:\n",
            "      Successfully uninstalled transformers-4.50.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.5.2\n",
            "    Uninstalling accelerate-1.5.2:\n",
            "      Successfully uninstalled accelerate-1.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 huggingface_hub-0.20.0 tokenizers-0.15.2 transformers-4.36.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "accelerate",
                  "huggingface_hub",
                  "transformers"
                ]
              },
              "id": "92a647366bc2486a9f4ca3e78205c2dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## without Medusa Head"
      ],
      "metadata": {
        "id": "lLV_VDvPtAhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from llama_cpp import Llama\n",
        "import numpy as np\n",
        "from typing import List, Optional, Dict\n",
        "import logging\n",
        "from time import time\n",
        "\n",
        "class MedusaLlamaCppModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf\",\n",
        "        medusa_num_heads: int = 4,\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        n_ctx: int = 2048,\n",
        "        n_batch: int = 512,  # Batch size for processing\n",
        "        n_threads: int = 8    # Number of threads for parallel processing\n",
        "    ):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.device = device\n",
        "\n",
        "        try:\n",
        "            # Initialize the base GGUF model with optimized parameters\n",
        "            self.base_model = Llama(\n",
        "                model_path=model_path,\n",
        "                n_ctx=n_ctx,\n",
        "                n_batch=n_batch,      # Enable batch processing\n",
        "                n_threads=n_threads,   # Enable multi-threading\n",
        "                n_gpu_layers=-1,      # Use GPU for all layers if available\n",
        "                verbose=False\n",
        "            )\n",
        "            self.logger.info(\"Base model loaded successfully\")\n",
        "\n",
        "            self.medusa_num_heads = medusa_num_heads\n",
        "            self.batch_size = n_batch\n",
        "            self.logger.info(f\"Initialized with {medusa_num_heads} Medusa heads\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error initializing model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _batch_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        n_tokens: int = 32,  # Generate multiple tokens at once\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Generate multiple tokens in a single batch\"\"\"\n",
        "        try:\n",
        "            response = self.base_model(\n",
        "                prompt,\n",
        "                max_tokens=n_tokens,\n",
        "                temperature=temperature,\n",
        "                echo=False,\n",
        "                stop=[\"</s>\", \"<|endoftext|>\"]\n",
        "            )\n",
        "\n",
        "            if isinstance(response, dict) and 'choices' in response:\n",
        "                return response['choices'][0]['text']\n",
        "            elif isinstance(response, list) and len(response) > 0:\n",
        "                return response[0]['text']\n",
        "            return \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in batch generation: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_tokens: int = 512,\n",
        "        temperature: float = 0.7,\n",
        "        batch_size: int = 32  # Number of tokens to generate per batch\n",
        "    ) -> str:\n",
        "        \"\"\"Generate text using optimized batch processing\"\"\"\n",
        "        try:\n",
        "            generated_text = prompt\n",
        "            tokens_generated = 0\n",
        "            start_time = time()\n",
        "\n",
        "            self.logger.info(f\"Starting generation with prompt: {prompt[:50]}...\")\n",
        "\n",
        "            while tokens_generated < max_tokens:\n",
        "                # Calculate remaining tokens\n",
        "                remaining_tokens = max_tokens - tokens_generated\n",
        "                current_batch_size = min(batch_size, remaining_tokens)\n",
        "\n",
        "                # Generate batch of tokens\n",
        "                new_text = self._batch_generate(\n",
        "                    generated_text,\n",
        "                    n_tokens=current_batch_size,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "\n",
        "                if not new_text:\n",
        "                    break\n",
        "\n",
        "                generated_text += new_text\n",
        "                tokens_generated += len(new_text.split())  # Approximate token count\n",
        "\n",
        "                # Log progress with speed metrics\n",
        "                if tokens_generated % 50 == 0:\n",
        "                    elapsed_time = time() - start_time\n",
        "                    speed = tokens_generated / elapsed_time\n",
        "                    self.logger.info(f\"Generated {tokens_generated} tokens. Speed: {speed:.2f} tokens/second\")\n",
        "\n",
        "            # Final statistics\n",
        "            total_time = time() - start_time\n",
        "            avg_speed = tokens_generated / total_time\n",
        "            self.logger.info(f\"Generation completed. Total tokens: {tokens_generated}\")\n",
        "            self.logger.info(f\"Average speed: {avg_speed:.2f} tokens/second\")\n",
        "\n",
        "            return generated_text\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in text generation: {str(e)}\")\n",
        "            return generated_text  # Return what we have so far\n",
        "\n",
        "def print_generation_stats(text: str, time_taken: float):\n",
        "    \"\"\"Print generation statistics\"\"\"\n",
        "    tokens = len(text.split())\n",
        "    speed = tokens / time_taken\n",
        "    print(f\"\\nGeneration Statistics:\")\n",
        "    print(f\"Total tokens: {tokens}\")\n",
        "    print(f\"Time taken: {time_taken:.2f} seconds\")\n",
        "    print(f\"Speed: {speed:.2f} tokens/second\")"
      ],
      "metadata": {
        "id": "0sEgEUCpWwo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MedusaLlamaCppModel(\n",
        "    model_path=\"/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf\",\n",
        "    medusa_num_heads=4,\n",
        "    n_batch=512,      # Increased batch size\n",
        "    n_threads=8\n",
        ")\n",
        "\n",
        "# Generate text\n"
      ],
      "metadata": {
        "id": "wYAveJmcS-VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time\"\n",
        "start_time = time()\n",
        "\n",
        "generated_text = model.generate(\n",
        "    prompt=prompt,\n",
        "    max_tokens=100,\n",
        "    temperature=0.7,\n",
        "    batch_size=32     # Adjust based on your GPU memory\n",
        ")\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "# Print results and statistics\n",
        "print(\"\\nGenerated text:\")\n",
        "print(generated_text)\n",
        "print_generation_stats(generated_text, end_time - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqSAUNUKVuE3",
        "outputId": "90c3f7c9-eb8c-40c1-d863-e5e3fcabc853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            "Once upon a time, in the early days of the internet, there was a group of people who were passionate about the music of the 1980s. They spent countless hours collecting and trading songs, creating playlists, and sharing their love for this decade’s music.\n",
            "One day, they decided to create a website where they could share their passion with others who loved 80s music as much as they did. They called it “The 80s Network” and it quickly became a hub for fans of this music genre.\n",
            "The 80s Network was different from other music websites because it was created and run by\n",
            "\n",
            "Generation Statistics:\n",
            "Total tokens: 103\n",
            "Time taken: 115.15 seconds\n",
            "Speed: 0.89 tokens/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Medusa Head"
      ],
      "metadata": {
        "id": "V0JSTQo2tLcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### with medusa head working code a bit"
      ],
      "metadata": {
        "id": "rkvOnrnT-CiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from llama_cpp import Llama\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class MedusaConfig:\n",
        "    \"\"\"Configuration for Medusa head.\"\"\"\n",
        "    num_heads: int = 4\n",
        "    max_tokens: int = 512\n",
        "    temperature: float = 0.7\n",
        "    posterior_threshold: float = 0.09\n",
        "    posterior_alpha: float = 0.3\n",
        "    tree_branching_factor: int = 2\n",
        "    draft_checkpoint_ratio: float = 0.5\n",
        "\n",
        "class MedusaHead:\n",
        "    \"\"\"Implementation of Medusa head for speculative decoding.\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads: int):\n",
        "        self.logger = logging.getLogger(\"MedusaHead\")\n",
        "        self.num_heads = num_heads\n",
        "        self.logger.info(f\"Initialized Medusa head with {num_heads} heads\")\n",
        "\n",
        "        self.tree_indices = self._create_tree_indices()\n",
        "        print(f\"Tree Indices: {self.tree_indices}\")\n",
        "\n",
        "    def _create_tree_indices(self) -> List[List[int]]:\n",
        "        \"\"\"Create tree indices for heads.\"\"\"\n",
        "        tree = []\n",
        "        for i in range(self.num_heads):\n",
        "            level = []\n",
        "            for j in range(min(2**i, self.num_heads - len(tree))):\n",
        "                level.append(i + j)\n",
        "            if level:\n",
        "                tree.append(level)\n",
        "        self.logger.info(f\"Tree indices created: {tree}\")\n",
        "        return tree\n",
        "\n",
        "    def generate_draft_tokens(self, model: Llama, prompt: str, temperature: float) -> List[str]:\n",
        "        \"\"\"Generate draft tokens using the tree structure.\"\"\"\n",
        "        base_token = self._generate_token(model, prompt, temperature)\n",
        "        print(f\"Base Token: {base_token}\")\n",
        "        if not base_token:\n",
        "            return []\n",
        "\n",
        "        draft_sequence = [base_token]\n",
        "        current_prompt = prompt + base_token\n",
        "\n",
        "        for level in self.tree_indices:\n",
        "            if not current_prompt:\n",
        "                break\n",
        "\n",
        "            level_tokens = []\n",
        "            for _ in level:\n",
        "                token = self._generate_token(model, current_prompt, temperature)\n",
        "                print(f\"Generated Token at Level {level}: {token}\")\n",
        "                if token:\n",
        "                    level_tokens.append(token)\n",
        "\n",
        "            if not level_tokens:\n",
        "                break\n",
        "\n",
        "            draft_sequence.append(level_tokens[0])\n",
        "            current_prompt += level_tokens[0]\n",
        "\n",
        "        print(f\"Final Draft Sequence: {draft_sequence}\")\n",
        "        return draft_sequence\n",
        "\n",
        "    def _generate_token(self, model: Llama, prompt: str, temperature: float) -> str:\n",
        "        \"\"\"Generate a single token from the model.\"\"\"\n",
        "        try:\n",
        "            response = model(\n",
        "                prompt,\n",
        "                max_tokens=1,\n",
        "                temperature=temperature,\n",
        "                echo=False\n",
        "            )\n",
        "            print(f\"Model Response: {response}\")\n",
        "            if isinstance(response, dict) and 'choices' in response:\n",
        "                return response['choices'][0]['text']\n",
        "            elif isinstance(response, list) and len(response) > 0:\n",
        "                return response[0]['text']\n",
        "            return \"\"\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating token: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "class MedusaModel:\n",
        "    \"\"\"Main Medusa model combining llama.cpp with Medusa head.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str, config: Optional[MedusaConfig] = None):\n",
        "        self.logger = logging.getLogger(\"MedusaModel\")\n",
        "        self.config = config or MedusaConfig()\n",
        "\n",
        "        try:\n",
        "            self.logger.info(f\"Loading model from {model_path}\")\n",
        "            self.base_model = Llama(\n",
        "                model_path=model_path,\n",
        "                n_ctx=4096,\n",
        "                n_batch=512,\n",
        "                n_threads=8,\n",
        "                n_gpu_layers=-1,\n",
        "                verbose=False\n",
        "            )\n",
        "            self.logger.info(\"Base model loaded successfully\")\n",
        "\n",
        "            self.medusa_head = MedusaHead(self.config.num_heads)\n",
        "            self.logger.info(f\"Medusa head initialized with {self.config.num_heads} heads\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error initializing model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate(self, prompt: str, config: Optional[MedusaConfig] = None) -> Dict:\n",
        "        \"\"\"Generate text using Medusa speculative decoding.\"\"\"\n",
        "        cfg = config or self.config\n",
        "        self.logger.info(f\"Generating text with Medusa ({cfg.num_heads} heads)\")\n",
        "\n",
        "        generated_text = prompt\n",
        "        tokens_generated = 0\n",
        "        tokens_accepted = 0\n",
        "        draft_tokens_generated = 0\n",
        "        iterations = 0\n",
        "        speedup = 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "        baseline_tokens_per_sec = 0\n",
        "\n",
        "        warmup_tokens = min(20, cfg.max_tokens // 10)\n",
        "        warmup_start = time.time()\n",
        "        warmup_text = \"WARMUP TEXT HERE\"\n",
        "        warmup_time = time.time() - warmup_start\n",
        "        if warmup_tokens > 0 and warmup_time > 0:\n",
        "            baseline_tokens_per_sec = warmup_tokens / warmup_time\n",
        "            print(f\"Baseline Speed: {baseline_tokens_per_sec:.2f} tokens/sec\")\n",
        "\n",
        "        while tokens_generated < cfg.max_tokens:\n",
        "            iterations += 1\n",
        "\n",
        "            draft_start = time.time()\n",
        "            drafts = self.medusa_head.generate_draft_tokens(\n",
        "                self.base_model, generated_text, cfg.temperature\n",
        "            )\n",
        "            draft_time = time.time() - draft_start\n",
        "            print(f\"Draft Tokens: {drafts}\")\n",
        "\n",
        "            draft_tokens_generated += len(drafts)\n",
        "\n",
        "            if not drafts:\n",
        "                token = \"FALLBACK TOKEN\"\n",
        "                generated_text += token\n",
        "                tokens_generated += 1\n",
        "                print(f\"Fallback Token: {token}\")\n",
        "                continue\n",
        "\n",
        "            verify_start = time.time()\n",
        "            accepted_count, probs = (len(drafts), [0.9] * len(drafts))\n",
        "            verify_time = time.time() - verify_start\n",
        "\n",
        "            if accepted_count > 0:\n",
        "                accepted_drafts = drafts[:accepted_count]\n",
        "                generated_text += ''.join(accepted_drafts)\n",
        "                tokens_generated += accepted_count\n",
        "                tokens_accepted += accepted_count\n",
        "                print(f\"Accepted Tokens: {accepted_drafts}\")\n",
        "                for token in accepted_drafts:\n",
        "                    print(token, end=\"\", flush=True)\n",
        "            else:\n",
        "                token = \"FALLBACK TOKEN\"\n",
        "                generated_text += token\n",
        "                tokens_generated += 1\n",
        "                print(f\"Generated Token: {token}\")\n",
        "\n",
        "            if tokens_generated % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                current_speed = tokens_generated / elapsed if elapsed > 0 else 0\n",
        "                if baseline_tokens_per_sec > 0:\n",
        "                    speedup = current_speed / baseline_tokens_per_sec\n",
        "                print(f\"Tokens Generated: {tokens_generated}, Speed: {current_speed:.2f} tokens/sec, Speedup: {speedup:.2f}\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        tokens_per_sec = tokens_generated / total_time if total_time > 0 else 0\n",
        "        acceptance_rate = tokens_accepted / draft_tokens_generated * 100 if draft_tokens_generated > 0 else 0\n",
        "\n",
        "        print(f\"Final Stats -> Tokens Generated: {tokens_generated}, Speed: {tokens_per_sec:.2f} tokens/sec, Acceptance Rate: {acceptance_rate:.1f}%\")\n",
        "\n",
        "        return {\n",
        "            \"tokens_generated\": tokens_generated,\n",
        "            \"draft_tokens_generated\": draft_tokens_generated,\n",
        "            \"tokens_per_sec\": tokens_per_sec,\n",
        "            \"acceptance_rate\": acceptance_rate\n",
        "        }\n"
      ],
      "metadata": {
        "id": "_7EgzJyupSWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medusa_config = MedusaConfig(\n",
        "    num_heads=4,\n",
        "    max_tokens=50,\n",
        "    temperature=0.7,\n",
        "    posterior_threshold=0.09,\n",
        "    posterior_alpha=0.3         # Alpha for posterior scaling\n",
        ")\n",
        "\n",
        "model = MedusaModel(\n",
        "    model_path=\"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n",
        "    config=medusa_config\n",
        ")\n",
        "\n",
        "prompt = \"there was a cricketer named MS dhoni\"\n",
        "result = model.generate(prompt)\n",
        "# generated_text = result[\"choices\"][0][\"text\"]\n",
        "# Print the generated text\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(result)\n",
        "\n",
        "# Print performance statistics\n",
        "# stats = result[\"stats\"]\n",
        "# print(f\"\\nPerformance Statistics:\")\n",
        "# print(f\"- Tokens generated: {stats['tokens_generated']}\")\n",
        "# print(f\"- Generation speed: {stats['tokens_per_sec']:.2f} tokens/second\")\n",
        "# print(f\"- Speedup: {stats['speedup']:.2f}x faster than standard generation\")\n",
        "# print(f\"- Acceptance rate: {stats['acceptance_rate']:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2FLmSSOpUUa",
        "outputId": "0b89b395-f83f-43ea-9fcd-492edb566215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree Indices: [[0], [1, 2], [2, 3], [3]]\n",
            "Baseline Speed: 10485760.00 tokens/sec\n",
            "Model Response: {'id': 'cmpl-44500d43-6c2b-4337-8a5d-114502cf5540', 'object': 'text_completion', 'created': 1743081958, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' who', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 12, 'completion_tokens': 1, 'total_tokens': 13}}\n",
            "Base Token:  who\n",
            "Model Response: {'id': 'cmpl-7e8eace2-b4c9-42b8-886f-63b4f8fecb94', 'object': 'text_completion', 'created': 1743081973, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' played', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 1, 'total_tokens': 14}}\n",
            "Generated Token at Level [0]:  played\n",
            "Model Response: {'id': 'cmpl-b5d578fd-4b36-497d-82d8-60c98de4e354', 'object': 'text_completion', 'created': 1743081973, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' for', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 1, 'total_tokens': 15}}\n",
            "Generated Token at Level [1, 2]:  for\n",
            "Model Response: {'id': 'cmpl-46d23067-6f50-40f8-b451-c428aa1109d7', 'object': 'text_completion', 'created': 1743081974, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' for', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 1, 'total_tokens': 15}}\n",
            "Generated Token at Level [1, 2]:  for\n",
            "Model Response: {'id': 'cmpl-6282416c-14bd-4b2d-8f8b-bcc58b2b24df', 'object': 'text_completion', 'created': 1743081974, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' India', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 1, 'total_tokens': 16}}\n",
            "Generated Token at Level [2, 3]:  India\n",
            "Model Response: {'id': 'cmpl-2aeaec4b-1137-46c3-81e1-07bf68e22ef3', 'object': 'text_completion', 'created': 1743081975, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' J', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 1, 'total_tokens': 16}}\n",
            "Generated Token at Level [2, 3]:  J\n",
            "Model Response: {'id': 'cmpl-f39e54eb-3a98-478e-b57b-9441e408fa60', 'object': 'text_completion', 'created': 1743081975, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' in', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 16, 'completion_tokens': 1, 'total_tokens': 17}}\n",
            "Generated Token at Level [3]:  in\n",
            "Final Draft Sequence: [' who', ' played', ' for', ' India', ' in']\n",
            "Draft Tokens: [' who', ' played', ' for', ' India', ' in']\n",
            "Accepted Tokens: [' who', ' played', ' for', ' India', ' in']\n",
            " who played for India inModel Response: {'id': 'cmpl-860609fb-ec3a-42d2-b4d3-ce5e4fcff14a', 'object': 'text_completion', 'created': 1743081976, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' the', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 1, 'total_tokens': 18}}\n",
            "Base Token:  the\n",
            "Model Response: {'id': 'cmpl-900f8c8e-301e-4321-8981-e7b38b84bab6', 'object': 'text_completion', 'created': 1743081977, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' world', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 1, 'total_tokens': 19}}\n",
            "Generated Token at Level [0]:  world\n",
            "Model Response: {'id': 'cmpl-d4f3553b-1a7b-4ba5-8fa0-0d04b3f24d34', 'object': 'text_completion', 'created': 1743081977, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' cup', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 19, 'completion_tokens': 1, 'total_tokens': 20}}\n",
            "Generated Token at Level [1, 2]:  cup\n",
            "Model Response: {'id': 'cmpl-3aff2740-f37e-4e06-b19f-7f887a75c2d1', 'object': 'text_completion', 'created': 1743081978, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' cup', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 19, 'completion_tokens': 1, 'total_tokens': 20}}\n",
            "Generated Token at Level [1, 2]:  cup\n",
            "Model Response: {'id': 'cmpl-eef121da-d249-4719-a88c-d4f6c8c57856', 'object': 'text_completion', 'created': 1743081979, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 1, 'total_tokens': 21}}\n",
            "Generated Token at Level [2, 3]: .\n",
            "Model Response: {'id': 'cmpl-efc01367-c3d9-43b9-9e4d-ae1a83b33d84', 'object': 'text_completion', 'created': 1743081980, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '\\n', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 1, 'total_tokens': 21}}\n",
            "Generated Token at Level [2, 3]: \n",
            "\n",
            "Model Response: {'id': 'cmpl-1aeacf8a-2b65-4372-9eb2-5711b1ceb120', 'object': 'text_completion', 'created': 1743081981, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '\\n', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 21, 'completion_tokens': 1, 'total_tokens': 22}}\n",
            "Generated Token at Level [3]: \n",
            "\n",
            "Final Draft Sequence: [' the', ' world', ' cup', '.', '\\n']\n",
            "Draft Tokens: [' the', ' world', ' cup', '.', '\\n']\n",
            "Accepted Tokens: [' the', ' world', ' cup', '.', '\\n']\n",
            " the world cup.\n",
            "Tokens Generated: 10, Speed: 0.42 tokens/sec, Speedup: 0.00\n",
            "Model Response: {'id': 'cmpl-8579e7c8-51bb-4fe7-a2f7-5f0bff31b76c', 'object': 'text_completion', 'created': 1743081982, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'I', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 22, 'completion_tokens': 1, 'total_tokens': 23}}\n",
            "Base Token: I\n",
            "Model Response: {'id': 'cmpl-cdf7a799-6735-463f-8084-a71082280926', 'object': 'text_completion', 'created': 1743081982, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' apolog', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 23, 'completion_tokens': 1, 'total_tokens': 24}}\n",
            "Generated Token at Level [0]:  apolog\n",
            "Model Response: {'id': 'cmpl-011ab5bb-7487-4bbd-ba9a-9f6e2f4289aa', 'object': 'text_completion', 'created': 1743081983, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'ize', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 1, 'total_tokens': 25}}\n",
            "Generated Token at Level [1, 2]: ize\n",
            "Model Response: {'id': 'cmpl-e0b6740d-c636-4698-8903-c58c3081b9d8', 'object': 'text_completion', 'created': 1743081983, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'ize', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 1, 'total_tokens': 25}}\n",
            "Generated Token at Level [1, 2]: ize\n",
            "Model Response: {'id': 'cmpl-604db280-8ce0-436f-a3a9-c305d8c0dbf2', 'object': 'text_completion', 'created': 1743081984, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ',', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 25, 'completion_tokens': 1, 'total_tokens': 26}}\n",
            "Generated Token at Level [2, 3]: ,\n",
            "Model Response: {'id': 'cmpl-3dde2f6e-12a5-4fa3-a73b-a45d2937dcb6', 'object': 'text_completion', 'created': 1743081985, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' for', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 25, 'completion_tokens': 1, 'total_tokens': 26}}\n",
            "Generated Token at Level [2, 3]:  for\n",
            "Model Response: {'id': 'cmpl-4b0245f4-5fd9-45dd-a506-4946d0f87d12', 'object': 'text_completion', 'created': 1743081985, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' but', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 26, 'completion_tokens': 1, 'total_tokens': 27}}\n",
            "Generated Token at Level [3]:  but\n",
            "Final Draft Sequence: ['I', ' apolog', 'ize', ',', ' but']\n",
            "Draft Tokens: ['I', ' apolog', 'ize', ',', ' but']\n",
            "Accepted Tokens: ['I', ' apolog', 'ize', ',', ' but']\n",
            "I apologize, butModel Response: {'id': 'cmpl-3dc59e56-1d49-4385-a386-2c6818c731f6', 'object': 'text_completion', 'created': 1743081986, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' that', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 27, 'completion_tokens': 1, 'total_tokens': 28}}\n",
            "Base Token:  that\n",
            "Model Response: {'id': 'cmpl-030e7be8-8760-4a5f-b63d-20faeac46f5e', 'object': 'text_completion', 'created': 1743081986, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' is', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 28, 'completion_tokens': 1, 'total_tokens': 29}}\n",
            "Generated Token at Level [0]:  is\n",
            "Model Response: {'id': 'cmpl-782d88af-e5fa-45c4-ae9f-c6e0775cb140', 'object': 'text_completion', 'created': 1743081987, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' not', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 29, 'completion_tokens': 1, 'total_tokens': 30}}\n",
            "Generated Token at Level [1, 2]:  not\n",
            "Model Response: {'id': 'cmpl-35b66aa3-619d-4752-af21-186068e52315', 'object': 'text_completion', 'created': 1743081987, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' not', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 29, 'completion_tokens': 1, 'total_tokens': 30}}\n",
            "Generated Token at Level [1, 2]:  not\n",
            "Model Response: {'id': 'cmpl-79810baa-0748-4c7c-b3d7-93448b8488c9', 'object': 'text_completion', 'created': 1743081988, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' true', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 30, 'completion_tokens': 1, 'total_tokens': 31}}\n",
            "Generated Token at Level [2, 3]:  true\n",
            "Model Response: {'id': 'cmpl-3241cad7-dfe3-4164-af7c-a144e71236aa', 'object': 'text_completion', 'created': 1743081988, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' true', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 30, 'completion_tokens': 1, 'total_tokens': 31}}\n",
            "Generated Token at Level [2, 3]:  true\n",
            "Model Response: {'id': 'cmpl-53b8134c-3054-4407-bd8b-aa4fe2dd4a43', 'object': 'text_completion', 'created': 1743081989, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 31, 'completion_tokens': 1, 'total_tokens': 32}}\n",
            "Generated Token at Level [3]: .\n",
            "Final Draft Sequence: [' that', ' is', ' not', ' true', '.']\n",
            "Draft Tokens: [' that', ' is', ' not', ' true', '.']\n",
            "Accepted Tokens: [' that', ' is', ' not', ' true', '.']\n",
            " that is not true.Tokens Generated: 20, Speed: 0.63 tokens/sec, Speedup: 0.00\n",
            "Model Response: {'id': 'cmpl-1336136c-8a31-4df6-985f-1d50d4427e76', 'object': 'text_completion', 'created': 1743081990, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' MS', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 32, 'completion_tokens': 1, 'total_tokens': 33}}\n",
            "Base Token:  MS\n",
            "Model Response: {'id': 'cmpl-9a623a60-faab-4866-970b-173438495984', 'object': 'text_completion', 'created': 1743081990, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' D', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 33, 'completion_tokens': 1, 'total_tokens': 34}}\n",
            "Generated Token at Level [0]:  D\n",
            "Model Response: {'id': 'cmpl-020ac0a7-05f3-4026-9920-0420cc1fca70', 'object': 'text_completion', 'created': 1743081991, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'h', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 34, 'completion_tokens': 1, 'total_tokens': 35}}\n",
            "Generated Token at Level [1, 2]: h\n",
            "Model Response: {'id': 'cmpl-cecc439a-21b2-4332-9d56-eb9193c29bed', 'object': 'text_completion', 'created': 1743081991, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'h', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 34, 'completion_tokens': 1, 'total_tokens': 35}}\n",
            "Generated Token at Level [1, 2]: h\n",
            "Model Response: {'id': 'cmpl-cf5f8359-78df-448f-a9f0-d06fb22f8318', 'object': 'text_completion', 'created': 1743081992, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'oni', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 35, 'completion_tokens': 1, 'total_tokens': 36}}\n",
            "Generated Token at Level [2, 3]: oni\n",
            "Model Response: {'id': 'cmpl-e5153950-995f-4fba-8e23-e478f9f253b0', 'object': 'text_completion', 'created': 1743081993, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'oni', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 35, 'completion_tokens': 1, 'total_tokens': 36}}\n",
            "Generated Token at Level [2, 3]: oni\n",
            "Model Response: {'id': 'cmpl-1c02aa04-7980-4cea-bf63-896d5d7d87c3', 'object': 'text_completion', 'created': 1743081994, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' is', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 1, 'total_tokens': 37}}\n",
            "Generated Token at Level [3]:  is\n",
            "Final Draft Sequence: [' MS', ' D', 'h', 'oni', ' is']\n",
            "Draft Tokens: [' MS', ' D', 'h', 'oni', ' is']\n",
            "Accepted Tokens: [' MS', ' D', 'h', 'oni', ' is']\n",
            " MS Dhoni isModel Response: {'id': 'cmpl-3d9516a4-9f42-415c-b1d0-94cbc4ba812a', 'object': 'text_completion', 'created': 1743081995, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' a', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 37, 'completion_tokens': 1, 'total_tokens': 38}}\n",
            "Base Token:  a\n",
            "Model Response: {'id': 'cmpl-54f17aa0-c8c7-4291-968b-3db85a10d542', 'object': 'text_completion', 'created': 1743081996, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' cr', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 1, 'total_tokens': 39}}\n",
            "Generated Token at Level [0]:  cr\n",
            "Model Response: {'id': 'cmpl-7beb1717-0b65-476f-89fa-4530324f50ea', 'object': 'text_completion', 'created': 1743081997, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'ick', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 1, 'total_tokens': 40}}\n",
            "Generated Token at Level [1, 2]: ick\n",
            "Model Response: {'id': 'cmpl-f98bc14b-2c25-423e-8cd2-54ede7414675', 'object': 'text_completion', 'created': 1743081997, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'ick', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 1, 'total_tokens': 40}}\n",
            "Generated Token at Level [1, 2]: ick\n",
            "Model Response: {'id': 'cmpl-83e84f95-5aad-4925-92b4-1e457da02309', 'object': 'text_completion', 'created': 1743081998, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'eter', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 40, 'completion_tokens': 1, 'total_tokens': 41}}\n",
            "Generated Token at Level [2, 3]: eter\n",
            "Model Response: {'id': 'cmpl-c73a589e-89d0-43eb-90c1-3cab2c92ab7b', 'object': 'text_completion', 'created': 1743081998, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'eter', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 40, 'completion_tokens': 1, 'total_tokens': 41}}\n",
            "Generated Token at Level [2, 3]: eter\n",
            "Model Response: {'id': 'cmpl-c5db88c2-968c-4c3d-ba0b-a003d5b9d047', 'object': 'text_completion', 'created': 1743081999, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' who', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 1, 'total_tokens': 42}}\n",
            "Generated Token at Level [3]:  who\n",
            "Final Draft Sequence: [' a', ' cr', 'ick', 'eter', ' who']\n",
            "Draft Tokens: [' a', ' cr', 'ick', 'eter', ' who']\n",
            "Accepted Tokens: [' a', ' cr', 'ick', 'eter', ' who']\n",
            " a cricketer whoTokens Generated: 30, Speed: 0.72 tokens/sec, Speedup: 0.00\n",
            "Model Response: {'id': 'cmpl-9bfde5e5-ca96-4f29-a999-a31e1f448194', 'object': 'text_completion', 'created': 1743081999, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' played', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 1, 'total_tokens': 43}}\n",
            "Base Token:  played\n",
            "Model Response: {'id': 'cmpl-25490fe3-25e6-47ce-9402-e84d73b9cd09', 'object': 'text_completion', 'created': 1743082000, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' for', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 1, 'total_tokens': 44}}\n",
            "Generated Token at Level [0]:  for\n",
            "Model Response: {'id': 'cmpl-77f04ab2-a4cf-44f3-85b1-65a8d99eac65', 'object': 'text_completion', 'created': 1743082000, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' India', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 44, 'completion_tokens': 1, 'total_tokens': 45}}\n",
            "Generated Token at Level [1, 2]:  India\n",
            "Model Response: {'id': 'cmpl-d8754ed6-34e3-442c-9c90-0ad94c542d70', 'object': 'text_completion', 'created': 1743082001, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' India', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 44, 'completion_tokens': 1, 'total_tokens': 45}}\n",
            "Generated Token at Level [1, 2]:  India\n",
            "Model Response: {'id': 'cmpl-249b883f-9008-4ed8-b3f1-5df8d9430a4b', 'object': 'text_completion', 'created': 1743082002, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' in', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 1, 'total_tokens': 46}}\n",
            "Generated Token at Level [2, 3]:  in\n",
            "Model Response: {'id': 'cmpl-60efd651-2270-4d68-ae29-4fa8a6e391cd', 'object': 'text_completion', 'created': 1743082002, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' in', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 1, 'total_tokens': 46}}\n",
            "Generated Token at Level [2, 3]:  in\n",
            "Model Response: {'id': 'cmpl-cc58ac5a-0987-4855-941e-c45d2b255a62', 'object': 'text_completion', 'created': 1743082003, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' the', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 46, 'completion_tokens': 1, 'total_tokens': 47}}\n",
            "Generated Token at Level [3]:  the\n",
            "Final Draft Sequence: [' played', ' for', ' India', ' in', ' the']\n",
            "Draft Tokens: [' played', ' for', ' India', ' in', ' the']\n",
            "Accepted Tokens: [' played', ' for', ' India', ' in', ' the']\n",
            " played for India in theModel Response: {'id': 'cmpl-d28b04ba-6edc-4414-8c80-de703a6b0570', 'object': 'text_completion', 'created': 1743082003, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' ', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 47, 'completion_tokens': 1, 'total_tokens': 48}}\n",
            "Base Token:  \n",
            "Model Response: {'id': 'cmpl-73560a9c-f358-40ac-8766-00cf736fcde1', 'object': 'text_completion', 'created': 1743082004, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '2', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 1, 'total_tokens': 49}}\n",
            "Generated Token at Level [0]: 2\n",
            "Model Response: {'id': 'cmpl-d747af48-d0ec-4382-949f-d81477d03d5e', 'object': 'text_completion', 'created': 1743082004, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '0', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 1, 'total_tokens': 50}}\n",
            "Generated Token at Level [1, 2]: 0\n",
            "Model Response: {'id': 'cmpl-aa63aad0-72d9-4c62-ace1-785cc7c323d0', 'object': 'text_completion', 'created': 1743082005, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '0', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 1, 'total_tokens': 50}}\n",
            "Generated Token at Level [1, 2]: 0\n",
            "Model Response: {'id': 'cmpl-db76ab20-ec12-429c-9993-1317cc97306c', 'object': 'text_completion', 'created': 1743082006, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '1', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 50, 'completion_tokens': 1, 'total_tokens': 51}}\n",
            "Generated Token at Level [2, 3]: 1\n",
            "Model Response: {'id': 'cmpl-65a87cf5-7da0-41bf-9610-c4bf09b83427', 'object': 'text_completion', 'created': 1743082006, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '1', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 50, 'completion_tokens': 1, 'total_tokens': 51}}\n",
            "Generated Token at Level [2, 3]: 1\n",
            "Model Response: {'id': 'cmpl-28aa4e1f-bfeb-427b-ab27-d8e0c9b8d28d', 'object': 'text_completion', 'created': 1743082007, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '1', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 1, 'total_tokens': 52}}\n",
            "Generated Token at Level [3]: 1\n",
            "Final Draft Sequence: [' ', '2', '0', '1', '1']\n",
            "Draft Tokens: [' ', '2', '0', '1', '1']\n",
            "Accepted Tokens: [' ', '2', '0', '1', '1']\n",
            " 2011Tokens Generated: 40, Speed: 0.80 tokens/sec, Speedup: 0.00\n",
            "Model Response: {'id': 'cmpl-4b816147-ab35-4337-b1f7-515af3d6787f', 'object': 'text_completion', 'created': 1743082008, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' Cr', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 52, 'completion_tokens': 1, 'total_tokens': 53}}\n",
            "Base Token:  Cr\n",
            "Model Response: {'id': 'cmpl-badc70d8-a4cb-4f75-825e-eb9b489ddf6e', 'object': 'text_completion', 'created': 1743082009, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': 'icket', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 53, 'completion_tokens': 1, 'total_tokens': 54}}\n",
            "Generated Token at Level [0]: icket\n",
            "Model Response: {'id': 'cmpl-7db4c90a-ed53-4e9d-be0b-02ad8f9c7d12', 'object': 'text_completion', 'created': 1743082010, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' World', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 54, 'completion_tokens': 1, 'total_tokens': 55}}\n",
            "Generated Token at Level [1, 2]:  World\n",
            "Model Response: {'id': 'cmpl-2b82fc14-def5-4995-a487-457443d2552e', 'object': 'text_completion', 'created': 1743082011, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' World', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 54, 'completion_tokens': 1, 'total_tokens': 55}}\n",
            "Generated Token at Level [1, 2]:  World\n",
            "Model Response: {'id': 'cmpl-3eb4ba62-19cb-418c-a2c3-ae6eba49565e', 'object': 'text_completion', 'created': 1743082011, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' Cup', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 55, 'completion_tokens': 1, 'total_tokens': 56}}\n",
            "Generated Token at Level [2, 3]:  Cup\n",
            "Model Response: {'id': 'cmpl-b4df2401-8582-4481-aaf6-adecb350bfc1', 'object': 'text_completion', 'created': 1743082012, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' Cup', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 55, 'completion_tokens': 1, 'total_tokens': 56}}\n",
            "Generated Token at Level [2, 3]:  Cup\n",
            "Model Response: {'id': 'cmpl-761b5e95-47d1-4d6d-9c9b-bf58fa268d32', 'object': 'text_completion', 'created': 1743082012, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ',', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 1, 'total_tokens': 57}}\n",
            "Generated Token at Level [3]: ,\n",
            "Final Draft Sequence: [' Cr', 'icket', ' World', ' Cup', ',']\n",
            "Draft Tokens: [' Cr', 'icket', ' World', ' Cup', ',']\n",
            "Accepted Tokens: [' Cr', 'icket', ' World', ' Cup', ',']\n",
            " Cricket World Cup,Model Response: {'id': 'cmpl-5abbcef2-bd56-4def-a1d4-0e1cac3893db', 'object': 'text_completion', 'created': 1743082013, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' held', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 57, 'completion_tokens': 1, 'total_tokens': 58}}\n",
            "Base Token:  held\n",
            "Model Response: {'id': 'cmpl-4847621b-a183-4cab-bc66-d2fea8c4e88b', 'object': 'text_completion', 'created': 1743082014, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' in', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 1, 'total_tokens': 59}}\n",
            "Generated Token at Level [0]:  in\n",
            "Model Response: {'id': 'cmpl-51e7a1ca-3a4c-4120-8143-4cf1b164c39c', 'object': 'text_completion', 'created': 1743082014, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' India', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 1, 'total_tokens': 60}}\n",
            "Generated Token at Level [1, 2]:  India\n",
            "Model Response: {'id': 'cmpl-579d51c9-684a-402f-a85a-b59d35bbc60e', 'object': 'text_completion', 'created': 1743082015, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' India', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 1, 'total_tokens': 60}}\n",
            "Generated Token at Level [1, 2]:  India\n",
            "Model Response: {'id': 'cmpl-e26565e7-bc2e-4269-aadf-3ed6e7079ad2', 'object': 'text_completion', 'created': 1743082015, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': '.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 1, 'total_tokens': 61}}\n",
            "Generated Token at Level [2, 3]: .\n",
            "Model Response: {'id': 'cmpl-34054d69-9592-472c-babd-e2b2d1b9dc83', 'object': 'text_completion', 'created': 1743082016, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ',', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 1, 'total_tokens': 61}}\n",
            "Generated Token at Level [2, 3]: ,\n",
            "Model Response: {'id': 'cmpl-59996433-8bfd-4b0d-9c02-70cbdfba465e', 'object': 'text_completion', 'created': 1743082016, 'model': '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf', 'choices': [{'text': ' He', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 61, 'completion_tokens': 1, 'total_tokens': 62}}\n",
            "Generated Token at Level [3]:  He\n",
            "Final Draft Sequence: [' held', ' in', ' India', '.', ' He']\n",
            "Draft Tokens: [' held', ' in', ' India', '.', ' He']\n",
            "Accepted Tokens: [' held', ' in', ' India', '.', ' He']\n",
            " held in India. HeTokens Generated: 50, Speed: 0.85 tokens/sec, Speedup: 0.00\n",
            "Final Stats -> Tokens Generated: 50, Speed: 0.85 tokens/sec, Acceptance Rate: 100.0%\n",
            "\n",
            "Generated Text:\n",
            "{'tokens_generated': 50, 'draft_tokens_generated': 50, 'tokens_per_sec': 0.8455450637169397, 'acceptance_rate': 100.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2rf1753VsPYXTOVl62iLwS2dITs_5XcrdEGFYarW57qMykxj6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwyg_7vW4C1A",
        "outputId": "87b42b13-b2f5-42c0-e45b-73a695f332cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### medusa working code final... need to make sure it produces response"
      ],
      "metadata": {
        "id": "YnUWAqj6-WiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1"
      ],
      "metadata": {
        "id": "JAFNHq_RFSYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import logging\n",
        "import asyncio\n",
        "from typing import List, Dict, Optional\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import uuid\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from threading import Lock\n",
        "from contextlib import asynccontextmanager\n",
        "\n",
        "# For notebook environments\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Import Medusa components from the repository\n",
        "from medusa.model.medusa_model import MedusaModel\n",
        "from medusa.model.medusa_choices import mc_sim_7b_63  # Pre-defined Medusa choices\n",
        "from medusa.model.utils import generate_medusa_buffers, reset_medusa_mode, initialize_medusa\n",
        "from medusa.model.kv_cache import initialize_past_key_values\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define request and response models\n",
        "class GenerationRequest(BaseModel):\n",
        "    prompt: str\n",
        "    max_length: int = 512\n",
        "    temperature: float = 0.7\n",
        "    posterior_threshold: float = 0.09\n",
        "    posterior_alpha: float = 0.3\n",
        "\n",
        "class GenerationResponse(BaseModel):\n",
        "    text: str\n",
        "    generation_time: float\n",
        "    tokens_generated: int\n",
        "    tokens_per_second: float\n",
        "    speedup_factor: Optional[float] = None\n",
        "\n",
        "@dataclass\n",
        "class BatchRequest:\n",
        "    id: str\n",
        "    prompt: str\n",
        "    timestamp: datetime\n",
        "    max_length: int = 512\n",
        "    temperature: float = 0.7\n",
        "    posterior_threshold: float = 0.09\n",
        "    posterior_alpha: float = 0.3\n",
        "\n",
        "class MedusaLlamaCppManager:\n",
        "    \"\"\"\n",
        "    Manager class that combines llama.cpp with Medusa for speculative decoding\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n",
        "        medusa_num_heads: int = 4,\n",
        "        n_ctx: int = 2048,\n",
        "        n_batch: int = 512,\n",
        "        n_threads: int = 8\n",
        "    ):\n",
        "        self.logger = logging.getLogger(\"MedusaLlamaCppManager\")\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.medusa_num_heads = medusa_num_heads\n",
        "        self.model_lock = Lock()  # For thread safety\n",
        "\n",
        "        # Load compiled model\n",
        "        self.llama_model = Llama(\n",
        "            model_path=model_path,\n",
        "            n_ctx=n_ctx,\n",
        "            n_batch=n_batch,\n",
        "            n_threads=n_threads,\n",
        "            n_gpu_layers=-1\n",
        "        )\n",
        "        self.logger.info(f\"Loaded GGUF model from {model_path}\")\n",
        "\n",
        "        # Initialize Medusa components\n",
        "        self.medusa_choices = mc_sim_7b_63\n",
        "        self.medusa_buffers = self._initialize_medusa_buffers()\n",
        "        self.logger.info(f\"Initialized Medusa with {medusa_num_heads} heads\")\n",
        "\n",
        "        # Baseline metrics for speedup calculation\n",
        "        self._baseline_tokens_per_second = self._calculate_baseline_speed()\n",
        "\n",
        "    def _initialize_medusa_buffers(self) -> Dict:\n",
        "        \"\"\"Initialize Medusa buffers for speculative decoding\"\"\"\n",
        "        # Create buffers similar to how the Medusa model does it\n",
        "        tree_indices = torch.zeros((self.medusa_num_heads, 2), dtype=torch.long)\n",
        "        medusa_attn_mask = torch.ones((self.medusa_num_heads + 1, self.medusa_num_heads + 1), dtype=torch.bool)\n",
        "        medusa_attn_mask = torch.triu(medusa_attn_mask, diagonal=1)\n",
        "        medusa_position_ids = torch.arange(self.medusa_num_heads, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"tree_indices\": tree_indices,\n",
        "            \"medusa_attn_mask\": medusa_attn_mask,\n",
        "            \"medusa_position_ids\": medusa_position_ids,\n",
        "            \"retrieve_indices\": None  # Will be set during generation\n",
        "        }\n",
        "\n",
        "    def _calculate_baseline_speed(self) -> float:\n",
        "        \"\"\"Calculate baseline generation speed without Medusa\"\"\"\n",
        "        prompt = \"Once upon a time\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        response = self.llama_model(prompt, max_tokens=20, temperature=0.7)\n",
        "\n",
        "        if response and 'choices' in response:\n",
        "            generated_text = response['choices'][0]['text']\n",
        "            tokens = len(generated_text.split())\n",
        "            elapsed_time = time.time() - start_time\n",
        "\n",
        "            if elapsed_time > 0 and tokens > 0:\n",
        "                tokens_per_second = tokens / elapsed_time\n",
        "                self.logger.info(f\"Baseline generation speed: {tokens_per_second:.2f} tokens/second\")\n",
        "                return tokens_per_second\n",
        "\n",
        "        # Default value if calculation fails\n",
        "        return 5.0\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 512,\n",
        "        temperature: float = 0.7,\n",
        "        posterior_threshold: float = 0.09,\n",
        "        posterior_alpha: float = 0.3\n",
        "    ) -> Dict:\n",
        "        \"\"\"Generate text using Medusa speculative decoding with llama.cpp backend\"\"\"\n",
        "        with self.model_lock:  # Ensure thread safety\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Initial generation\n",
        "            input_text = prompt\n",
        "            generated_text = \"\"\n",
        "            tokens_generated = 0\n",
        "            draft_tokens_generated = 0\n",
        "            accepted_tokens = 0\n",
        "\n",
        "            while tokens_generated < max_length:\n",
        "                # Generate base prediction and drafts\n",
        "                base_token, drafts = self._generate_drafts(input_text, temperature)\n",
        "\n",
        "                if not base_token:\n",
        "                    break\n",
        "\n",
        "                # Verify drafts\n",
        "                accepted_count, accepted_drafts = self._verify_drafts(\n",
        "                    input_text,\n",
        "                    [base_token] + drafts,\n",
        "                    temperature,\n",
        "                    posterior_threshold,\n",
        "                    posterior_alpha\n",
        "                )\n",
        "\n",
        "                # Update counts and text\n",
        "                draft_tokens_generated += len(drafts) + 1  # base + drafts\n",
        "                accepted_tokens += accepted_count\n",
        "\n",
        "                if accepted_count > 0:\n",
        "                    accepted_text = ''.join(accepted_drafts)\n",
        "                    input_text += accepted_text\n",
        "                    generated_text += accepted_text\n",
        "                    tokens_generated += accepted_count\n",
        "                else:\n",
        "                    # If no drafts accepted, use base token\n",
        "                    input_text += base_token\n",
        "                    generated_text += base_token\n",
        "                    tokens_generated += 1\n",
        "\n",
        "                # Check for generation completion\n",
        "                if tokens_generated % 50 == 0:\n",
        "                    self.logger.info(f\"Generated {tokens_generated} tokens\")\n",
        "\n",
        "            # Calculate statistics\n",
        "            elapsed_time = time.time() - start_time\n",
        "            tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n",
        "            speedup = tokens_per_second / self._baseline_tokens_per_second\n",
        "\n",
        "            return {\n",
        "                \"text\": generated_text,\n",
        "                \"generation_time\": elapsed_time,\n",
        "                \"tokens_generated\": tokens_generated,\n",
        "                \"tokens_per_second\": tokens_per_second,\n",
        "                \"speedup_factor\": speedup,\n",
        "                \"acceptance_rate\": (accepted_tokens / draft_tokens_generated * 100) if draft_tokens_generated > 0 else 0\n",
        "            }\n",
        "\n",
        "    def _generate_drafts(self, context: str, temperature: float):\n",
        "        \"\"\"Generate base token and draft tokens using Medusa tree structure\"\"\"\n",
        "        try:\n",
        "            # Generate base token\n",
        "            base_response = self.llama_model(\n",
        "                context,\n",
        "                max_tokens=1,\n",
        "                temperature=temperature,\n",
        "                echo=False\n",
        "            )\n",
        "\n",
        "            if not base_response or 'choices' not in base_response:\n",
        "                return \"\", []\n",
        "\n",
        "            base_token = base_response['choices'][0]['text']\n",
        "\n",
        "            # Generate draft tokens following the Medusa tree structure\n",
        "            drafts = []\n",
        "            draft_context = context + base_token\n",
        "\n",
        "            for _ in range(self.medusa_num_heads - 1):  # -1 because we already have the base token\n",
        "                draft_response = self.llama_model(\n",
        "                    draft_context,\n",
        "                    max_tokens=1,\n",
        "                    temperature=temperature,\n",
        "                    echo=False\n",
        "                )\n",
        "\n",
        "                if draft_response and 'choices' in draft_response:\n",
        "                    draft_token = draft_response['choices'][0]['text']\n",
        "                    drafts.append(draft_token)\n",
        "                    draft_context += draft_token\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            return base_token, drafts\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating drafts: {str(e)}\")\n",
        "            return \"\", []\n",
        "\n",
        "    def _verify_drafts(\n",
        "        self,\n",
        "        context: str,\n",
        "        drafts: List[str],\n",
        "        temperature: float,\n",
        "        threshold: float,\n",
        "        alpha: float\n",
        "    ):\n",
        "        \"\"\"Verify draft tokens and return accepted ones\"\"\"\n",
        "        if not drafts:\n",
        "            return 0, []\n",
        "\n",
        "        # Calculate verification scores\n",
        "        scores = []\n",
        "        accepted_drafts = []\n",
        "        current_context = context\n",
        "\n",
        "        for draft in drafts:\n",
        "            # Calculate probability score for this draft\n",
        "            try:\n",
        "                verify_response = self.llama_model(\n",
        "                    current_context + draft,\n",
        "                    max_tokens=0,\n",
        "                    temperature=0.0,  # Use 0 for verification\n",
        "                    echo=True\n",
        "                )\n",
        "\n",
        "                # Get verification score (this is an approximation)\n",
        "                score = 0.0\n",
        "                if verify_response and 'choices' in verify_response:\n",
        "                    # In real implementation, we'd get the token probability\n",
        "                    # Here we use a simple heuristic\n",
        "                    score = float(verify_response['choices'][0].get('logprobs', {}).get('token_logprobs', [-1.0])[-1])\n",
        "\n",
        "                # Apply temperature and alpha\n",
        "                score = np.exp(score / max(temperature, 1e-6)) ** alpha\n",
        "                scores.append(score)\n",
        "\n",
        "                # Accept if above threshold\n",
        "                if score >= threshold:\n",
        "                    accepted_drafts.append(draft)\n",
        "                    current_context += draft\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error verifying draft: {str(e)}\")\n",
        "                break\n",
        "\n",
        "        return len(accepted_drafts), accepted_drafts\n",
        "\n",
        "class BatchProcessor:\n",
        "    \"\"\"Processes generation requests in batches for better efficiency\"\"\"\n",
        "    def __init__(self, model_manager, batch_size=4, max_wait_time=0.1):\n",
        "        self.model_manager = model_manager\n",
        "        self.batch_size = batch_size\n",
        "        self.max_wait_time = max_wait_time\n",
        "        self.queue = asyncio.Queue()\n",
        "        self.logger = logging.getLogger(\"BatchProcessor\")\n",
        "        self.processing = False\n",
        "        self.results = {}\n",
        "        self.background_task = None\n",
        "\n",
        "    async def add_request(self, request: BatchRequest) -> str:\n",
        "        \"\"\"Add a request to the processing queue\"\"\"\n",
        "        await self.queue.put(request)\n",
        "        self.logger.info(f\"Added request {request.id} to queue, size: {self.queue.qsize()}\")\n",
        "\n",
        "        # Start background processing if not already running\n",
        "        if not self.processing:\n",
        "            self.processing = True\n",
        "            self.background_task = asyncio.create_task(self._process_queue())\n",
        "\n",
        "        return request.id\n",
        "\n",
        "    async def get_result(self, request_id: str, timeout: float = 60.0) -> Optional[Dict]:\n",
        "        \"\"\"Wait for and retrieve result for a specific request ID\"\"\"\n",
        "        start_time = time.time()\n",
        "        while time.time() - start_time < timeout:\n",
        "            if request_id in self.results:\n",
        "                result = self.results.pop(request_id)\n",
        "                return result\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "        return None  # Timeout\n",
        "\n",
        "    async def _process_queue(self):\n",
        "        \"\"\"Background task to process requests in the queue\"\"\"\n",
        "        try:\n",
        "            while True:\n",
        "                # Process requests in batches up to batch_size\n",
        "                batch = []\n",
        "\n",
        "                # Try to get up to batch_size requests\n",
        "                for _ in range(self.batch_size):\n",
        "                    try:\n",
        "                        request = await asyncio.wait_for(\n",
        "                            self.queue.get(),\n",
        "                            timeout=self.max_wait_time\n",
        "                        )\n",
        "                        batch.append(request)\n",
        "                    except asyncio.TimeoutError:\n",
        "                        break\n",
        "\n",
        "                if not batch:\n",
        "                    self.processing = False\n",
        "                    break\n",
        "\n",
        "                self.logger.info(f\"Processing batch of {len(batch)} requests\")\n",
        "\n",
        "                # Process each request in the batch\n",
        "                for request in batch:\n",
        "                    try:\n",
        "                        # Generate text using the model\n",
        "                        result = self.model_manager.generate(\n",
        "                            prompt=request.prompt,\n",
        "                            max_length=request.max_length,\n",
        "                            temperature=request.temperature,\n",
        "                            posterior_threshold=request.posterior_threshold,\n",
        "                            posterior_alpha=request.posterior_alpha\n",
        "                        )\n",
        "\n",
        "                        # Store the result\n",
        "                        self.results[request.id] = result\n",
        "                        self.logger.info(f\"Completed request {request.id}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.logger.error(f\"Error processing request {request.id}: {str(e)}\")\n",
        "                        self.results[request.id] = {\"error\": str(e)}\n",
        "\n",
        "                    finally:\n",
        "                        self.queue.task_done()\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in batch processing: {str(e)}\")\n",
        "            self.processing = False\n",
        "\n",
        "# Define global variables for model manager and batch processor\n",
        "model_manager = None\n",
        "batch_processor = None\n",
        "\n",
        "# Lifespan context manager for FastAPI\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    # Startup - initialize resources\n",
        "    global model_manager, batch_processor\n",
        "\n",
        "    logger.info(\"Initializing model and batch processor...\")\n",
        "    try:\n",
        "        model_manager = MedusaLlamaCppManager(\n",
        "            model_path=\"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n",
        "            medusa_num_heads=4\n",
        "        )\n",
        "        batch_processor = BatchProcessor(model_manager)\n",
        "        logger.info(\"Model and batch processor initialized successfully\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    try:\n",
        "        # Set up ngrok tunnel\n",
        "        ngrok_tunnel = ngrok.connect(8000)\n",
        "        logger.info(f\"Ngrok tunnel established at: {ngrok_tunnel.public_url}\")\n",
        "        print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to establish ngrok tunnel: {str(e)}\")\n",
        "\n",
        "    yield\n",
        "\n",
        "    # Shutdown - clean up resources\n",
        "    logger.info(\"Shutting down server and resources\")\n",
        "    # Add any cleanup code here if needed\n",
        "\n",
        "# Initialize FastAPI app with lifespan\n",
        "app = FastAPI(\n",
        "    title=\"Medusa LLM Service\",\n",
        "    description=\"Language model service with Medusa speculative decoding and dynamic batching\",\n",
        "    lifespan=lifespan\n",
        ")\n",
        "\n",
        "# Endpoint for text generation\n",
        "@app.post(\"/generate\", response_model=GenerationResponse)\n",
        "async def generate_text(request: GenerationRequest):\n",
        "    try:\n",
        "        # Create a batch request\n",
        "        request_id = str(uuid.uuid4())\n",
        "        batch_request = BatchRequest(\n",
        "            id=request_id,\n",
        "            prompt=request.prompt,\n",
        "            timestamp=datetime.now(),\n",
        "            max_length=request.max_length,\n",
        "            temperature=request.temperature,\n",
        "            posterior_threshold=request.posterior_threshold,\n",
        "            posterior_alpha=request.posterior_alpha\n",
        "        )\n",
        "\n",
        "        # Add request to batch processor\n",
        "        await batch_processor.add_request(batch_request)\n",
        "\n",
        "        # Wait for result\n",
        "        result = await batch_processor.get_result(request_id)\n",
        "\n",
        "        if not result:\n",
        "            raise HTTPException(status_code=408, detail=\"Request timed out\")\n",
        "\n",
        "        if \"error\" in result:\n",
        "            raise HTTPException(status_code=500, detail=result[\"error\"])\n",
        "\n",
        "        # Prepare response\n",
        "        return GenerationResponse(\n",
        "            text=result[\"text\"],\n",
        "            generation_time=result[\"generation_time\"],\n",
        "            tokens_generated=result[\"tokens_generated\"],\n",
        "            tokens_per_second=result[\"tokens_per_second\"],\n",
        "            speedup_factor=result[\"speedup_factor\"]\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in generate endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Health check endpoint\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"model\": \"Medusa LLM Service\"}\n",
        "\n",
        "# Benchmark endpoint with and without Medusa\n",
        "@app.post(\"/benchmark\")\n",
        "async def benchmark(request: GenerationRequest):\n",
        "    try:\n",
        "        # Generate with Medusa speculative decoding\n",
        "        medusa_result = model_manager.generate(\n",
        "            prompt=request.prompt,\n",
        "            max_length=request.max_length,\n",
        "            temperature=request.temperature,\n",
        "            posterior_threshold=request.posterior_threshold,\n",
        "            posterior_alpha=request.posterior_alpha\n",
        "        )\n",
        "\n",
        "        # Generate without Medusa (using baseline approach)\n",
        "        start_time = time.time()\n",
        "        standard_response = model_manager.llama_model(\n",
        "            request.prompt,\n",
        "            max_tokens=request.max_length,\n",
        "            temperature=request.temperature\n",
        "        )\n",
        "        standard_time = time.time() - start_time\n",
        "\n",
        "        standard_text = standard_response['choices'][0]['text'] if standard_response and 'choices' in standard_response else \"\"\n",
        "        standard_tokens = len(standard_text.split())\n",
        "        standard_tokens_per_second = standard_tokens / standard_time if standard_time > 0 else 0\n",
        "\n",
        "        # Prepare comparative benchmark results\n",
        "        return {\n",
        "            \"medusa\": {\n",
        "                \"text\": medusa_result[\"text\"],\n",
        "                \"generation_time\": medusa_result[\"generation_time\"],\n",
        "                \"tokens_generated\": medusa_result[\"tokens_generated\"],\n",
        "                \"tokens_per_second\": medusa_result[\"tokens_per_second\"],\n",
        "                \"acceptance_rate\": medusa_result[\"acceptance_rate\"]\n",
        "            },\n",
        "            \"standard\": {\n",
        "                \"text\": standard_text,\n",
        "                \"generation_time\": standard_time,\n",
        "                \"tokens_generated\": standard_tokens,\n",
        "                \"tokens_per_second\": standard_tokens_per_second\n",
        "            },\n",
        "            \"speedup\": medusa_result[\"tokens_per_second\"] / standard_tokens_per_second if standard_tokens_per_second > 0 else 0\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in benchmark endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Function to start the server\n",
        "async def start_server():\n",
        "    # No need to initialize model here since it's done in the lifespan context manager\n",
        "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n",
        "    server = uvicorn.Server(config)\n",
        "    await server.serve()\n",
        "\n",
        "# When running the file directly\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "\n",
        "    # For notebook environments\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        # Run for Colab/Jupyter\n",
        "        asyncio.run(start_server())\n",
        "    else:\n",
        "        # Run for standard Python environments\n",
        "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRYVr3ma3pYb",
        "outputId": "c9145c12-2a72-4e8d-bc4b-19d653fa7714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "Using `is_flash_attn_available` is deprecated and will be removed in v4.38. Please use `is_flash_attn_2_available` instead.\n",
            "Using `is_flash_attn_available` is deprecated and will be removed in v4.38. Please use `is_flash_attn_2_available` instead.\n",
            "INFO:     Started server process [68612]\n",
            "INFO:     Waiting for application startup.\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Vicuna 7b v1.3\n",
            "llama_model_loader: - kv   3:                            general.version str              = v1.3\n",
            "llama_model_loader: - kv   4:                           general.basename str              = vicuna\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   7:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  12:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  13:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  25:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = Vicuna 7b v1.3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.block_count': '32', 'tokenizer.ggml.padding_token_id': '0', 'general.basename': 'vicuna', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '2048', 'general.name': 'Vicuna 7b v1.3', 'llama.rope.dimension_count': '128', 'general.version': 'v1.3', 'general.type': 'model', 'general.size_label': '7B', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.vocab_size': '32000'}\n",
            "Using fallback chat format: llama-2\n",
            "\n",
            "llama_print_timings:        load time =    2204.98 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /    20 runs   (    0.06 ms per token, 17809.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2204.93 ms /     5 tokens (  440.99 ms per token,     2.27 tokens per second)\n",
            "llama_print_timings:        eval time =   12453.49 ms /    19 runs   (  655.45 ms per token,     1.53 tokens per second)\n",
            "llama_print_timings:       total time =   14676.44 ms /    24 tokens\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://fd1a-104-196-160-201.ngrok-free.app\n",
            "INFO:     14.102.161.98:0 - \"POST /generate HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     14.102.161.98:0 - \"POST /generate HTTP/1.1\" 422 Unprocessable Entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2204.98 ms\n",
            "llama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23255.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     756.32 ms /     1 runs   (  756.32 ms per token,     1.32 tokens per second)\n",
            "llama_print_timings:       total time =     757.24 ms /     1 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2204.98 ms\n",
            "llama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23809.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     588.10 ms /     1 runs   (  588.10 ms per token,     1.70 tokens per second)\n",
            "llama_print_timings:       total time =     588.61 ms /     1 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2204.98 ms\n",
            "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     556.89 ms /     1 runs   (  556.89 ms per token,     1.80 tokens per second)\n",
            "llama_print_timings:       total time =     557.87 ms /     1 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2204.98 ms\n",
            "llama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     571.35 ms /     1 runs   (  571.35 ms per token,     1.75 tokens per second)\n",
            "llama_print_timings:       total time =     574.67 ms /     1 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken\n"
      ],
      "metadata": {
        "id": "pn415ae9Tiap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c011ef-d1ff-45bb-c217-31387479951e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 9675\n"
      ],
      "metadata": {
        "id": "Q2RT2O8Qb2dY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}