{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7e0486bd82334c9786d4a682a7c867f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a57ce004e1cf4ac386a7f28e01c81cd1","IPY_MODEL_d7f43223762d4fe29ad65e12aea985ed","IPY_MODEL_d8c36726c50e4ec9b51ab9b95933c69b"],"layout":"IPY_MODEL_2456245b43cb4d008e5a3cff9c473883"}},"a57ce004e1cf4ac386a7f28e01c81cd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c152aef3faa49e6a8370cb4a1d76856","placeholder":"​","style":"IPY_MODEL_9657883fd54e47348635119813a8cfea","value":"vicuna-7B-v1.3-F16.gguf: 100%"}},"d7f43223762d4fe29ad65e12aea985ed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba23535ee4f14566aa3a6319034b821a","max":13478105056,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a29e7d5d60e4230a53f302ea7794ff0","value":13478105056}},"d8c36726c50e4ec9b51ab9b95933c69b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_661a5f9442c641a2ad4b84a466837d47","placeholder":"​","style":"IPY_MODEL_aabef08df1f3431db0a3b25a22c1a275","value":" 13.5G/13.5G [06:25&lt;00:00, 42.7MB/s]"}},"2456245b43cb4d008e5a3cff9c473883":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c152aef3faa49e6a8370cb4a1d76856":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9657883fd54e47348635119813a8cfea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba23535ee4f14566aa3a6319034b821a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a29e7d5d60e4230a53f302ea7794ff0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"661a5f9442c641a2ad4b84a466837d47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aabef08df1f3431db0a3b25a22c1a275":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f2bedf81edd438db1e6c9a2aa41f232":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_433c50d1f40442e39ab4e89756633ed2","IPY_MODEL_f7abca3d5824472e8efe63668cee5f8f","IPY_MODEL_ec133885fe2b419286ae609f52efbf59","IPY_MODEL_6b0f7d81472b4eaabb0325861c3e17d8","IPY_MODEL_29495f23ed974a9a9f54c06aee42708e"],"layout":"IPY_MODEL_b526e765ebfa410184392667c4ff7262"}},"433c50d1f40442e39ab4e89756633ed2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6950c1233bb4bb588ccc86961e2e907","placeholder":"​","style":"IPY_MODEL_bf355ee0edf24dc18831d76ac9d39842","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"f7abca3d5824472e8efe63668cee5f8f":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_9b1ecd205f02453b9a66535130d66185","placeholder":"​","style":"IPY_MODEL_35e12a72c3d140aab8a45a289db88155","value":""}},"ec133885fe2b419286ae609f52efbf59":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_4d55d82c57c445739214a38419f638a5","style":"IPY_MODEL_9ffd8b1684444acd94294959214c3c28","value":true}},"6b0f7d81472b4eaabb0325861c3e17d8":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_12b14e88bc09472897076dd6b8b62945","style":"IPY_MODEL_81941dffdc6e4c90b77fc6cb6506a626","tooltip":""}},"29495f23ed974a9a9f54c06aee42708e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9f5552a9c0e42b09c96cb11bbad8b35","placeholder":"​","style":"IPY_MODEL_9bda68f535524c5997e0d11d81fae635","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"b526e765ebfa410184392667c4ff7262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"a6950c1233bb4bb588ccc86961e2e907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf355ee0edf24dc18831d76ac9d39842":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b1ecd205f02453b9a66535130d66185":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35e12a72c3d140aab8a45a289db88155":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d55d82c57c445739214a38419f638a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ffd8b1684444acd94294959214c3c28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12b14e88bc09472897076dd6b8b62945":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81941dffdc6e4c90b77fc6cb6506a626":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"d9f5552a9c0e42b09c96cb11bbad8b35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bda68f535524c5997e0d11d81fae635":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":305494,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":260644,"modelId":281799}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Necessary libraries","metadata":{"id":"EE_-hYiqr__R"}},{"cell_type":"code","source":"!apt update && apt install -y cmake git curl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4mbw1dd6NjtG","outputId":"e577cabe-acb9-43b1-8b06-145fded5b267","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyngrok  # Required to expose FastAPI publicly","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eIUWJz1XNkWO","outputId":"0f962a8e-ac01-4dc1-d109-b2d4db2e87a9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastapi uvicorn transformers huggingface_hub torch accelerate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lWgkJ-gENpKQ","outputId":"c24832fe-8967-4438-c4a6-cdddc678a098","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/FasterDecoding/Medusa.git\n%cd Medusa\n!pip install -e .","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tun5_m6PuP0T","outputId":"c00555e0-002a-43aa-f916-44c194c718e6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd ..","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_pvgGlUvCTo","outputId":"058d0671-4da2-490f-d7ac-95dd57965c2d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyngrok sentencepiece\n# !pip install medusa-llm","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVhj8n8qhAtY","outputId":"3c192740-e7f2-4e3f-ec4a-4978f3e257fc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp.git\n%cd llama.cpp\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8ta3f63NtvU","outputId":"302f7a3a-e001-46cd-e270-13a38c39a176","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build LlamaCPP","metadata":{"id":"wZEfwGXPsIx8"}},{"cell_type":"code","source":"!cmake -B build\n!cmake --build build --config Release -j 8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEy6V_IhvXAt","outputId":"1c22bce5-a58b-436a-cc4b-1bee48d696d0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359,"referenced_widgets":["4f2bedf81edd438db1e6c9a2aa41f232","433c50d1f40442e39ab4e89756633ed2","f7abca3d5824472e8efe63668cee5f8f","ec133885fe2b419286ae609f52efbf59","6b0f7d81472b4eaabb0325861c3e17d8","29495f23ed974a9a9f54c06aee42708e","b526e765ebfa410184392667c4ff7262","a6950c1233bb4bb588ccc86961e2e907","bf355ee0edf24dc18831d76ac9d39842","9b1ecd205f02453b9a66535130d66185","35e12a72c3d140aab8a45a289db88155","4d55d82c57c445739214a38419f638a5","9ffd8b1684444acd94294959214c3c28","12b14e88bc09472897076dd6b8b62945","81941dffdc6e4c90b77fc6cb6506a626","d9f5552a9c0e42b09c96cb11bbad8b35","9bda68f535524c5997e0d11d81fae635"]},"id":"qY2ULRBMOM0P","outputId":"b70a9f57-e639-41f6-ceaf-a8e19faa7d65","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## clone Model form HF","metadata":{"id":"N8f2WIDRsOOP"}},{"cell_type":"code","source":"!git clone https://huggingface.co/lmsys/vicuna-7b-v1.3\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6jWhQbFmOWQ2","outputId":"82362f26-5abf-4d5f-9c1f-bb2eb65ac9a7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd .. && mkdir optimized_model","metadata":{"id":"MM--8KqQ9pqI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pwd","metadata":{"id":"Per5ErHq90UP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## convert to GGUF","metadata":{"id":"rDGr7YKBsRQF"}},{"cell_type":"code","source":"!python convert_hf_to_gguf.py vicuna-7b-v1.3 --outtype f16 --outfile ../optimized_model/vicuna-7b-v1.3-F16.gguf\n","metadata":{"id":"bJq8yYSVOdDA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9867498-87ea-4041-c30e-f587e21372e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd ..","metadata":{"id":"z4Jz2bFi_2Ef","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Qauntize using llama CPP","metadata":{"id":"BJvYQpZasXKD"}},{"cell_type":"code","source":"!cd /content/llama.cpp/build/bin && ./llama-quantize /content/optimized_model/vicuna-7b-v1.3-F16.gguf /content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf q4_K_M","metadata":{"id":"TA_jcLwug-Yd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fcf3c2de-b3d6-40e1-b83b-21f36f411e31","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /content/llama.cpp/vicuna-7b-v1.3/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4G_zX7LThF4G","outputId":"33fd218e-74ce-43a9-b3ba-f0f58e34f79d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## install python helper for llama cpp","metadata":{"id":"mDbo1A4zscoK"}},{"cell_type":"code","source":"!pip install llama-cpp-python==0.2.85","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3MUE7jF_MZK5","outputId":"e3c70327-bd45-4e53-8a7e-08a886790887","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:10:29.581817Z","iopub.execute_input":"2025-03-28T10:10:29.582171Z","iopub.status.idle":"2025-03-28T10:11:50.345999Z","shell.execute_reply.started":"2025-03-28T10:10:29.582140Z","shell.execute_reply":"2025-03-28T10:11:50.345119Z"}},"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python==0.2.85\n  Using cached llama_cpp_python-0.2.85.tar.gz (49.3 MB)\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.85)\n  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.85) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\nUsing cached diskcache-5.6.3-py3-none-any.whl (45 kB)\nBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.85-cp310-cp310-linux_x86_64.whl size=2873164 sha256=00443c2cf73398331ef51b22b7c0f952bc1e9fe53a4459631db430e9e5bea031\n  Stored in directory: /root/.cache/pip/wheels/3f/e8/4e/29a754f9175ef52b6481cd75e3af4de38bf6dfa9c2972f75d4\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.85\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from llama_cpp import Llama","metadata":{"id":"QgLsYyKRM5Ol","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:12:58.046021Z","iopub.execute_input":"2025-03-28T10:12:58.046617Z","iopub.status.idle":"2025-03-28T10:12:58.110847Z","shell.execute_reply.started":"2025-03-28T10:12:58.046565Z","shell.execute_reply":"2025-03-28T10:12:58.110071Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model_path = \"/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf\"","metadata":{"id":"K4u04_z7NXKs","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## load model for interfence testing","metadata":{"id":"CMbGjAN6siQ5"}},{"cell_type":"code","source":"llm = Llama(model_path=model_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJXnBsnwNn2c","outputId":"47eeb6c2-3806-40a8-adc8-908b824e044e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### inference test 1","metadata":{"id":"GB7dA3bosnpa"}},{"cell_type":"code","source":"generation_kwargs = {\n    \"max_tokens\":200,\n    \"echo\":False,\n    \"top_k\":1\n}\n\nprompt = \"Which country hosted 2018 fifa world cup?\"\nres = llm(prompt, **generation_kwargs)\nres","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xLMhixENw1z","outputId":"86891487-47f5-4cdd-ce92-3f876aebed07","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### inference test 2","metadata":{"id":"QyMg1vuZsug7"}},{"cell_type":"code","source":"generation_kwargs = {\n    \"max_tokens\":200,\n    \"echo\":False,\n    \"top_k\":1\n}\n\nprompt = \"who is MS dhoni?\"\nres = llm(prompt, **generation_kwargs)\nres","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYLejfxAP0FM","outputId":"031d457c-87f1-46a4-d795-94e6c9dc891f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### save quantized model to drive","metadata":{"id":"bVC9REdAsykS"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaiQVhf_QBJe","outputId":"4583d594-63b5-480c-c796-6e4fe2c4e733","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir \"/content/drive/My Drive/quantized_models\"","metadata":{"id":"zzbEmjiZQ7bB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nsource_file_path = '/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf'\ndestination_file_path = '/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf'\n\nshutil.copy(source_file_path, destination_file_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"w5YZJgfRQwUR","outputId":"fc396c5e-611e-4ae3-cb5e-fc42d945367c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.36.0 accelerate==0.25.0 huggingface_hub==0.20.0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Ud3Ha0-rVFM-","outputId":"31444739-6321-45a9-c04c-1a2af4f7c632","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from llama_cpp import Llama\n# from medusa.model.medusa_model import MedusaModel\n# from transformers import LlamaConfig\n# import numpy as np\n# from typing import List, Optional, Dict\n# import logging\n\n# class MedusaLlamaCppModel:\n#     def __init__(\n#         self,\n#         model_path: str = \"/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf\",\n#         medusa_num_heads: int = 4,\n#         device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n#         n_ctx: int = 2048\n#     ):\n#         self.logger = logging.getLogger(__name__)\n#         self.device = device\n\n#         try:\n#             # Initialize the base GGUF model\n#             self.base_model = Llama(\n#                 model_path=model_path,\n#                 n_ctx=n_ctx,\n#                 n_gpu_layers=-1  # Use GPU for all layers if available\n#             )\n#             self.logger.info(\"Base model loaded successfully\")\n\n#             # Initialize Medusa components\n#             self.medusa_num_heads = medusa_num_heads\n#             self.medusa_buffers = self._initialize_medusa_buffers()\n#             self.logger.info(\"Medusa buffers initialized\")\n\n#         except Exception as e:\n#             self.logger.error(f\"Error initializing model: {str(e)}\")\n#             raise\n\n#     def _initialize_medusa_buffers(self) -> Dict:\n#         \"\"\"Initialize Medusa buffers for speculative decoding\"\"\"\n#         try:\n#             # Create tree indices for Medusa heads\n#             tree_indices = torch.zeros(\n#                 (self.medusa_num_heads, 2),\n#                 dtype=torch.long,\n#                 device=self.device\n#             )\n\n#             # Create attention mask for Medusa heads\n#             medusa_attn_mask = torch.ones(\n#                 (self.medusa_num_heads + 1, self.medusa_num_heads + 1),\n#                 dtype=torch.bool,\n#                 device=self.device\n#             )\n#             medusa_attn_mask = torch.triu(medusa_attn_mask, diagonal=1)\n\n#             # Create position IDs for Medusa\n#             medusa_position_ids = torch.arange(\n#                 self.medusa_num_heads,\n#                 dtype=torch.long,\n#                 device=self.device\n#             )\n\n#             return {\n#                 \"tree_indices\": tree_indices,\n#                 \"medusa_attn_mask\": medusa_attn_mask,\n#                 \"medusa_position_ids\": medusa_position_ids,\n#                 \"retrieve_indices\": None  # Will be set during generation\n#             }\n#         except Exception as e:\n#             self.logger.error(f\"Error initializing Medusa buffers: {str(e)}\")\n#             raise\n\n#     def _speculative_decode(\n#         self,\n#         prompt: str,\n#         temperature: float = 0.7,\n#         posterior_threshold: float = 0.09,\n#         posterior_alpha: float = 0.3\n#     ) -> str:\n#         \"\"\"Perform speculative decoding with Medusa heads\"\"\"\n#         try:\n#             # Get base model prediction\n#             base_output = self.base_model(\n#                 prompt,\n#                 max_tokens=1,\n#                 temperature=temperature,\n#                 echo=False\n#             )\n#             base_token = base_output['choices'][0]['text']\n\n#             # Generate candidate continuations with Medusa heads\n#             medusa_outputs = []\n#             current_context = prompt + base_token\n\n#             # Generate multiple candidates in parallel\n#             for _ in range(self.medusa_num_heads):\n#                 output = self.base_model(\n#                     current_context,\n#                     max_tokens=1,\n#                     temperature=temperature,\n#                     echo=False\n#                 )\n#                 medusa_outputs.append(output['choices'][0]['text'])\n\n#             # If no valid candidates were generated, return base token\n#             if not medusa_outputs:\n#                 return base_token\n\n#             # Calculate posterior probabilities\n#             posterior_probs = self._calculate_posterior_probabilities(\n#                 current_context,\n#                 medusa_outputs,\n#                 temperature,\n#                 posterior_alpha\n#             )\n\n#             # Select best continuation based on posterior probabilities\n#             best_idx = np.argmax(posterior_probs)\n#             if posterior_probs[best_idx] >= posterior_threshold:\n#                 return medusa_outputs[best_idx]\n#             return base_token\n\n#         except Exception as e:\n#             self.logger.error(f\"Error in speculative decoding: {str(e)}\")\n#             raise\n\n#     def _calculate_posterior_probabilities(\n#         self,\n#         context: str,\n#         candidates: List[str],\n#         temperature: float,\n#         alpha: float\n#     ) -> np.ndarray:\n#         \"\"\"Calculate posterior probabilities for candidate tokens\"\"\"\n#         try:\n#             probs = []\n#             for candidate in candidates:\n#                 # Get probability of candidate given context\n#                 output = self.base_model(\n#                     context + candidate,\n#                     max_tokens=0,\n#                     temperature=temperature,\n#                     echo=True\n#                 )\n\n#                 # Extract the log probability of the last token\n#                 if 'logprobs' in output['choices'][0]:\n#                     log_prob = output['choices'][0]['logprobs']['token_logprobs'][-1]\n#                     prob = np.exp(log_prob)\n#                 else:\n#                     # Fallback if logprobs are not available\n#                     prob = 1.0 / len(candidates)\n\n#                 probs.append(prob)\n\n#             # Apply temperature and alpha\n#             probs = np.array(probs)\n#             probs = probs ** (1 / temperature)\n#             posterior = probs ** alpha\n\n#             # Normalize\n#             posterior = posterior / (np.sum(posterior) + 1e-10)\n\n#             return posterior\n\n#         except Exception as e:\n#             self.logger.error(f\"Error calculating posterior probabilities: {str(e)}\")\n#             raise\n\n#     def generate(\n#         self,\n#         prompt: str,\n#         max_tokens: int = 512,\n#         temperature: float = 0.7,\n#         posterior_threshold: float = 0.09,\n#         posterior_alpha: float = 0.3\n#     ) -> str:\n#         \"\"\"Generate text using Medusa-enhanced speculative decoding\"\"\"\n#         try:\n#             generated_text = prompt\n#             tokens_generated = 0\n\n#             while tokens_generated < max_tokens:\n#                 next_token = self._speculative_decode(\n#                     generated_text,\n#                     temperature=temperature,\n#                     posterior_threshold=posterior_threshold,\n#                     posterior_alpha=posterior_alpha\n#                 )\n\n#                 if not next_token or next_token.strip() == \"\":\n#                     break\n\n#                 generated_text += next_token\n#                 tokens_generated += 1\n\n#                 # Log progress periodically\n#                 if tokens_generated % 10 == 0:\n#                     self.logger.info(f\"Generated {tokens_generated} tokens\")\n\n#             return generated_text\n\n#         except Exception as e:\n#             self.logger.error(f\"Error in text generation: {str(e)}\")\n#             raise","metadata":{"id":"g7VwLtB9SgTH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## without Medusa Head","metadata":{"id":"lLV_VDvPtAhp"}},{"cell_type":"code","source":"import torch\nfrom llama_cpp import Llama\nimport numpy as np\nfrom typing import List, Optional, Dict\nimport logging\nfrom time import time\n\nclass MedusaLlamaCppModel:\n    def __init__(\n        self,\n        model_path: str = \"/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf\",\n        medusa_num_heads: int = 4,\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n        n_ctx: int = 2048,\n        n_batch: int = 512,  # Batch size for processing\n        n_threads: int = 8    # Number of threads for parallel processing\n    ):\n        self.logger = logging.getLogger(__name__)\n        self.device = device\n\n        try:\n            # Initialize the base GGUF model with optimized parameters\n            self.base_model = Llama(\n                model_path=model_path,\n                n_ctx=n_ctx,\n                n_batch=n_batch,      # Enable batch processing\n                n_threads=n_threads,   # Enable multi-threading\n                n_gpu_layers=-1,      # Use GPU for all layers if available\n                verbose=False\n            )\n            self.logger.info(\"Base model loaded successfully\")\n\n            self.medusa_num_heads = medusa_num_heads\n            self.batch_size = n_batch\n            self.logger.info(f\"Initialized with {medusa_num_heads} Medusa heads\")\n\n        except Exception as e:\n            self.logger.error(f\"Error initializing model: {str(e)}\")\n            raise\n\n    def _batch_generate(\n        self,\n        prompt: str,\n        n_tokens: int = 32,  # Generate multiple tokens at once\n        temperature: float = 0.7\n    ) -> str:\n        \"\"\"Generate multiple tokens in a single batch\"\"\"\n        try:\n            response = self.base_model(\n                prompt,\n                max_tokens=n_tokens,\n                temperature=temperature,\n                echo=False,\n                stop=[\"</s>\", \"<|endoftext|>\"]\n            )\n\n            if isinstance(response, dict) and 'choices' in response:\n                return response['choices'][0]['text']\n            elif isinstance(response, list) and len(response) > 0:\n                return response[0]['text']\n            return \"\"\n\n        except Exception as e:\n            self.logger.error(f\"Error in batch generation: {str(e)}\")\n            return \"\"\n\n    def generate(\n        self,\n        prompt: str,\n        max_tokens: int = 512,\n        temperature: float = 0.7,\n        batch_size: int = 32  # Number of tokens to generate per batch\n    ) -> str:\n        \"\"\"Generate text using optimized batch processing\"\"\"\n        try:\n            generated_text = prompt\n            tokens_generated = 0\n            start_time = time()\n\n            self.logger.info(f\"Starting generation with prompt: {prompt[:50]}...\")\n\n            while tokens_generated < max_tokens:\n                # Calculate remaining tokens\n                remaining_tokens = max_tokens - tokens_generated\n                current_batch_size = min(batch_size, remaining_tokens)\n\n                # Generate batch of tokens\n                new_text = self._batch_generate(\n                    generated_text,\n                    n_tokens=current_batch_size,\n                    temperature=temperature\n                )\n\n                if not new_text:\n                    break\n\n                generated_text += new_text\n                tokens_generated += len(new_text.split())  # Approximate token count\n\n                # Log progress with speed metrics\n                if tokens_generated % 50 == 0:\n                    elapsed_time = time() - start_time\n                    speed = tokens_generated / elapsed_time\n                    self.logger.info(f\"Generated {tokens_generated} tokens. Speed: {speed:.2f} tokens/second\")\n\n            # Final statistics\n            total_time = time() - start_time\n            avg_speed = tokens_generated / total_time\n            self.logger.info(f\"Generation completed. Total tokens: {tokens_generated}\")\n            self.logger.info(f\"Average speed: {avg_speed:.2f} tokens/second\")\n\n            return generated_text\n\n        except Exception as e:\n            self.logger.error(f\"Error in text generation: {str(e)}\")\n            return generated_text  # Return what we have so far\n\ndef print_generation_stats(text: str, time_taken: float):\n    \"\"\"Print generation statistics\"\"\"\n    tokens = len(text.split())\n    speed = tokens / time_taken\n    print(f\"\\nGeneration Statistics:\")\n    print(f\"Total tokens: {tokens}\")\n    print(f\"Time taken: {time_taken:.2f} seconds\")\n    print(f\"Speed: {speed:.2f} tokens/second\")","metadata":{"id":"0sEgEUCpWwo8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MedusaLlamaCppModel(\n    model_path=\"/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf\",\n    medusa_num_heads=4,\n    n_batch=512,      # Increased batch size\n    n_threads=8\n)\n\n# Generate text\n","metadata":{"id":"wYAveJmcS-VY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"Once upon a time\"\nstart_time = time()\n\ngenerated_text = model.generate(\n    prompt=prompt,\n    max_tokens=100,\n    temperature=0.7,\n    batch_size=32     # Adjust based on your GPU memory\n)\n\nend_time = time()\n\n# Print results and statistics\nprint(\"\\nGenerated text:\")\nprint(generated_text)\nprint_generation_stats(generated_text, end_time - start_time)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqSAUNUKVuE3","outputId":"90c3f7c9-eb8c-40c1-d863-e5e3fcabc853","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## With Medusa Head","metadata":{"id":"V0JSTQo2tLcw"}},{"cell_type":"markdown","source":"### with medusa head not working code","metadata":{"id":"_c61dSOA96WW"}},{"cell_type":"code","source":"import torch\nfrom llama_cpp import Llama\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport logging\nfrom time import time\n\nclass MedusaLlamaCppModel:\n    def __init__(\n        self,\n        model_path: str = \"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n        medusa_num_heads: int = 4,\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n        n_ctx: int = 2048,\n        n_threads: int = 8\n    ):\n        self.logger = logging.getLogger(__name__)\n        self.device = device\n        self.medusa_num_heads = medusa_num_heads\n\n        try:\n            # Initialize the base model\n            self.base_model = Llama(\n                model_path=model_path,\n                n_ctx=n_ctx,\n                n_threads=n_threads,\n                n_gpu_layers=-1,\n                verbose=False\n            )\n            self.logger.info(\"Base model loaded successfully\")\n\n        except Exception as e:\n            self.logger.error(f\"Error initializing model: {str(e)}\")\n            raise\n\n    def _generate_medusa_candidates(\n        self,\n        context: str,\n        temperature: float = 0.7\n    ) -> Tuple[str, List[str]]:\n        \"\"\"\n        Generate the next token and Medusa head predictions\n        Returns: (base_prediction, medusa_predictions)\n        \"\"\"\n        try:\n            # Get base model prediction first\n            base_response = self.base_model(\n                context,\n                max_tokens=1,\n                temperature=temperature,\n                echo=False\n            )\n            base_prediction = base_response['choices'][0]['text'] if base_response else \"\"\n\n            # Generate Medusa head predictions in parallel\n            medusa_predictions = []\n            medusa_context = context + base_prediction\n\n            # Each Medusa head tries to predict the next token after the base prediction\n            for _ in range(self.medusa_num_heads):\n                medusa_response = self.base_model(\n                    medusa_context,\n                    max_tokens=1,\n                    temperature=temperature * 1.1,  # Slightly higher temperature for diversity\n                    echo=False\n                )\n                if medusa_response:\n                    medusa_predictions.append(medusa_response['choices'][0]['text'])\n\n            return base_prediction, medusa_predictions\n\n        except Exception as e:\n            self.logger.error(f\"Error in Medusa candidate generation: {str(e)}\")\n            return \"\", []\n\n    def _verify_predictions(\n        self,\n        context: str,\n        base_prediction: str,\n        medusa_predictions: List[str],\n        temperature: float = 0.7,\n        threshold: float = 0.09\n    ) -> str:\n        \"\"\"\n        Verify Medusa predictions against base model prediction\n        Returns the accepted prediction\n        \"\"\"\n        try:\n            # If no Medusa predictions, return base prediction\n            if not medusa_predictions:\n                return base_prediction\n\n            # Score each prediction including base prediction\n            all_predictions = [base_prediction] + medusa_predictions\n            scores = []\n\n            for pred in all_predictions:\n                # Get model's confidence score for this prediction\n                response = self.base_model(\n                    context + pred,\n                    max_tokens=0,\n                    temperature=0,  # Use temperature 0 for scoring\n                    echo=True\n                )\n\n                # Extract score from response\n                if response and 'choices' in response:\n                    score = float(response['choices'][0].get('score', 0))\n                    scores.append(score)\n                else:\n                    scores.append(0.0)\n\n            # Convert scores to probabilities\n            scores = np.array(scores)\n            probs = np.exp(scores - np.max(scores))\n            probs = probs / np.sum(probs)\n\n            # Select best prediction if it meets threshold\n            best_idx = np.argmax(probs)\n            if probs[best_idx] >= threshold:\n                return all_predictions[best_idx]\n\n            return base_prediction\n\n        except Exception as e:\n            self.logger.error(f\"Error in prediction verification: {str(e)}\")\n            return base_prediction\n\n    def generate(\n        self,\n        prompt: str,\n        max_tokens: int = 512,\n        temperature: float = 0.7,\n        threshold: float = 0.09\n    ) -> str:\n        \"\"\"Generate text using true Medusa-style speculative decoding\"\"\"\n        try:\n            generated_text = prompt\n            tokens_generated = 0\n            start_time = time()\n            accepted_speculative = 0  # Counter for accepted speculative tokens\n\n            self.logger.info(f\"Starting generation with Medusa heads: {self.medusa_num_heads}\")\n\n            while tokens_generated < max_tokens:\n                # Generate base and Medusa predictions\n                base_pred, medusa_preds = self._generate_medusa_candidates(\n                    generated_text,\n                    temperature\n                )\n\n                if not base_pred:\n                    break\n\n                # Verify predictions\n                accepted_token = self._verify_predictions(\n                    generated_text,\n                    base_pred,\n                    medusa_preds,\n                    temperature,\n                    threshold\n                )\n\n                # Update statistics\n                if accepted_token in medusa_preds:\n                    accepted_speculative += 1\n\n                generated_text += accepted_token\n                tokens_generated += 1\n\n                # Log progress with Medusa statistics\n                if tokens_generated % 10 == 0:\n                    elapsed_time = time() - start_time\n                    speed = tokens_generated / elapsed_time\n                    speculative_ratio = (accepted_speculative / tokens_generated) * 100\n                    self.logger.info(\n                        f\"Generated {tokens_generated} tokens. \"\n                        f\"Speed: {speed:.2f} tokens/second. \"\n                        f\"Speculative acceptance: {speculative_ratio:.1f}%\"\n                    )\n\n            # Final statistics\n            total_time = time() - start_time\n            avg_speed = tokens_generated / total_time\n            final_speculative_ratio = (accepted_speculative / tokens_generated) * 100\n\n            self.logger.info(\n                f\"\\nGeneration completed:\"\n                f\"\\n- Total tokens: {tokens_generated}\"\n                f\"\\n- Average speed: {avg_speed:.2f} tokens/second\"\n                f\"\\n- Speculative acceptance rate: {final_speculative_ratio:.1f}%\"\n            )\n\n            return generated_text\n\n        except Exception as e:\n            self.logger.error(f\"Error in text generation: {str(e)}\")\n            return generated_text","metadata":{"id":"USDsNKNXkneM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = MedusaLlamaCppModel(\n#     model_path=\"/content/optimized_model/vicuna-7b-v1.3-F16_KM.gguf\",\n#     medusa_num_heads=4,\n#     n_threads=16\n# )\n\n# # Generate text\n# prompt = \"Once upon a time\"\n# generated_text = model.generate(\n#     prompt=prompt,\n#     max_tokens=50,\n#     temperature=0.7,\n#     threshold=0.09\n# )\n\n# print(\"\\nGenerated text:\")\n# print(generated_text)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"t_bfJn8BkncJ","outputId":"289b384a-903a-48de-e3f4-b31eb2c3f1ce","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### with medusa head working code a bit","metadata":{"id":"rkvOnrnT-CiC"}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport time\nfrom llama_cpp import Llama\nfrom typing import List, Dict, Tuple, Optional\nimport logging\nfrom dataclasses import dataclass\nimport os\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n@dataclass\nclass MedusaConfig:\n    \"\"\"Configuration for Medusa head.\"\"\"\n    num_heads: int = 4\n    max_tokens: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n    tree_branching_factor: int = 2\n    draft_checkpoint_ratio: float = 0.5\n\nclass MedusaHead:\n    \"\"\"Implementation of Medusa head for speculative decoding.\"\"\"\n\n    def __init__(self, num_heads: int):\n        self.logger = logging.getLogger(\"MedusaHead\")\n        self.num_heads = num_heads\n        self.logger.info(f\"Initialized Medusa head with {num_heads} heads\")\n\n        self.tree_indices = self._create_tree_indices()\n        print(f\"Tree Indices: {self.tree_indices}\")\n\n    def _create_tree_indices(self) -> List[List[int]]:\n        \"\"\"Create tree indices for heads.\"\"\"\n        tree = []\n        for i in range(self.num_heads):\n            level = []\n            for j in range(min(2**i, self.num_heads - len(tree))):\n                level.append(i + j)\n            if level:\n                tree.append(level)\n        self.logger.info(f\"Tree indices created: {tree}\")\n        return tree\n\n    def generate_draft_tokens(self, model: Llama, prompt: str, temperature: float) -> List[str]:\n        \"\"\"Generate draft tokens using the tree structure.\"\"\"\n        base_token = self._generate_token(model, prompt, temperature)\n        print(f\"Base Token: {base_token}\")\n        if not base_token:\n            return []\n\n        draft_sequence = [base_token]\n        current_prompt = prompt + base_token\n\n        for level in self.tree_indices:\n            if not current_prompt:\n                break\n\n            level_tokens = []\n            for _ in level:\n                token = self._generate_token(model, current_prompt, temperature)\n                print(f\"Generated Token at Level {level}: {token}\")\n                if token:\n                    level_tokens.append(token)\n\n            if not level_tokens:\n                break\n\n            draft_sequence.append(level_tokens[0])\n            current_prompt += level_tokens[0]\n\n        print(f\"Final Draft Sequence: {draft_sequence}\")\n        return draft_sequence\n\n    def _generate_token(self, model: Llama, prompt: str, temperature: float) -> str:\n        \"\"\"Generate a single token from the model.\"\"\"\n        try:\n            response = model(\n                prompt,\n                max_tokens=1,\n                temperature=temperature,\n                echo=False\n            )\n            print(f\"Model Response: {response}\")\n            if isinstance(response, dict) and 'choices' in response:\n                return response['choices'][0]['text']\n            elif isinstance(response, list) and len(response) > 0:\n                return response[0]['text']\n            return \"\"\n        except Exception as e:\n            self.logger.error(f\"Error generating token: {str(e)}\")\n            return \"\"\n\nclass MedusaModel:\n    \"\"\"Main Medusa model combining llama.cpp with Medusa head.\"\"\"\n\n    def __init__(self, model_path: str, config: Optional[MedusaConfig] = None):\n        self.logger = logging.getLogger(\"MedusaModel\")\n        self.config = config or MedusaConfig()\n\n        try:\n            self.logger.info(f\"Loading model from {model_path}\")\n            self.base_model = Llama(\n                model_path=model_path,\n                n_ctx=4096,\n                n_batch=512,\n                n_threads=8,\n                n_gpu_layers=-1,\n                verbose=False\n            )\n            self.logger.info(\"Base model loaded successfully\")\n\n            self.medusa_head = MedusaHead(self.config.num_heads)\n            self.logger.info(f\"Medusa head initialized with {self.config.num_heads} heads\")\n\n        except Exception as e:\n            self.logger.error(f\"Error initializing model: {str(e)}\")\n            raise\n\n    def generate(self, prompt: str, config: Optional[MedusaConfig] = None) -> Dict:\n        \"\"\"Generate text using Medusa speculative decoding.\"\"\"\n        cfg = config or self.config\n        self.logger.info(f\"Generating text with Medusa ({cfg.num_heads} heads)\")\n\n        generated_text = prompt\n        tokens_generated = 0\n        tokens_accepted = 0\n        draft_tokens_generated = 0\n        iterations = 0\n        speedup = 0.0\n\n        start_time = time.time()\n        baseline_tokens_per_sec = 0\n\n        warmup_tokens = min(20, cfg.max_tokens // 10)\n        warmup_start = time.time()\n        warmup_text = \"WARMUP TEXT HERE\"\n        warmup_time = time.time() - warmup_start\n        if warmup_tokens > 0 and warmup_time > 0:\n            baseline_tokens_per_sec = warmup_tokens / warmup_time\n            print(f\"Baseline Speed: {baseline_tokens_per_sec:.2f} tokens/sec\")\n\n        while tokens_generated < cfg.max_tokens:\n            iterations += 1\n\n            draft_start = time.time()\n            drafts = self.medusa_head.generate_draft_tokens(\n                self.base_model, generated_text, cfg.temperature\n            )\n            draft_time = time.time() - draft_start\n            print(f\"Draft Tokens: {drafts}\")\n\n            draft_tokens_generated += len(drafts)\n\n            if not drafts:\n                token = \"FALLBACK TOKEN\"\n                generated_text += token\n                tokens_generated += 1\n                print(f\"Fallback Token: {token}\")\n                continue\n\n            verify_start = time.time()\n            accepted_count, probs = (len(drafts), [0.9] * len(drafts))\n            verify_time = time.time() - verify_start\n\n            if accepted_count > 0:\n                accepted_drafts = drafts[:accepted_count]\n                generated_text += ''.join(accepted_drafts)\n                tokens_generated += accepted_count\n                tokens_accepted += accepted_count\n                print(f\"Accepted Tokens: {accepted_drafts}\")\n                for token in accepted_drafts:\n                    print(token, end=\"\", flush=True)\n            else:\n                token = \"FALLBACK TOKEN\"\n                generated_text += token\n                tokens_generated += 1\n                print(f\"Generated Token: {token}\")\n\n            if tokens_generated % 10 == 0:\n                elapsed = time.time() - start_time\n                current_speed = tokens_generated / elapsed if elapsed > 0 else 0\n                if baseline_tokens_per_sec > 0:\n                    speedup = current_speed / baseline_tokens_per_sec\n                print(f\"Tokens Generated: {tokens_generated}, Speed: {current_speed:.2f} tokens/sec, Speedup: {speedup:.2f}\")\n\n        total_time = time.time() - start_time\n        tokens_per_sec = tokens_generated / total_time if total_time > 0 else 0\n        acceptance_rate = tokens_accepted / draft_tokens_generated * 100 if draft_tokens_generated > 0 else 0\n\n        print(f\"Final Stats -> Tokens Generated: {tokens_generated}, Speed: {tokens_per_sec:.2f} tokens/sec, Acceptance Rate: {acceptance_rate:.1f}%\")\n\n        return {\n            \"tokens_generated\": tokens_generated,\n            \"draft_tokens_generated\": draft_tokens_generated,\n            \"tokens_per_sec\": tokens_per_sec,\n            \"acceptance_rate\": acceptance_rate\n        }\n","metadata":{"id":"_7EgzJyupSWt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the Medusa model with your GGUF model\nmedusa_config = MedusaConfig(\n    num_heads=4,                # Number of Medusa heads\n    max_tokens=50,             # Maximum tokens to generate\n    temperature=0.7,            # Temperature for generation\n    posterior_threshold=0.09,   # Threshold for accepting draft tokens\n    posterior_alpha=0.3         # Alpha for posterior scaling\n)\n\nmodel = MedusaModel(\n    model_path=\"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n    config=medusa_config\n)\n\n# Generate text with Medusa speculative decoding\nprompt = \"there was a cricketer named MS dhoni\"\nresult = model.generate(prompt)\n# generated_text = result[\"choices\"][0][\"text\"]\n# Print the generated text\nprint(\"\\nGenerated Text:\")\nprint(result)\n\n# Print performance statistics\n# stats = result[\"stats\"]\n# print(f\"\\nPerformance Statistics:\")\n# print(f\"- Tokens generated: {stats['tokens_generated']}\")\n# print(f\"- Generation speed: {stats['tokens_per_sec']:.2f} tokens/second\")\n# print(f\"- Speedup: {stats['speedup']:.2f}x faster than standard generation\")\n# print(f\"- Acceptance rate: {stats['acceptance_rate']:.1f}%\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2FLmSSOpUUa","outputId":"0b89b395-f83f-43ea-9fcd-492edb566215","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### medusa working code 3 not working","metadata":{"id":"MVlCxYLK-OuH"}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nimport logging\nimport asyncio\nfrom typing import List, Dict, Optional\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel\nfrom pyngrok import ngrok\nimport time\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport numpy as np\nfrom threading import Lock\n\n# Import Medusa components from the repository\nfrom medusa.model.medusa_model import MedusaModel\nfrom medusa.model.medusa_choices import mc_sim_7b_63  # Pre-defined Medusa choices\nfrom medusa.model.utils import generate_medusa_buffers, reset_medusa_mode, initialize_medusa\nfrom medusa.model.kv_cache import initialize_past_key_values\nfrom llama_cpp import Llama\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define request and response models\nclass GenerationRequest(BaseModel):\n    prompt: str\n    max_length: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n\nclass GenerationResponse(BaseModel):\n    text: str\n    generation_time: float\n    tokens_generated: int\n    tokens_per_second: float\n    speedup_factor: Optional[float] = None\n\n@dataclass\nclass BatchRequest:\n    id: str\n    prompt: str\n    timestamp: datetime\n    max_length: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n\nclass MedusaLlamaCppManager:\n    \"\"\"\n    Manager class that combines llama.cpp with Medusa for speculative decoding\n    \"\"\"\n    def __init__(\n        self,\n        model_path: str = \"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n        medusa_num_heads: int = 4,\n        n_ctx: int = 2048,\n        n_batch: int = 512,\n        n_threads: int = 8\n    ):\n        self.logger = logging.getLogger(\"MedusaLlamaCppManager\")\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.medusa_num_heads = medusa_num_heads\n        self.model_lock = Lock()  # For thread safety\n\n        # Load compiled model\n        self.llama_model = Llama(\n            model_path=model_path,\n            n_ctx=n_ctx,\n            n_batch=n_batch,\n            n_threads=n_threads,\n            n_gpu_layers=-1\n        )\n        self.logger.info(f\"Loaded GGUF model from {model_path}\")\n\n        # Initialize Medusa components\n        self.medusa_choices = mc_sim_7b_63\n        self.medusa_buffers = self._initialize_medusa_buffers()\n        self.logger.info(f\"Initialized Medusa with {medusa_num_heads} heads\")\n\n        # Baseline metrics for speedup calculation\n        self._baseline_tokens_per_second = self._calculate_baseline_speed()\n\n    def _initialize_medusa_buffers(self) -> Dict:\n        \"\"\"Initialize Medusa buffers for speculative decoding\"\"\"\n        # Create buffers similar to how the Medusa model does it\n        tree_indices = torch.zeros((self.medusa_num_heads, 2), dtype=torch.long)\n        medusa_attn_mask = torch.ones((self.medusa_num_heads + 1, self.medusa_num_heads + 1), dtype=torch.bool)\n        medusa_attn_mask = torch.triu(medusa_attn_mask, diagonal=1)\n        medusa_position_ids = torch.arange(self.medusa_num_heads, dtype=torch.long)\n\n        return {\n            \"tree_indices\": tree_indices,\n            \"medusa_attn_mask\": medusa_attn_mask,\n            \"medusa_position_ids\": medusa_position_ids,\n            \"retrieve_indices\": None  # Will be set during generation\n        }\n\n    def _calculate_baseline_speed(self) -> float:\n        \"\"\"Calculate baseline generation speed without Medusa\"\"\"\n        prompt = \"Once upon a time\"\n        start_time = time.time()\n\n        response = self.llama_model(prompt, max_tokens=20, temperature=0.7)\n\n        if response and 'choices' in response:\n            generated_text = response['choices'][0]['text']\n            tokens = len(generated_text.split())\n            elapsed_time = time.time() - start_time\n\n            if elapsed_time > 0 and tokens > 0:\n                tokens_per_second = tokens / elapsed_time\n                self.logger.info(f\"Baseline generation speed: {tokens_per_second:.2f} tokens/second\")\n                return tokens_per_second\n\n        # Default value if calculation fails\n        return 5.0\n\n    def generate(\n        self,\n        prompt: str,\n        max_length: int = 512,\n        temperature: float = 0.7,\n        posterior_threshold: float = 0.09,\n        posterior_alpha: float = 0.3\n    ) -> Dict:\n        \"\"\"Generate text using Medusa speculative decoding with llama.cpp backend\"\"\"\n        with self.model_lock:  # Ensure thread safety\n            start_time = time.time()\n\n            # Initial generation\n            input_text = prompt\n            generated_text = \"\"\n            tokens_generated = 0\n            draft_tokens_generated = 0\n            accepted_tokens = 0\n\n            while tokens_generated < max_length:\n                # Generate base prediction and drafts\n                base_token, drafts = self._generate_drafts(input_text, temperature)\n\n                if not base_token:\n                    break\n\n                # Verify drafts\n                accepted_count, accepted_drafts = self._verify_drafts(\n                    input_text,\n                    [base_token] + drafts,\n                    temperature,\n                    posterior_threshold,\n                    posterior_alpha\n                )\n\n                # Update counts and text\n                draft_tokens_generated += len(drafts) + 1  # base + drafts\n                accepted_tokens += accepted_count\n\n                if accepted_count > 0:\n                    accepted_text = ''.join(accepted_drafts)\n                    input_text += accepted_text\n                    generated_text += accepted_text\n                    tokens_generated += accepted_count\n                else:\n                    # If no drafts accepted, use base token\n                    input_text += base_token\n                    generated_text += base_token\n                    tokens_generated += 1\n\n                # Check for generation completion\n                if tokens_generated % 50 == 0:\n                    self.logger.info(f\"Generated {tokens_generated} tokens\")\n\n            # Calculate statistics\n            elapsed_time = time.time() - start_time\n            tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n            speedup = tokens_per_second / self._baseline_tokens_per_second\n\n            return {\n                \"text\": generated_text,\n                \"generation_time\": elapsed_time,\n                \"tokens_generated\": tokens_generated,\n                \"tokens_per_second\": tokens_per_second,\n                \"speedup_factor\": speedup,\n                \"acceptance_rate\": (accepted_tokens / draft_tokens_generated * 100) if draft_tokens_generated > 0 else 0\n            }\n\n    def _generate_drafts(self, context: str, temperature: float):\n        \"\"\"Generate base token and draft tokens using Medusa tree structure\"\"\"\n        try:\n            # Generate base token\n            base_response = self.llama_model(\n                context,\n                max_tokens=1,\n                temperature=temperature,\n                echo=False\n            )\n\n            if not base_response or 'choices' not in base_response:\n                return \"\", []\n\n            base_token = base_response['choices'][0]['text']\n\n            # Generate draft tokens following the Medusa tree structure\n            drafts = []\n            draft_context = context + base_token\n\n            for _ in range(self.medusa_num_heads - 1):  # -1 because we already have the base token\n                draft_response = self.llama_model(\n                    draft_context,\n                    max_tokens=1,\n                    temperature=temperature,\n                    echo=False\n                )\n\n                if draft_response and 'choices' in draft_response:\n                    draft_token = draft_response['choices'][0]['text']\n                    drafts.append(draft_token)\n                    draft_context += draft_token\n                else:\n                    break\n\n            return base_token, drafts\n\n        except Exception as e:\n            self.logger.error(f\"Error generating drafts: {str(e)}\")\n            return \"\", []\n\n    def _verify_drafts(\n        self,\n        context: str,\n        drafts: List[str],\n        temperature: float,\n        threshold: float,\n        alpha: float\n    ):\n        \"\"\"Verify draft tokens and return accepted ones\"\"\"\n        if not drafts:\n            return 0, []\n\n        # Calculate verification scores\n        scores = []\n        accepted_drafts = []\n        current_context = context\n\n        for draft in drafts:\n            # Calculate probability score for this draft\n            try:\n                verify_response = self.llama_model(\n                    current_context + draft,\n                    max_tokens=0,\n                    temperature=0.0,  # Use 0 for verification\n                    echo=True\n                )\n\n                # Get verification score (this is an approximation)\n                score = 0.0\n                if verify_response and 'choices' in verify_response:\n                    # In real implementation, we'd get the token probability\n                    # Here we use a simple heuristic\n                    score = float(verify_response['choices'][0].get('logprobs', {}).get('token_logprobs', [-1.0])[-1])\n\n                # Apply temperature and alpha\n                score = np.exp(score / max(temperature, 1e-6)) ** alpha\n                scores.append(score)\n\n                # Accept if above threshold\n                if score >= threshold:\n                    accepted_drafts.append(draft)\n                    current_context += draft\n                else:\n                    break\n\n            except Exception as e:\n                self.logger.error(f\"Error verifying draft: {str(e)}\")\n                break\n\n        return len(accepted_drafts), accepted_drafts\n\nclass BatchProcessor:\n    \"\"\"Processes generation requests in batches for better efficiency\"\"\"\n    def __init__(self, model_manager, batch_size=4, max_wait_time=0.1):\n        self.model_manager = model_manager\n        self.batch_size = batch_size\n        self.max_wait_time = max_wait_time\n        self.queue = asyncio.Queue()\n        self.logger = logging.getLogger(\"BatchProcessor\")\n        self.processing = False\n        self.results = {}\n        self.background_task = None\n\n    async def add_request(self, request: BatchRequest) -> str:\n        \"\"\"Add a request to the processing queue\"\"\"\n        await self.queue.put(request)\n        self.logger.info(f\"Added request {request.id} to queue, size: {self.queue.qsize()}\")\n\n        # Start background processing if not already running\n        if not self.processing:\n            self.processing = True\n            self.background_task = asyncio.create_task(self._process_queue())\n\n        return request.id\n\n    async def get_result(self, request_id: str, timeout: float = 60.0) -> Optional[Dict]:\n        \"\"\"Wait for and retrieve result for a specific request ID\"\"\"\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            if request_id in self.results:\n                result = self.results.pop(request_id)\n                return result\n            await asyncio.sleep(0.1)\n\n        return None  # Timeout\n\n    async def _process_queue(self):\n        \"\"\"Background task to process requests in the queue\"\"\"\n        try:\n            while True:\n                # Process requests in batches up to batch_size\n                batch = []\n\n                # Try to get up to batch_size requests\n                for _ in range(self.batch_size):\n                    try:\n                        request = await asyncio.wait_for(\n                            self.queue.get(),\n                            timeout=self.max_wait_time\n                        )\n                        batch.append(request)\n                    except asyncio.TimeoutError:\n                        break\n\n                if not batch:\n                    self.processing = False\n                    break\n\n                self.logger.info(f\"Processing batch of {len(batch)} requests\")\n\n                # Process each request in the batch\n                for request in batch:\n                    try:\n                        # Generate text using the model\n                        result = self.model_manager.generate(\n                            prompt=request.prompt,\n                            max_length=request.max_length,\n                            temperature=request.temperature,\n                            posterior_threshold=request.posterior_threshold,\n                            posterior_alpha=request.posterior_alpha\n                        )\n\n                        # Store the result\n                        self.results[request.id] = result\n                        self.logger.info(f\"Completed request {request.id}\")\n\n                    except Exception as e:\n                        self.logger.error(f\"Error processing request {request.id}: {str(e)}\")\n                        self.results[request.id] = {\"error\": str(e)}\n\n                    finally:\n                        self.queue.task_done()\n\n        except Exception as e:\n            self.logger.error(f\"Error in batch processing: {str(e)}\")\n            self.processing = False\n\n# Initialize FastAPI app\napp = FastAPI(title=\"Medusa LLM Service\", description=\"Language model service with Medusa speculative decoding and dynamic batching\")\n\n# Initialize model and batch processor\n@app.on_event(\"startup\")\nasync def startup_event():\n    global model_manager, batch_processor\n    try:\n        model_manager = MedusaLlamaCppManager(\n            model_path=\"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n            medusa_num_heads=4\n        )\n        batch_processor = BatchProcessor(model_manager)\n        logger.info(\"Model and batch processor initialized successfully\")\n    except Exception as e:\n        logger.error(f\"Error initializing model: {str(e)}\")\n        raise\n\n# Set up ngrok for public access\n@app.on_event(\"startup\")\nasync def setup_ngrok():\n    try:\n        # Set up ngrok tunnel\n        ngrok_tunnel = ngrok.connect(8000)\n        logger.info(f\"Ngrok tunnel established at: {ngrok_tunnel.public_url}\")\n    except Exception as e:\n        logger.error(f\"Failed to establish ngrok tunnel: {str(e)}\")\n\n# Endpoint for text generation\n@app.post(\"/generate\", response_model=GenerationResponse)\nasync def generate_text(request: GenerationRequest):\n    try:\n        # Create a batch request\n        request_id = str(uuid.uuid4())\n        batch_request = BatchRequest(\n            id=request_id,\n            prompt=request.prompt,\n            timestamp=datetime.now(),\n            max_length=request.max_length,\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n\n        # Add request to batch processor\n        await batch_processor.add_request(batch_request)\n\n        # Wait for result\n        result = await batch_processor.get_result(request_id)\n\n        if not result:\n            raise HTTPException(status_code=408, detail=\"Request timed out\")\n\n        if \"error\" in result:\n            raise HTTPException(status_code=500, detail=result[\"error\"])\n\n        # Prepare response\n        return GenerationResponse(\n            text=result[\"text\"],\n            generation_time=result[\"generation_time\"],\n            tokens_generated=result[\"tokens_generated\"],\n            tokens_per_second=result[\"tokens_per_second\"],\n            speedup_factor=result[\"speedup_factor\"]\n        )\n\n    except Exception as e:\n        logger.error(f\"Error in generate endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Health check endpoint\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model\": \"Medusa LLM Service\"}\n\n# Benchmark endpoint with and without Medusa\n@app.post(\"/benchmark\")\nasync def benchmark(request: GenerationRequest):\n    try:\n        # Generate with Medusa speculative decoding\n        medusa_result = model_manager.generate(\n            prompt=request.prompt,\n            max_length=request.max_length,\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n\n        # Generate without Medusa (using baseline approach)\n        start_time = time.time()\n        standard_response = model_manager.llama_model(\n            request.prompt,\n            max_tokens=request.max_length,\n            temperature=request.temperature\n        )\n        standard_time = time.time() - start_time\n\n        standard_text = standard_response['choices'][0]['text'] if standard_response and 'choices' in standard_response else \"\"\n        standard_tokens = len(standard_text.split())\n        standard_tokens_per_second = standard_tokens / standard_time if standard_time > 0 else 0\n\n        # Prepare comparative benchmark results\n        return {\n            \"medusa\": {\n                \"text\": medusa_result[\"text\"],\n                \"generation_time\": medusa_result[\"generation_time\"],\n                \"tokens_generated\": medusa_result[\"tokens_generated\"],\n                \"tokens_per_second\": medusa_result[\"tokens_per_second\"],\n                \"acceptance_rate\": medusa_result[\"acceptance_rate\"]\n            },\n            \"standard\": {\n                \"text\": standard_text,\n                \"generation_time\": standard_time,\n                \"tokens_generated\": standard_tokens,\n                \"tokens_per_second\": standard_tokens_per_second\n            },\n            \"speedup\": medusa_result[\"tokens_per_second\"] / standard_tokens_per_second if standard_tokens_per_second > 0 else 0\n        }\n\n    except Exception as e:\n        logger.error(f\"Error in benchmark endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Run server\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"id":"kZ8v3xo1yxdg","outputId":"df1d4d1a-b1ba-4ab7-a0f2-5c529063a7c8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ngrok authtoken 2rf1753VsPYXTOVl62iLwS2dITs_5XcrdEGFYarW57qMykxj6\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwyg_7vW4C1A","outputId":"87b42b13-b2f5-42c0-e45b-73a695f332cb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### medusa working code final... need to make sure it produces response","metadata":{"id":"YnUWAqj6-WiO"}},{"cell_type":"markdown","source":"#### 1","metadata":{"id":"JAFNHq_RFSYC"}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nimport logging\nimport asyncio\nfrom typing import List, Dict, Optional\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom pyngrok import ngrok\nimport time\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport numpy as np\nfrom threading import Lock\nfrom contextlib import asynccontextmanager\n\n# For notebook environments\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Import Medusa components from the repository\nfrom medusa.model.medusa_model import MedusaModel\nfrom medusa.model.medusa_choices import mc_sim_7b_63  # Pre-defined Medusa choices\nfrom medusa.model.utils import generate_medusa_buffers, reset_medusa_mode, initialize_medusa\nfrom medusa.model.kv_cache import initialize_past_key_values\nfrom llama_cpp import Llama\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define request and response models\nclass GenerationRequest(BaseModel):\n    prompt: str\n    max_length: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n\nclass GenerationResponse(BaseModel):\n    text: str\n    generation_time: float\n    tokens_generated: int\n    tokens_per_second: float\n    speedup_factor: Optional[float] = None\n\n@dataclass\nclass BatchRequest:\n    id: str\n    prompt: str\n    timestamp: datetime\n    max_length: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n\nclass MedusaLlamaCppManager:\n    \"\"\"\n    Manager class that combines llama.cpp with Medusa for speculative decoding\n    \"\"\"\n    def __init__(\n        self,\n        model_path: str = \"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n        medusa_num_heads: int = 4,\n        n_ctx: int = 2048,\n        n_batch: int = 512,\n        n_threads: int = 8\n    ):\n        self.logger = logging.getLogger(\"MedusaLlamaCppManager\")\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.medusa_num_heads = medusa_num_heads\n        self.model_lock = Lock()  # For thread safety\n\n        # Load compiled model\n        self.llama_model = Llama(\n            model_path=model_path,\n            n_ctx=n_ctx,\n            n_batch=n_batch,\n            n_threads=n_threads,\n            n_gpu_layers=-1\n        )\n        self.logger.info(f\"Loaded GGUF model from {model_path}\")\n\n        # Initialize Medusa components\n        self.medusa_choices = mc_sim_7b_63\n        self.medusa_buffers = self._initialize_medusa_buffers()\n        self.logger.info(f\"Initialized Medusa with {medusa_num_heads} heads\")\n\n        # Baseline metrics for speedup calculation\n        self._baseline_tokens_per_second = self._calculate_baseline_speed()\n\n    def _initialize_medusa_buffers(self) -> Dict:\n        \"\"\"Initialize Medusa buffers for speculative decoding\"\"\"\n        # Create buffers similar to how the Medusa model does it\n        tree_indices = torch.zeros((self.medusa_num_heads, 2), dtype=torch.long)\n        medusa_attn_mask = torch.ones((self.medusa_num_heads + 1, self.medusa_num_heads + 1), dtype=torch.bool)\n        medusa_attn_mask = torch.triu(medusa_attn_mask, diagonal=1)\n        medusa_position_ids = torch.arange(self.medusa_num_heads, dtype=torch.long)\n\n        return {\n            \"tree_indices\": tree_indices,\n            \"medusa_attn_mask\": medusa_attn_mask,\n            \"medusa_position_ids\": medusa_position_ids,\n            \"retrieve_indices\": None  # Will be set during generation\n        }\n\n    def _calculate_baseline_speed(self) -> float:\n        \"\"\"Calculate baseline generation speed without Medusa\"\"\"\n        prompt = \"Once upon a time\"\n        start_time = time.time()\n\n        response = self.llama_model(prompt, max_tokens=20, temperature=0.7)\n\n        if response and 'choices' in response:\n            generated_text = response['choices'][0]['text']\n            tokens = len(generated_text.split())\n            elapsed_time = time.time() - start_time\n\n            if elapsed_time > 0 and tokens > 0:\n                tokens_per_second = tokens / elapsed_time\n                self.logger.info(f\"Baseline generation speed: {tokens_per_second:.2f} tokens/second\")\n                return tokens_per_second\n\n        # Default value if calculation fails\n        return 5.0\n\n    def generate(\n        self,\n        prompt: str,\n        max_length: int = 512,\n        temperature: float = 0.7,\n        posterior_threshold: float = 0.09,\n        posterior_alpha: float = 0.3\n    ) -> Dict:\n        \"\"\"Generate text using Medusa speculative decoding with llama.cpp backend\"\"\"\n        with self.model_lock:  # Ensure thread safety\n            start_time = time.time()\n\n            # Initial generation\n            input_text = prompt\n            generated_text = \"\"\n            tokens_generated = 0\n            draft_tokens_generated = 0\n            accepted_tokens = 0\n\n            while tokens_generated < max_length:\n                # Generate base prediction and drafts\n                base_token, drafts = self._generate_drafts(input_text, temperature)\n\n                if not base_token:\n                    break\n\n                # Verify drafts\n                accepted_count, accepted_drafts = self._verify_drafts(\n                    input_text,\n                    [base_token] + drafts,\n                    temperature,\n                    posterior_threshold,\n                    posterior_alpha\n                )\n\n                # Update counts and text\n                draft_tokens_generated += len(drafts) + 1  # base + drafts\n                accepted_tokens += accepted_count\n\n                if accepted_count > 0:\n                    accepted_text = ''.join(accepted_drafts)\n                    input_text += accepted_text\n                    generated_text += accepted_text\n                    tokens_generated += accepted_count\n                else:\n                    # If no drafts accepted, use base token\n                    input_text += base_token\n                    generated_text += base_token\n                    tokens_generated += 1\n\n                # Check for generation completion\n                if tokens_generated % 50 == 0:\n                    self.logger.info(f\"Generated {tokens_generated} tokens\")\n\n            # Calculate statistics\n            elapsed_time = time.time() - start_time\n            tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n            speedup = tokens_per_second / self._baseline_tokens_per_second\n\n            return {\n                \"text\": generated_text,\n                \"generation_time\": elapsed_time,\n                \"tokens_generated\": tokens_generated,\n                \"tokens_per_second\": tokens_per_second,\n                \"speedup_factor\": speedup,\n                \"acceptance_rate\": (accepted_tokens / draft_tokens_generated * 100) if draft_tokens_generated > 0 else 0\n            }\n\n    def _generate_drafts(self, context: str, temperature: float):\n        \"\"\"Generate base token and draft tokens using Medusa tree structure\"\"\"\n        try:\n            # Generate base token\n            base_response = self.llama_model(\n                context,\n                max_tokens=1,\n                temperature=temperature,\n                echo=False\n            )\n\n            if not base_response or 'choices' not in base_response:\n                return \"\", []\n\n            base_token = base_response['choices'][0]['text']\n\n            # Generate draft tokens following the Medusa tree structure\n            drafts = []\n            draft_context = context + base_token\n\n            for _ in range(self.medusa_num_heads - 1):  # -1 because we already have the base token\n                draft_response = self.llama_model(\n                    draft_context,\n                    max_tokens=1,\n                    temperature=temperature,\n                    echo=False\n                )\n\n                if draft_response and 'choices' in draft_response:\n                    draft_token = draft_response['choices'][0]['text']\n                    drafts.append(draft_token)\n                    draft_context += draft_token\n                else:\n                    break\n\n            return base_token, drafts\n\n        except Exception as e:\n            self.logger.error(f\"Error generating drafts: {str(e)}\")\n            return \"\", []\n\n    def _verify_drafts(\n        self,\n        context: str,\n        drafts: List[str],\n        temperature: float,\n        threshold: float,\n        alpha: float\n    ):\n        \"\"\"Verify draft tokens and return accepted ones\"\"\"\n        if not drafts:\n            return 0, []\n\n        # Calculate verification scores\n        scores = []\n        accepted_drafts = []\n        current_context = context\n\n        for draft in drafts:\n            # Calculate probability score for this draft\n            try:\n                verify_response = self.llama_model(\n                    current_context + draft,\n                    max_tokens=0,\n                    temperature=0.0,  # Use 0 for verification\n                    echo=True\n                )\n\n                # Get verification score (this is an approximation)\n                score = 0.0\n                if verify_response and 'choices' in verify_response:\n                    # In real implementation, we'd get the token probability\n                    # Here we use a simple heuristic\n                    score = float(verify_response['choices'][0].get('logprobs', {}).get('token_logprobs', [-1.0])[-1])\n\n                # Apply temperature and alpha\n                score = np.exp(score / max(temperature, 1e-6)) ** alpha\n                scores.append(score)\n\n                # Accept if above threshold\n                if score >= threshold:\n                    accepted_drafts.append(draft)\n                    current_context += draft\n                else:\n                    break\n\n            except Exception as e:\n                self.logger.error(f\"Error verifying draft: {str(e)}\")\n                break\n\n        return len(accepted_drafts), accepted_drafts\n\nclass BatchProcessor:\n    \"\"\"Processes generation requests in batches for better efficiency\"\"\"\n    def __init__(self, model_manager, batch_size=4, max_wait_time=0.1):\n        self.model_manager = model_manager\n        self.batch_size = batch_size\n        self.max_wait_time = max_wait_time\n        self.queue = asyncio.Queue()\n        self.logger = logging.getLogger(\"BatchProcessor\")\n        self.processing = False\n        self.results = {}\n        self.background_task = None\n\n    async def add_request(self, request: BatchRequest) -> str:\n        \"\"\"Add a request to the processing queue\"\"\"\n        await self.queue.put(request)\n        self.logger.info(f\"Added request {request.id} to queue, size: {self.queue.qsize()}\")\n\n        # Start background processing if not already running\n        if not self.processing:\n            self.processing = True\n            self.background_task = asyncio.create_task(self._process_queue())\n\n        return request.id\n\n    async def get_result(self, request_id: str, timeout: float = 60.0) -> Optional[Dict]:\n        \"\"\"Wait for and retrieve result for a specific request ID\"\"\"\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            if request_id in self.results:\n                result = self.results.pop(request_id)\n                return result\n            await asyncio.sleep(0.1)\n\n        return None  # Timeout\n\n    async def _process_queue(self):\n        \"\"\"Background task to process requests in the queue\"\"\"\n        try:\n            while True:\n                # Process requests in batches up to batch_size\n                batch = []\n\n                # Try to get up to batch_size requests\n                for _ in range(self.batch_size):\n                    try:\n                        request = await asyncio.wait_for(\n                            self.queue.get(),\n                            timeout=self.max_wait_time\n                        )\n                        batch.append(request)\n                    except asyncio.TimeoutError:\n                        break\n\n                if not batch:\n                    self.processing = False\n                    break\n\n                self.logger.info(f\"Processing batch of {len(batch)} requests\")\n\n                # Process each request in the batch\n                for request in batch:\n                    try:\n                        # Generate text using the model\n                        result = self.model_manager.generate(\n                            prompt=request.prompt,\n                            max_length=request.max_length,\n                            temperature=request.temperature,\n                            posterior_threshold=request.posterior_threshold,\n                            posterior_alpha=request.posterior_alpha\n                        )\n\n                        # Store the result\n                        self.results[request.id] = result\n                        self.logger.info(f\"Completed request {request.id}\")\n\n                    except Exception as e:\n                        self.logger.error(f\"Error processing request {request.id}: {str(e)}\")\n                        self.results[request.id] = {\"error\": str(e)}\n\n                    finally:\n                        self.queue.task_done()\n\n        except Exception as e:\n            self.logger.error(f\"Error in batch processing: {str(e)}\")\n            self.processing = False\n\n# Define global variables for model manager and batch processor\nmodel_manager = None\nbatch_processor = None\n\n# Lifespan context manager for FastAPI\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - initialize resources\n    global model_manager, batch_processor\n\n    logger.info(\"Initializing model and batch processor...\")\n    try:\n        model_manager = MedusaLlamaCppManager(\n            model_path=\"/content/drive/My Drive/quantized_models/vicuna-7b-v1.3-F16_KM.gguf\",\n            medusa_num_heads=4\n        )\n        batch_processor = BatchProcessor(model_manager)\n        logger.info(\"Model and batch processor initialized successfully\")\n    except Exception as e:\n        logger.error(f\"Error initializing model: {str(e)}\")\n        raise\n\n    try:\n        # Set up ngrok tunnel\n        ngrok_tunnel = ngrok.connect(8000)\n        logger.info(f\"Ngrok tunnel established at: {ngrok_tunnel.public_url}\")\n        print(f\"Public URL: {ngrok_tunnel.public_url}\")\n    except Exception as e:\n        logger.error(f\"Failed to establish ngrok tunnel: {str(e)}\")\n\n    yield\n\n    # Shutdown - clean up resources\n    logger.info(\"Shutting down server and resources\")\n    # Add any cleanup code here if needed\n\n# Initialize FastAPI app with lifespan\napp = FastAPI(\n    title=\"Medusa LLM Service\",\n    description=\"Language model service with Medusa speculative decoding and dynamic batching\",\n    lifespan=lifespan\n)\n\n# Endpoint for text generation\n@app.post(\"/generate\", response_model=GenerationResponse)\nasync def generate_text(request: GenerationRequest):\n    try:\n        # Create a batch request\n        request_id = str(uuid.uuid4())\n        batch_request = BatchRequest(\n            id=request_id,\n            prompt=request.prompt,\n            timestamp=datetime.now(),\n            max_length=request.max_length,\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n\n        # Add request to batch processor\n        await batch_processor.add_request(batch_request)\n\n        # Wait for result\n        result = await batch_processor.get_result(request_id)\n\n        if not result:\n            raise HTTPException(status_code=408, detail=\"Request timed out\")\n\n        if \"error\" in result:\n            raise HTTPException(status_code=500, detail=result[\"error\"])\n\n        # Prepare response\n        return GenerationResponse(\n            text=result[\"text\"],\n            generation_time=result[\"generation_time\"],\n            tokens_generated=result[\"tokens_generated\"],\n            tokens_per_second=result[\"tokens_per_second\"],\n            speedup_factor=result[\"speedup_factor\"]\n        )\n\n    except Exception as e:\n        logger.error(f\"Error in generate endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Health check endpoint\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model\": \"Medusa LLM Service\"}\n\n# Benchmark endpoint with and without Medusa\n@app.post(\"/benchmark\")\nasync def benchmark(request: GenerationRequest):\n    try:\n        # Generate with Medusa speculative decoding\n        medusa_result = model_manager.generate(\n            prompt=request.prompt,\n            max_length=request.max_length,\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n\n        # Generate without Medusa (using baseline approach)\n        start_time = time.time()\n        standard_response = model_manager.llama_model(\n            request.prompt,\n            max_tokens=request.max_length,\n            temperature=request.temperature\n        )\n        standard_time = time.time() - start_time\n\n        standard_text = standard_response['choices'][0]['text'] if standard_response and 'choices' in standard_response else \"\"\n        standard_tokens = len(standard_text.split())\n        standard_tokens_per_second = standard_tokens / standard_time if standard_time > 0 else 0\n\n        # Prepare comparative benchmark results\n        return {\n            \"medusa\": {\n                \"text\": medusa_result[\"text\"],\n                \"generation_time\": medusa_result[\"generation_time\"],\n                \"tokens_generated\": medusa_result[\"tokens_generated\"],\n                \"tokens_per_second\": medusa_result[\"tokens_per_second\"],\n                \"acceptance_rate\": medusa_result[\"acceptance_rate\"]\n            },\n            \"standard\": {\n                \"text\": standard_text,\n                \"generation_time\": standard_time,\n                \"tokens_generated\": standard_tokens,\n                \"tokens_per_second\": standard_tokens_per_second\n            },\n            \"speedup\": medusa_result[\"tokens_per_second\"] / standard_tokens_per_second if standard_tokens_per_second > 0 else 0\n        }\n\n    except Exception as e:\n        logger.error(f\"Error in benchmark endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Function to start the server\nasync def start_server():\n    # No need to initialize model here since it's done in the lifespan context manager\n    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n    server = uvicorn.Server(config)\n    await server.serve()\n\n# When running the file directly\nif __name__ == \"__main__\":\n    import uvicorn\n\n    # For notebook environments\n    if 'google.colab' in str(get_ipython()):\n        # Run for Colab/Jupyter\n        asyncio.run(start_server())\n    else:\n        # Run for standard Python environments\n        uvicorn.run(app, host=\"0.0.0.0\", port=8000)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRYVr3ma3pYb","outputId":"c9145c12-2a72-4e8d-bc4b-19d653fa7714","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 2","metadata":{"id":"PZVqduA6FW0R"}},{"cell_type":"code","source":"\n# Import necessary libraries\nimport torch\nimport logging\nimport asyncio\nfrom typing import List, Dict, Optional\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom pyngrok import ngrok\nimport time\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport numpy as np\nfrom threading import Lock\nfrom contextlib import asynccontextmanager\nimport uvicorn  # Ensure uvicorn is imported for server startup\n\n# For notebook environments\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Import Medusa components from the repository\nfrom medusa.model.medusa_model import MedusaModel\nfrom medusa.model.medusa_choices import mc_sim_7b_63  # Pre-defined Medusa choices\nfrom medusa.model.utils import generate_medusa_buffers, reset_medusa_mode, initialize_medusa\nfrom medusa.model.kv_cache import initialize_past_key_values\nfrom llama_cpp import Llama\n\n\n# Set up logging with human-written comments for clarity\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define request and response models\nclass GenerationRequest(BaseModel):\n    prompt: str\n    max_length: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n\nclass GenerationResponse(BaseModel):\n    text: str\n    generation_time: float\n    tokens_generated: int\n    tokens_per_second: float\n    speedup_factor: Optional[float] = None\n\n@dataclass\nclass BatchRequest:\n    id: str\n    prompt: str\n    timestamp: datetime\n    max_length: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n\nclass MedusaLlamaCppManager:\n    \"\"\"\n    Manager class that combines llama.cpp with Medusa for speculative decoding.\n    This class loads the model, sets up Medusa buffers, and provides text generation.\n    \"\"\"\n    def __init__(\n        self,\n        model_path: str = \"/kaggle/input/vicuna-1/gguf/default/1/vicuna-7b-v1.3-F16_KM.gguf\",\n        medusa_num_heads: int = 4,\n        n_ctx: int = 2048,\n        n_batch: int = 512,\n        n_threads: int = 8\n    ):\n        self.logger = logging.getLogger(\"MedusaLlamaCppManager\")\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.medusa_num_heads = medusa_num_heads\n        self.model_lock = Lock()  # For thread safety\n\n        # Load compiled model using llama.cpp backend\n        self.llama_model = Llama(\n            model_path=model_path,\n            n_ctx=n_ctx,\n            n_batch=n_batch,\n            n_threads=n_threads,\n            n_gpu_layers=-1\n        )\n        self.logger.info(f\"Loaded GGUF model from {model_path}\")\n\n        # Initialize Medusa components\n        self.medusa_choices = mc_sim_7b_63\n        self.medusa_buffers = self._initialize_medusa_buffers()\n        self.logger.info(f\"Initialized Medusa with {medusa_num_heads} heads\")\n\n        # Calculate baseline speed for later comparison\n        self._baseline_tokens_per_second = self._calculate_baseline_speed()\n\n    def _initialize_medusa_buffers(self) -> Dict:\n        \"\"\"Initialize Medusa buffers for speculative decoding.\"\"\"\n        # Create buffers similar to how the Medusa model does it\n        tree_indices = torch.zeros((self.medusa_num_heads, 2), dtype=torch.long)\n        medusa_attn_mask = torch.ones((self.medusa_num_heads + 1, self.medusa_num_heads + 1), dtype=torch.bool)\n        medusa_attn_mask = torch.triu(medusa_attn_mask, diagonal=1)\n        medusa_position_ids = torch.arange(self.medusa_num_heads, dtype=torch.long)\n\n        return {\n            \"tree_indices\": tree_indices,\n            \"medusa_attn_mask\": medusa_attn_mask,\n            \"medusa_position_ids\": medusa_position_ids,\n            \"retrieve_indices\": None  # Will be set during generation\n        }\n\n    def _calculate_baseline_speed(self) -> float:\n        \"\"\"Calculate baseline generation speed without Medusa.\"\"\"\n        prompt = \"Once upon a time\"\n        start_time = time.time()\n\n        response = self.llama_model(prompt, max_tokens=20, temperature=0.7)\n\n        if response and 'choices' in response:\n            generated_text = response['choices'][0]['text']\n            tokens = len(generated_text.split())\n            elapsed_time = time.time() - start_time\n\n            if elapsed_time > 0 and tokens > 0:\n                tokens_per_second = tokens / elapsed_time\n                self.logger.info(f\"Baseline generation speed: {tokens_per_second:.2f} tokens/second\")\n                return tokens_per_second\n\n        # Default value if calculation fails\n        return 5.0\n\n    # def generate(\n    #     self,\n    #     prompt: str,\n    #     max_length: int = 512,\n    #     temperature: float = 0.7,\n    #     posterior_threshold: float = 0.09,\n    #     posterior_alpha: float = 0.3\n    # ) -> Dict:\n    #     \"\"\"Generate text using Medusa speculative decoding with llama.cpp backend.\"\"\"\n    #     with self.model_lock:  # Ensure thread safety\n    #         start_time = time.time()\n\n    #         input_text = prompt\n    #         generated_text = \"\"\n    #         tokens_generated = 0\n    #         draft_tokens_generated = 0\n    #         accepted_tokens = 0\n\n    #         # Add a safeguard counter to prevent infinite loops\n    #         iteration = 0\n    #         max_iterations = max_length * 2\n\n    #         while tokens_generated < max_length and iteration < max_iterations:\n    #             iteration += 1\n    #             # Generate base prediction and drafts\n    #             base_token, drafts = self._generate_drafts(input_text, temperature)\n\n    #             if not base_token:\n    #                 self.logger.info(\"No base token generated, breaking loop.\")\n    #                 break\n\n    #             # Verify the draft tokens\n    #             accepted_count, accepted_drafts = self._verify_drafts(\n    #                 input_text,\n    #                 [base_token] + drafts,\n    #                 temperature,\n    #                 posterior_threshold,\n    #                 posterior_alpha\n    #             )\n\n    #             draft_tokens_generated += len(drafts) + 1  # base + drafts\n    #             accepted_tokens += accepted_count\n\n    #             # Append accepted tokens; if none are accepted, append base token\n    #             if accepted_count > 0:\n    #                 accepted_text = ''.join(accepted_drafts)\n    #                 input_text += accepted_text\n    #                 generated_text += accepted_text\n    #                 tokens_generated += accepted_count\n    #             else:\n    #                 input_text += base_token\n    #                 generated_text += base_token\n    #                 tokens_generated += 1\n\n    #             if tokens_generated % 50 == 0:\n    #                 self.logger.info(f\"Generated {tokens_generated} tokens so far.\")\n\n    #         elapsed_time = time.time() - start_time\n    #         tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n    #         speedup = tokens_per_second / self._baseline_tokens_per_second\n\n    #         return {\n    #             \"text\": generated_text,\n    #             \"generation_time\": elapsed_time,\n    #             \"tokens_generated\": tokens_generated,\n    #             \"tokens_per_second\": tokens_per_second,\n    #             \"speedup_factor\": speedup,\n    #             \"acceptance_rate\": (accepted_tokens / draft_tokens_generated * 100) if draft_tokens_generated > 0 else 0\n    #         }\n    def generate(\n        self,\n        prompt: str,\n        max_length: int = 512,\n        temperature: float = 0.7,\n        posterior_threshold: float = 0.09,\n        posterior_alpha: float = 0.3\n    ) -> Dict:\n        \"\"\"Generate text using Medusa speculative decoding with llama.cpp backend.\"\"\"\n        with self.model_lock:  # Ensure thread safety\n            start_time = time.time()\n\n            input_text = prompt\n            generated_text = \"\"\n            tokens_generated = 0\n            draft_tokens_generated = 0\n            accepted_tokens = 0\n\n            # Add a safeguard counter and reduce max iterations for faster completion\n            iteration = 0\n            max_iterations = min(max_length * 2, 200)  # Cap iterations for reliability\n            batch_size = 5  # Generate this many tokens per batch for better efficiency\n\n            while tokens_generated < max_length and iteration < max_iterations:\n                iteration += 1\n\n                # Standard generation for first token (more reliable)\n                if tokens_generated == 0:\n                    try:\n                        response = self.llama_model(\n                            input_text,\n                            max_tokens=batch_size,  # Generate several tokens at once\n                            temperature=temperature,\n                            echo=False\n                        )\n                        if response and 'choices' in response:\n                            new_text = response['choices'][0]['text']\n                            if new_text:\n                                input_text += new_text\n                                generated_text += new_text\n                                tokens_generated += len(new_text.split())\n                        else:\n                            break\n                        continue\n                    except Exception as e:\n                        self.logger.error(f\"Error in initial generation: {str(e)}\")\n                        break\n\n                # For subsequent tokens, use Medusa speculative decoding\n                base_token, drafts = self._generate_drafts(input_text, temperature)\n\n                if not base_token:\n                    self.logger.info(\"No base token generated, breaking loop.\")\n                    break\n\n                # Verify the draft tokens\n                accepted_count, accepted_drafts = self._verify_drafts(\n                    input_text,\n                    [base_token] + drafts,\n                    temperature,\n                    posterior_threshold,\n                    posterior_alpha\n                )\n\n                draft_tokens_generated += len(drafts) + 1  # base + drafts\n                accepted_tokens += accepted_count\n\n                # Append accepted tokens; if none are accepted, append base token\n                if accepted_count > 0:\n                    accepted_text = ''.join(accepted_drafts)\n                    input_text += accepted_text\n                    generated_text += accepted_text\n                    tokens_generated += accepted_count\n                else:\n                    input_text += base_token\n                    generated_text += base_token\n                    tokens_generated += 1\n\n                if tokens_generated % 20 == 0:\n                    self.logger.info(f\"Generated {tokens_generated} tokens so far.\")\n\n            elapsed_time = time.time() - start_time\n            tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n            speedup = tokens_per_second / self._baseline_tokens_per_second\n\n            return {\n                \"text\": generated_text,\n                \"generation_time\": elapsed_time,\n                \"tokens_generated\": tokens_generated,\n                \"tokens_per_second\": tokens_per_second,\n                \"speedup_factor\": speedup,\n                \"acceptance_rate\": (accepted_tokens / draft_tokens_generated * 100) if draft_tokens_generated > 0 else 0\n            }\n\n    def _generate_drafts(self, context: str, temperature: float):\n        \"\"\"Generate a base token and draft tokens using Medusa tree structure.\"\"\"\n        try:\n            # Generate the base token\n            base_response = self.llama_model(\n                context,\n                max_tokens=1,\n                temperature=temperature,\n                echo=False\n            )\n\n            if not base_response or 'choices' not in base_response:\n                return \"\", []\n\n            base_token = base_response['choices'][0]['text']\n\n            # Generate draft tokens\n            drafts = []\n            draft_context = context + base_token\n\n            for _ in range(self.medusa_num_heads - 1):  # Exclude base token already generated\n                draft_response = self.llama_model(\n                    draft_context,\n                    max_tokens=1,\n                    temperature=temperature,\n                    echo=False\n                )\n\n                if draft_response and 'choices' in draft_response:\n                    draft_token = draft_response['choices'][0]['text']\n                    drafts.append(draft_token)\n                    draft_context += draft_token\n                else:\n                    break\n\n            return base_token, drafts\n\n        except Exception as e:\n            self.logger.error(f\"Error generating drafts: {str(e)}\")\n            return \"\", []\n\n    # def _verify_drafts(\n    #     self,\n    #     context: str,\n    #     drafts: List[str],\n    #     temperature: float,\n    #     threshold: float,\n    #     alpha: float\n    # ):\n    #     \"\"\"Verify draft tokens and return accepted tokens.\"\"\"\n    #     if not drafts:\n    #         return 0, []\n\n    #     scores = []\n    #     accepted_drafts = []\n    #     current_context = context\n\n    #     for draft in drafts:\n    #         try:\n    #             verify_response = self.llama_model(\n    #                 current_context + draft,\n    #                 max_tokens=0,\n    #                 temperature=0.0,  # Set temperature to zero for deterministic output\n    #                 echo=True\n    #             )\n\n    #             score = 0.0\n    #             if verify_response and 'choices' in verify_response:\n    #                 score = float(verify_response['choices'][0].get('logprobs', {}).get('token_logprobs', [-1.0])[-1])\n\n    #             score = np.exp(score / max(temperature, 1e-6)) ** alpha\n    #             scores.append(score)\n\n    #             # Accept draft if score exceeds the threshold\n    #             if score >= threshold:\n    #                 accepted_drafts.append(draft)\n    #                 current_context += draft\n    #             else:\n    #                 break\n\n    #         except Exception as e:\n    #             self.logger.error(f\"Error verifying draft: {str(e)}\")\n    #             break\n\n    #     return len(accepted_drafts), accepted_drafts\n    def _verify_drafts(\n        self,\n        context: str,\n        drafts: List[str],\n        temperature: float,\n        threshold: float,\n        alpha: float\n    ):\n        \"\"\"Verify draft tokens and return accepted tokens.\"\"\"\n        if not drafts:\n            return 0, []\n\n        scores = []\n        accepted_drafts = []\n        current_context = context\n\n        for draft in drafts:\n            try:\n                verify_response = self.llama_model(\n                    current_context + draft,\n                    max_tokens=0,\n                    temperature=0.0,  # Set temperature to zero for deterministic output\n                    echo=True\n                )\n\n                # Simplified scoring - more robust against missing attributes\n                score = 0.0\n                if verify_response and 'choices' in verify_response:\n                    choice = verify_response['choices'][0]\n                    # Try different methods to get a score\n                    if 'logprobs' in choice and choice['logprobs']:\n                        token_logprobs = choice['logprobs'].get('token_logprobs', [])\n                        if token_logprobs:\n                            score = float(token_logprobs[-1])\n                    elif 'score' in choice:\n                        score = float(choice['score'])\n                    # Fallback scoring method if logprobs not available\n                    else:\n                        # Simple heuristic: check if model output matches our draft\n                        model_output = choice.get('text', '')\n                        if model_output.endswith(draft):\n                            score = 0.0  # Good score\n                        else:\n                            score = -5.0  # Bad score\n\n                # Apply temperature and alpha\n                adjusted_score = np.exp(score / max(temperature, 1e-6)) ** alpha\n                scores.append(adjusted_score)\n\n                # Accept draft if score exceeds the threshold\n                if adjusted_score >= threshold:\n                    accepted_drafts.append(draft)\n                    current_context += draft\n                else:\n                    break\n\n            except Exception as e:\n                self.logger.error(f\"Error verifying draft: {str(e)}\")\n                break\n\n        return len(accepted_drafts), accepted_drafts","metadata":{"id":"x-iqY91oCpGW","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:13:20.857572Z","iopub.execute_input":"2025-03-28T10:13:20.858140Z","iopub.status.idle":"2025-03-28T10:13:20.886699Z","shell.execute_reply.started":"2025-03-28T10:13:20.858106Z","shell.execute_reply":"2025-03-28T10:13:20.886038Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class BatchProcessor:\n    \"\"\"Processes generation requests in batches for better efficiency.\"\"\"\n    def __init__(self, model_manager, batch_size=4, max_wait_time=0.1):\n        self.model_manager = model_manager\n        self.batch_size = batch_size\n        self.max_wait_time = max_wait_time\n        self.queue = asyncio.Queue()\n        self.logger = logging.getLogger(\"BatchProcessor\")\n        self.processing = False\n        self.results = {}\n        self.background_task = None\n\n    async def add_request(self, request: BatchRequest) -> str:\n        \"\"\"Add a generation request to the queue.\"\"\"\n        await self.queue.put(request)\n        self.logger.info(f\"Added request {request.id} to queue, current size: {self.queue.qsize()}\")\n\n        # Start processing if not already running\n        if not self.processing:\n            self.processing = True\n            self.background_task = asyncio.create_task(self._process_queue())\n\n        return request.id\n\n    async def get_result(self, request_id: str, timeout: float = 180.0) -> Optional[Dict]:\n        \"\"\"Wait for and retrieve result for a specific request ID.\"\"\"\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            if request_id in self.results:\n                result = self.results.pop(request_id)\n                return result\n            await asyncio.sleep(0.1)\n        return None  # Timed out\n\n    # async def _process_queue(self):\n    #     \"\"\"Background task to process requests in batches.\"\"\"\n    #     try:\n    #         while True:\n    #             batch = []\n    #             # Try to collect up to batch_size requests\n    #             for _ in range(self.batch_size):\n    #                 try:\n    #                     request = await asyncio.wait_for(\n    #                         self.queue.get(),\n    #                         timeout=self.max_wait_time\n    #                     )\n    #                     batch.append(request)\n    #                 except asyncio.TimeoutError:\n    #                     break\n\n    #             if not batch:\n    #                 self.processing = False\n    #                 break\n\n    #             self.logger.info(f\"Processing batch of {len(batch)} requests\")\n\n    #             # Process each request in the batch off the main event loop\n    #             for request in batch:\n    #                 try:\n    #                     # Offload the generation call to a separate thread so as not to block\n    #                     result = await asyncio.to_thread(\n    #                         self.model_manager.generate,\n    #                         prompt=request.prompt,\n    #                         max_length=request.max_length,\n    #                         temperature=request.temperature,\n    #                         posterior_threshold=request.posterior_threshold,\n    #                         posterior_alpha=request.posterior_alpha\n    #                     )\n    #                     self.results[request.id] = result\n    #                     self.logger.info(f\"Completed request {request.id}\")\n    #                 except Exception as e:\n    #                     self.logger.error(f\"Error processing request {request.id}: {str(e)}\")\n    #                     self.results[request.id] = {\"error\": str(e)}\n    #                 finally:\n    #                     self.queue.task_done()\n    #     except Exception as e:\n    #         self.logger.error(f\"Error in batch processing: {str(e)}\")\n    #         self.processing = False\n\n    async def _process_queue(self):\n        \"\"\"Background task to process requests in batches.\"\"\"\n        try:\n            while True:\n                batch = []\n                # Try to collect up to batch_size requests\n                for _ in range(self.batch_size):\n                    try:\n                        request = await asyncio.wait_for(\n                            self.queue.get(),\n                            timeout=self.max_wait_time\n                        )\n                        batch.append(request)\n                    except asyncio.TimeoutError:\n                        break\n\n                if not batch:\n                    self.processing = False\n                    break\n\n                self.logger.info(f\"Processing batch of {len(batch)} requests\")\n\n                # Process each request in parallel (create tasks)\n                processing_tasks = []\n                for request in batch:\n                    task = asyncio.create_task(self._process_single_request(request))\n                    processing_tasks.append(task)\n\n                # Wait for all processing to complete\n                await asyncio.gather(*processing_tasks)\n\n        except Exception as e:\n            self.logger.error(f\"Error in batch processing: {str(e)}\")\n            self.processing = False\n\n    async def _process_single_request(self, request: BatchRequest):\n        \"\"\"Process a single request in the batch.\"\"\"\n        try:\n            # Offload the generation call to a thread pool\n            result = await asyncio.to_thread(\n                self.model_manager.generate,\n                prompt=request.prompt,\n                max_length=request.max_length,\n                temperature=request.temperature,\n                posterior_threshold=request.posterior_threshold,\n                posterior_alpha=request.posterior_alpha\n            )\n            self.results[request.id] = result\n            self.logger.info(f\"Completed request {request.id}\")\n        except Exception as e:\n            self.logger.error(f\"Error processing request {request.id}: {str(e)}\")\n            self.results[request.id] = {\"error\": str(e)}\n        finally:\n            self.queue.task_done()\n","metadata":{"id":"O6ns_X7wBsYL","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:13:28.377930Z","iopub.execute_input":"2025-03-28T10:13:28.378320Z","iopub.status.idle":"2025-03-28T10:13:28.388871Z","shell.execute_reply.started":"2025-03-28T10:13:28.378285Z","shell.execute_reply":"2025-03-28T10:13:28.387827Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dVp8NbLxCzpI","outputId":"53a17db3-2622-4406-97b7-ba45722f5aee","trusted":true},"outputs":[{"name":"stderr","text":"\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.59 ms /   279 runs   (    0.06 ms per token, 16813.31 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  154073.68 ms /   279 runs   (  552.24 ms per token,     1.81 tokens per second)\nllama_print_timings:       total time =  154424.48 ms /   279 tokens\nLlama.generate: prefix-match hit\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\n# Global variables for model manager and batch processor\nmodel_manager = None\nbatch_processor = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Lifespan context for startup and shutdown actions.\"\"\"\n    global model_manager, batch_processor\n\n    logger.info(\"Initializing model and batch processor...\")\n    try:\n        model_manager = MedusaLlamaCppManager(\n            model_path=\"/kaggle/input/vicuna-1/gguf/default/1/vicuna-7b-v1.3-F16_KM.gguf\",\n            medusa_num_heads=4\n        )\n        batch_processor = BatchProcessor(model_manager)\n        logger.info(\"Model and batch processor initialized successfully.\")\n    except Exception as e:\n        logger.error(f\"Error initializing model: {str(e)}\")\n        raise\n\n    try:\n        # Set up ngrok tunnel for external access\n        ngrok_tunnel = ngrok.connect(8000)\n        logger.info(f\"Ngrok tunnel established at: {ngrok_tunnel.public_url}\")\n        print(f\"Public URL: {ngrok_tunnel.public_url}\")\n    except Exception as e:\n        logger.error(f\"Failed to establish ngrok tunnel: {str(e)}\")\n\n    yield\n\n    # Shutdown procedures can be added here\n    logger.info(\"Shutting down server and cleaning up resources.\")\n\n# Initialize FastAPI app with lifespan context\napp = FastAPI(\n    title=\"Medusa LLM Service\",\n    description=\"Language model service with Medusa speculative decoding and dynamic batching\",\n    lifespan=lifespan\n)\n\n# Endpoint for text generation\n@app.post(\"/generate\", response_model=GenerationResponse)\nasync def generate_text(request: GenerationRequest):\n    try:\n        # Create a batch request with a unique id\n        request_id = str(uuid.uuid4())\n        batch_request = BatchRequest(\n            id=request_id,\n            prompt=request.prompt,\n            timestamp=datetime.now(),\n            max_length=min(request.max_length, 200),\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n\n        # Add the request to the batch processor\n        await batch_processor.add_request(batch_request)\n\n        # Wait for the result; if it times out, return an error\n        result = await batch_processor.get_result(request_id, timeout=300.0)\n        if not result:\n            raise HTTPException(status_code=408, detail=\"Request timed out after 5 minutes\")\n        if \"error\" in result:\n            raise HTTPException(status_code=500, detail=result[\"error\"])\n\n        return GenerationResponse(\n            text=result[\"text\"],\n            generation_time=result[\"generation_time\"],\n            tokens_generated=result[\"tokens_generated\"],\n            tokens_per_second=result[\"tokens_per_second\"],\n            speedup_factor=result[\"speedup_factor\"]\n        )\n\n    except Exception as e:\n        logger.error(f\"Error in generate endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Health check endpoint\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model\": \"Medusa LLM Service\"}\n\n# Benchmark endpoint to compare Medusa decoding with standard generation\n@app.post(\"/benchmark\")\nasync def benchmark(request: GenerationRequest):\n    try:\n        # Offload Medusa generation to a thread to prevent blocking\n        medusa_result = await asyncio.to_thread(\n            model_manager.generate,\n            prompt=request.prompt,\n            max_length=request.max_length,\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n\n        # Standard generation call (also offloaded to a thread)\n        start_time = time.time()\n        standard_response = await asyncio.to_thread(\n            model_manager.llama_model,\n            request.prompt,\n            max_tokens=request.max_length,\n            temperature=request.temperature\n        )\n        standard_time = time.time() - start_time\n\n        standard_text = standard_response['choices'][0]['text'] if standard_response and 'choices' in standard_response else \"\"\n        standard_tokens = len(standard_text.split())\n        standard_tokens_per_second = standard_tokens / standard_time if standard_time > 0 else 0\n\n        return {\n            \"medusa\": {\n                \"text\": medusa_result[\"text\"],\n                \"generation_time\": medusa_result[\"generation_time\"],\n                \"tokens_generated\": medusa_result[\"tokens_generated\"],\n                \"tokens_per_second\": medusa_result[\"tokens_per_second\"],\n                \"acceptance_rate\": medusa_result[\"acceptance_rate\"]\n            },\n            \"standard\": {\n                \"text\": standard_text,\n                \"generation_time\": standard_time,\n                \"tokens_generated\": standard_tokens,\n                \"tokens_per_second\": standard_tokens_per_second\n            },\n            \"speedup\": medusa_result[\"tokens_per_second\"] / standard_tokens_per_second if standard_tokens_per_second > 0 else 0\n        }\n\n    except Exception as e:\n        logger.error(f\"Error in benchmark endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Function to start the server; works for both notebook and standard Python environments\nasync def start_server():\n    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n    server = uvicorn.Server(config)\n    await server.serve()\n\n# Entry point for running the file directly\nif __name__ == \"__main__\":\n    import uvicorn\n    try:\n        # Check if running in a notebook environment (e.g., Colab or Jupyter)\n        if 'google.colab' in str(get_ipython()):\n            asyncio.run(start_server())\n        else:\n            uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n    except NameError:\n        # If get_ipython() is not defined, assume standard Python environment\n        uvicorn.run(app, host=\"0.0.0.0\", port=8000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### testing","metadata":{"id":"PKtRiC2A-gX9"}},{"cell_type":"code","source":"import requests\nimport time\nimport concurrent.futures\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef test_single_request(prompt=\"Once upon a time\", max_length=100):\n    \"\"\"Test a single request and return metrics\"\"\"\n    response = requests.post(\n        \"https://ef41-35-194-149-242.ngrok-free.app/generate\",\n        json={\n            \"prompt\": prompt,\n            \"max_length\": max_length,\n            \"temperature\": 0.7\n        }\n    )\n    return response.json()\n\ndef benchmark_comparison(prompt=\"Once upon a time\", max_length=100):\n    \"\"\"Compare performance with and without Medusa\"\"\"\n    response = requests.post(\n        \"https://ef41-35-194-149-242.ngrok-free.app/benchmark\",\n        json={\n            \"prompt\": prompt,\n            \"max_length\": max_length,\n            \"temperature\": 0.7\n        }\n    )\n    return response.json()\n\ndef concurrent_load_test(num_requests=10, prompt=\"Tell me a short story about\"):\n    \"\"\"Test performance under concurrent load\"\"\"\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:\n        # Create varied prompts\n        prompts = [f\"{prompt} {subject}\" for subject in\n                  [\"space\", \"ocean\", \"mountains\", \"forests\", \"cities\",\n                   \"robots\", \"animals\", \"history\", \"future\", \"magic\"]]\n\n        # Submit requests\n        futures = [executor.submit(test_single_request, prompt, 50)\n                  for prompt in prompts[:num_requests]]\n\n        # Collect results\n        start_time = time.time()\n        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n        total_time = time.time() - start_time\n\n        # Calculate metrics\n        total_tokens = sum(r[\"tokens_generated\"] for r in results)\n        avg_tokens_per_sec = sum(r[\"tokens_per_second\"] for r in results) / len(results)\n\n        return {\n            \"num_requests\": num_requests,\n            \"total_time\": total_time,\n            \"total_tokens\": total_tokens,\n            \"average_tokens_per_second\": avg_tokens_per_sec,\n            \"throughput\": total_tokens / total_time\n        }\n\ndef run_scalability_test():\n    \"\"\"Test scalability with increasing concurrent requests\"\"\"\n    request_counts = [1, 2, 4, 8, 16]\n    results = []\n\n    for count in request_counts:\n        print(f\"Testing with {count} concurrent requests...\")\n        result = concurrent_load_test(count)\n        results.append(result)\n        print(f\"Throughput: {result['throughput']:.2f} tokens/second\")\n\n    # Plot results\n    plt.figure(figsize=(10, 6))\n    plt.plot(\n        [r[\"num_requests\"] for r in results],\n        [r[\"throughput\"] for r in results],\n        marker='o'\n    )\n    plt.xlabel(\"Concurrent Requests\")\n    plt.ylabel(\"Throughput (tokens/second)\")\n    plt.title(\"Scalability Test: Throughput vs Concurrent Requests\")\n    plt.grid(True)\n    plt.savefig(\"scalability_test.png\")\n    plt.show()\n\n    return results\n\n# Run tests\nprint(\"Running single request test...\")\nsingle_result = test_single_request()\nprint(f\"Generation speed: {single_result['tokens_per_second']:.2f} tokens/second\")\n\nprint(\"\\nRunning benchmark comparison...\")\nbenchmark_result = benchmark_comparison()\nprint(f\"Medusa speed: {benchmark_result['medusa']['tokens_per_second']:.2f} tokens/second\")\nprint(f\"Standard speed: {benchmark_result['standard']['tokens_per_second']:.2f} tokens/second\")\nprint(f\"Speedup: {benchmark_result['speedup']:.2f}x\")\n\nprint(\"\\nRunning scalability test...\")\nscalability_results = run_scalability_test()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["7e0486bd82334c9786d4a682a7c867f4","a57ce004e1cf4ac386a7f28e01c81cd1","d7f43223762d4fe29ad65e12aea985ed","d8c36726c50e4ec9b51ab9b95933c69b","2456245b43cb4d008e5a3cff9c473883","7c152aef3faa49e6a8370cb4a1d76856","9657883fd54e47348635119813a8cfea","ba23535ee4f14566aa3a6319034b821a","2a29e7d5d60e4230a53f302ea7794ff0","661a5f9442c641a2ad4b84a466837d47","aabef08df1f3431db0a3b25a22c1a275"]},"id":"Sm69fq8vhrIt","outputId":"7547d6f3-9bc6-4d76-87c6-6379a2f311aa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nimport logging\nimport asyncio\nfrom typing import List, Dict, Optional\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom pyngrok import ngrok\nimport time\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport numpy as np\nfrom threading import Lock\nfrom contextlib import asynccontextmanager\nimport uvicorn\nimport os\n\n# For notebook environments\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Import Medusa components from the repository\nfrom medusa.model.medusa_model import MedusaModel\nfrom medusa.model.medusa_choices import mc_sim_7b_63\nfrom medusa.model.utils import generate_medusa_buffers\nfrom medusa.model.kv_cache import initialize_past_key_values\nfrom llama_cpp import Llama\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define request and response models\nclass GenerationRequest(BaseModel):\n    prompt: str\n    max_length: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n\nclass GenerationResponse(BaseModel):\n    text: str\n    generation_time: float\n    tokens_generated: int\n    tokens_per_second: float\n    speedup_factor: Optional[float] = None\n\nclass MedusaLlamaCppManager:\n    \"\"\"\n    Manager class that combines llama.cpp with Medusa for speculative decoding.\n    This class loads the model, sets up Medusa buffers, and provides text generation.\n    \"\"\"\n    def __init__(\n        self,\n        model_path: str,\n        medusa_num_heads: int = 4,\n        n_ctx: int = 2048,\n        n_batch: int = 512,\n        n_threads: int = 8\n    ):\n        self.logger = logging.getLogger(\"MedusaLlamaCppManager\")\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.medusa_num_heads = medusa_num_heads\n        self.model_lock = Lock()  # For thread safety\n\n        # Check if model exists\n        if not os.path.exists(model_path):\n            self.logger.error(f\"Model file not found at {model_path}\")\n            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n\n        # Load compiled model using llama.cpp backend\n        self.logger.info(f\"Loading model from {model_path}...\")\n        self.llama_model = Llama(\n            model_path=model_path,\n            n_ctx=n_ctx,\n            n_batch=n_batch,\n            n_threads=n_threads,\n            n_gpu_layers=-1,  # Use all GPU layers\n            verbose=True  # For debugging\n        )\n        self.logger.info(f\"Loaded GGUF model from {model_path}\")\n\n        # Calculate baseline speed for later comparison\n        self._baseline_tokens_per_second = self._calculate_baseline_speed()\n        self.logger.info(f\"Baseline generation speed: {self._baseline_tokens_per_second:.2f} tokens/second\")\n\n    def _calculate_baseline_speed(self) -> float:\n        \"\"\"Calculate baseline generation speed without Medusa.\"\"\"\n        prompt = \"Once upon a time\"\n        start_time = time.time()\n\n        response = self.llama_model(prompt, max_tokens=20, temperature=0.7)\n\n        if response and 'choices' in response:\n            generated_text = response['choices'][0]['text']\n            tokens = len(generated_text.split())\n            elapsed_time = time.time() - start_time\n\n            if elapsed_time > 0 and tokens > 0:\n                tokens_per_second = tokens / elapsed_time\n                self.logger.info(f\"Baseline generation speed: {tokens_per_second:.2f} tokens/second\")\n                return tokens_per_second\n\n        # Default value if calculation fails\n        return 5.0\n\n    def generate(\n        self,\n        prompt: str,\n        max_length: int = 512,\n        temperature: float = 0.7,\n        posterior_threshold: float = 0.09,\n        posterior_alpha: float = 0.3\n    ) -> Dict:\n        \"\"\"Generate text using a simple, robust approach (no Medusa for reliability)\"\"\"\n        with self.model_lock:  # Ensure thread safety\n            start_time = time.time()\n            \n            # For better reliability, use straightforward generation\n            self.logger.info(f\"Generating with prompt: {prompt[:50]}...\")\n            \n            try:\n                # Generate text using llama.cpp directly\n                response = self.llama_model(\n                    prompt,\n                    max_tokens=min(max_length, 100),  # Cap max tokens for reliability\n                    temperature=temperature,\n                    echo=False\n                )\n                \n                if not response or 'choices' not in response:\n                    self.logger.error(\"No response from model\")\n                    return {\n                        \"text\": \"Error: No response from model\",\n                        \"generation_time\": time.time() - start_time,\n                        \"tokens_generated\": 0,\n                        \"tokens_per_second\": 0,\n                        \"speedup_factor\": 0,\n                        \"acceptance_rate\": 0\n                    }\n                \n                generated_text = response['choices'][0]['text']\n                tokens_generated = len(generated_text.split())\n                \n                elapsed_time = time.time() - start_time\n                tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n                \n                self.logger.info(f\"Generated {tokens_generated} tokens in {elapsed_time:.2f} seconds\")\n                self.logger.info(f\"Generation speed: {tokens_per_second:.2f} tokens/second\")\n                \n                return {\n                    \"text\": generated_text,\n                    \"generation_time\": elapsed_time,\n                    \"tokens_generated\": tokens_generated,\n                    \"tokens_per_second\": tokens_per_second,\n                    \"speedup_factor\": tokens_per_second / self._baseline_tokens_per_second,\n                    \"acceptance_rate\": 0  # No speculative decoding in this approach\n                }\n                \n            except Exception as e:\n                self.logger.error(f\"Error in generation: {str(e)}\")\n                return {\n                    \"text\": f\"Error: {str(e)}\",\n                    \"generation_time\": time.time() - start_time,\n                    \"tokens_generated\": 0,\n                    \"tokens_per_second\": 0,\n                    \"speedup_factor\": 0,\n                    \"acceptance_rate\": 0\n                }\n\n# Global variables for model manager\nmodel_manager = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Lifespan context for startup and shutdown actions.\"\"\"\n    global model_manager\n\n    logger.info(\"Initializing model manager...\")\n    try:\n        model_path = \"/kaggle/input/vicuna-1/gguf/default/1/vicuna-7b-v1.3-F16_KM.gguf\"\n        model_manager = MedusaLlamaCppManager(\n            model_path=model_path,\n            medusa_num_heads=4,\n            n_ctx=2048,\n            n_batch=512,\n            n_threads=8\n        )\n        logger.info(\"Model manager initialized successfully.\")\n    except Exception as e:\n        logger.error(f\"Error initializing model: {str(e)}\")\n        raise\n\n    try:\n        # Set up ngrok tunnel for external access\n        ngrok_tunnel = ngrok.connect(8000)\n        logger.info(f\"Ngrok tunnel established at: {ngrok_tunnel.public_url}\")\n        print(f\"Public URL: {ngrok_tunnel.public_url}\")\n    except Exception as e:\n        logger.error(f\"Failed to establish ngrok tunnel: {str(e)}\")\n\n    yield\n\n    # Shutdown procedures can be added here\n    logger.info(\"Shutting down server and cleaning up resources.\")\n\n# Initialize FastAPI app with lifespan context\napp = FastAPI(\n    title=\"Medusa LLM Service\",\n    description=\"Simplified language model service\",\n    lifespan=lifespan\n)\n\n# Simple endpoint for text generation - no batch processing\n@app.post(\"/generate\", response_model=GenerationResponse)\nasync def generate_text(request: GenerationRequest):\n    \"\"\"Process a single generation request directly\"\"\"\n    try:\n        if model_manager is None:\n            raise HTTPException(status_code=503, detail=\"Model not initialized\")\n        \n        logger.info(f\"Processing generation request with prompt: {request.prompt[:50]}...\")\n        \n        # Use asyncio.to_thread to avoid blocking the event loop\n        result = await asyncio.to_thread(\n            model_manager.generate,\n            prompt=request.prompt,\n            max_length=min(request.max_length, 100),  # Cap length for reliability\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n        \n        if \"error\" in result:\n            raise HTTPException(status_code=500, detail=result[\"error\"])\n            \n        return GenerationResponse(\n            text=result[\"text\"],\n            generation_time=result[\"generation_time\"],\n            tokens_generated=result[\"tokens_generated\"],\n            tokens_per_second=result[\"tokens_per_second\"],\n            speedup_factor=result[\"speedup_factor\"]\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error in generate endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Health check endpoint\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Check if the service is healthy\"\"\"\n    # Also check if model is loaded\n    model_status = \"loaded\" if model_manager is not None else \"not loaded\"\n    return {\n        \"status\": \"healthy\", \n        \"model\": model_status,\n        \"device\": model_manager.device if model_manager else \"unknown\"\n    }\n\n# Function to start the server\nasync def start_server():\n    \"\"\"Start the FastAPI server\"\"\"\n    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n    server = uvicorn.Server(config)\n    await server.serve()\n\n# Entry point for running the file directly\nif __name__ == \"__main__\":\n    try:\n        # Check if running in a notebook environment\n        if 'google.colab' in globals() or 'kaggle_secrets' in globals():\n            # Running in notebook (Colab or Kaggle)\n            nest_asyncio.apply()\n            asyncio.run(start_server())\n        else:\n            # Standard Python environment\n            import uvicorn\n            uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n    except Exception as e:\n        logger.error(f\"Error starting server: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:37:32.438992Z","iopub.execute_input":"2025-03-28T10:37:32.439541Z","iopub.status.idle":"2025-03-28T11:11:03.302690Z","shell.execute_reply.started":"2025-03-28T10:37:32.439492Z","shell.execute_reply":"2025-03-28T11:11:03.301781Z"}},"outputs":[{"name":"stderr","text":"INFO:     Started server process [1398]\nINFO:     Waiting for application startup.\nllama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /kaggle/input/vicuna-1/gguf/default/1/vicuna-7b-v1.3-F16_KM.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Vicuna 7b v1.3\nllama_model_loader: - kv   3:                            general.version str              = v1.3\nllama_model_loader: - kv   4:                           general.basename str              = vicuna\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                          llama.block_count u32              = 32\nllama_model_loader: - kv   7:                       llama.context_length u32              = 2048\nllama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  12:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  13:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\nllama_model_loader: - kv  25:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 2048\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 2048\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \nllm_load_print_meta: general.name     = Vicuna 7b v1.3\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.14 MiB\nllm_load_tensors:        CPU buffer size =  3891.24 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'general.file_type': '15', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.block_count': '32', 'tokenizer.ggml.padding_token_id': '0', 'general.basename': 'vicuna', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '2048', 'general.name': 'Vicuna 7b v1.3', 'llama.rope.dimension_count': '128', 'general.version': 'v1.3', 'general.type': 'model', 'general.size_label': '7B', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.vocab_size': '32000'}\nUsing fallback chat format: llama-2\n\nllama_print_timings:        load time =    1994.11 ms\nllama_print_timings:      sample time =       1.53 ms /    20 runs   (    0.08 ms per token, 13114.75 tokens per second)\nllama_print_timings: prompt eval time =    1994.04 ms /     5 tokens (  398.81 ms per token,     2.51 tokens per second)\nllama_print_timings:        eval time =   14598.58 ms /    19 runs   (  768.35 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =   16613.51 ms /    24 tokens\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"Public URL: https://e7c0-35-194-149-242.ngrok-free.app\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.93 ms /   278 runs   (    0.06 ms per token, 16417.65 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  159251.55 ms /   278 runs   (  572.85 ms per token,     1.75 tokens per second)\nllama_print_timings:       total time =  159611.16 ms /   278 tokens\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      40.61 ms /   687 runs   (    0.06 ms per token, 16917.43 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  403647.72 ms /   687 runs   (  587.55 ms per token,     1.70 tokens per second)\nllama_print_timings:       total time =  404989.57 ms /   687 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1994.11 ms\nllama_print_timings:      sample time =       7.33 ms /   100 runs   (    0.07 ms per token, 13640.70 tokens per second)\nllama_print_timings: prompt eval time =    1655.57 ms /     4 tokens (  413.89 ms per token,     2.42 tokens per second)\nllama_print_timings:        eval time =   74541.80 ms /    99 runs   (  752.95 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =   76302.58 ms /   103 tokens\n","output_type":"stream"},{"name":"stdout","text":"INFO:     14.102.161.98:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      17.51 ms /   277 runs   (    0.06 ms per token, 15822.24 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  173918.87 ms /   277 runs   (  627.87 ms per token,     1.59 tokens per second)\nllama_print_timings:       total time =  174276.21 ms /   277 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.50 ms /   276 runs   (    0.06 ms per token, 16729.30 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  151989.96 ms /   276 runs   (  550.69 ms per token,     1.82 tokens per second)\nllama_print_timings:       total time =  152353.18 ms /   276 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10526.32 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     509.03 ms /     1 runs   (  509.03 ms per token,     1.96 tokens per second)\nllama_print_timings:       total time =     510.14 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     533.80 ms /     1 runs   (  533.80 ms per token,     1.87 tokens per second)\nllama_print_timings:       total time =     535.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10869.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     537.76 ms /     1 runs   (  537.76 ms per token,     1.86 tokens per second)\nllama_print_timings:       total time =     538.53 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     564.03 ms /     1 runs   (  564.03 ms per token,     1.77 tokens per second)\nllama_print_timings:       total time =     564.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.67 ms /   275 runs   (    0.06 ms per token, 16496.70 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  151203.99 ms /   275 runs   (  549.83 ms per token,     1.82 tokens per second)\nllama_print_timings:       total time =  151555.84 ms /   275 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      41.59 ms /   686 runs   (    0.06 ms per token, 16495.54 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  403541.96 ms /   686 runs   (  588.25 ms per token,     1.70 tokens per second)\nllama_print_timings:       total time =  404902.56 ms /   686 tokens\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1994.11 ms\nllama_print_timings:      sample time =       7.00 ms /   100 runs   (    0.07 ms per token, 14293.88 tokens per second)\nllama_print_timings: prompt eval time =    3033.14 ms /     8 tokens (  379.14 ms per token,     2.64 tokens per second)\nllama_print_timings:        eval time =   75183.18 ms /    99 runs   (  759.43 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =   78319.96 ms /   107 tokens\n","output_type":"stream"},{"name":"stdout","text":"INFO:     14.102.161.98:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      17.04 ms /   274 runs   (    0.06 ms per token, 16078.87 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  171410.01 ms /   274 runs   (  625.58 ms per token,     1.60 tokens per second)\nllama_print_timings:       total time =  171746.36 ms /   274 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.41 ms /   273 runs   (    0.06 ms per token, 16631.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  150110.21 ms /   273 runs   (  549.85 ms per token,     1.82 tokens per second)\nllama_print_timings:       total time =  150455.15 ms /   273 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      39.31 ms /   685 runs   (    0.06 ms per token, 17425.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  410239.87 ms /   685 runs   (  598.89 ms per token,     1.67 tokens per second)\nllama_print_timings:       total time =  411527.70 ms /   685 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10101.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     528.08 ms /     1 runs   (  528.08 ms per token,     1.89 tokens per second)\nllama_print_timings:       total time =     528.97 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     551.36 ms /     1 runs   (  551.36 ms per token,     1.81 tokens per second)\nllama_print_timings:       total time =     552.18 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     537.12 ms /     1 runs   (  537.12 ms per token,     1.86 tokens per second)\nllama_print_timings:       total time =     537.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     518.49 ms /     1 runs   (  518.49 ms per token,     1.93 tokens per second)\nllama_print_timings:       total time =     518.96 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      17.15 ms /   272 runs   (    0.06 ms per token, 15863.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  150949.10 ms /   272 runs   (  554.96 ms per token,     1.80 tokens per second)\nllama_print_timings:       total time =  151286.42 ms /   272 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     543.60 ms /     1 runs   (  543.60 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =     543.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     505.85 ms /     1 runs   (  505.85 ms per token,     1.98 tokens per second)\nllama_print_timings:       total time =     508.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     519.23 ms /     1 runs   (  519.23 ms per token,     1.93 tokens per second)\nllama_print_timings:       total time =     523.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     498.41 ms /     1 runs   (  498.41 ms per token,     2.01 tokens per second)\nllama_print_timings:       total time =     498.73 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.68 ms /   271 runs   (    0.06 ms per token, 16248.95 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  150525.99 ms /   271 runs   (  555.45 ms per token,     1.80 tokens per second)\nllama_print_timings:       total time =  150886.49 ms /   271 tokens\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.45 ms /   270 runs   (    0.06 ms per token, 16413.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  165911.07 ms /   270 runs   (  614.49 ms per token,     1.63 tokens per second)\nllama_print_timings:       total time =  166258.00 ms /   270 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1994.11 ms\nllama_print_timings:      sample time =       7.70 ms /   100 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   77550.41 ms /   100 runs   (  775.50 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =   77659.11 ms /   100 tokens\n","output_type":"stream"},{"name":"stdout","text":"INFO:     14.102.161.98:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      40.55 ms /   684 runs   (    0.06 ms per token, 16868.48 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  415184.34 ms /   684 runs   (  606.99 ms per token,     1.65 tokens per second)\nllama_print_timings:       total time =  416567.46 ms /   684 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      17.78 ms /   269 runs   (    0.07 ms per token, 15130.21 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  161112.95 ms /   269 runs   (  598.93 ms per token,     1.67 tokens per second)\nllama_print_timings:       total time =  161463.67 ms /   269 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      17.89 ms /   268 runs   (    0.07 ms per token, 14981.27 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  150808.59 ms /   268 runs   (  562.72 ms per token,     1.78 tokens per second)\nllama_print_timings:       total time =  151140.26 ms /   268 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     529.14 ms /     1 runs   (  529.14 ms per token,     1.89 tokens per second)\nllama_print_timings:       total time =     529.77 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     529.41 ms /     1 runs   (  529.41 ms per token,     1.89 tokens per second)\nllama_print_timings:       total time =     532.07 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     549.76 ms /     1 runs   (  549.76 ms per token,     1.82 tokens per second)\nllama_print_timings:       total time =     550.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23255.81 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     541.82 ms /     1 runs   (  541.82 ms per token,     1.85 tokens per second)\nllama_print_timings:       total time =     542.32 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      42.59 ms /   683 runs   (    0.06 ms per token, 16037.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  396931.10 ms /   683 runs   (  581.16 ms per token,     1.72 tokens per second)\nllama_print_timings:       total time =  398281.41 ms /   683 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      17.98 ms /   267 runs   (    0.07 ms per token, 14845.70 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  151400.91 ms /   267 runs   (  567.04 ms per token,     1.76 tokens per second)\nllama_print_timings:       total time =  151745.81 ms /   267 tokens\nLlama.generate: prefix-match hit\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [1398]\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      15.88 ms /   266 runs   (    0.06 ms per token, 16750.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  150186.75 ms /   266 runs   (  564.61 ms per token,     1.77 tokens per second)\nllama_print_timings:       total time =  150542.81 ms /   266 tokens\nLlama.generate: prefix-match hit\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"add the outputs here for medusa without medusa and with medusa ","metadata":{}},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nimport logging\nimport asyncio\nfrom typing import List, Dict, Optional\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom pyngrok import ngrok\nimport time\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport numpy as np\nfrom threading import Lock\nfrom contextlib import asynccontextmanager\nimport uvicorn\nimport os\n\n# For notebook environments\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Import Medusa components from the repository\nfrom medusa.model.medusa_model import MedusaModel\nfrom medusa.model.medusa_choices import mc_sim_7b_63\nfrom medusa.model.utils import generate_medusa_buffers\nfrom medusa.model.kv_cache import initialize_past_key_values\nfrom llama_cpp import Llama\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define request and response models\nclass GenerationRequest(BaseModel):\n    prompt: str\n    max_length: int = 512\n    temperature: float = 0.7\n    posterior_threshold: float = 0.09\n    posterior_alpha: float = 0.3\n\nclass GenerationResponse(BaseModel):\n    text: str\n    generation_time: float\n    tokens_generated: int\n    tokens_per_second: float\n    speedup_factor: Optional[float] = None\n    acceptance_rate: Optional[float] = None\n\nclass ComparisonResponse(BaseModel):\n    normal: GenerationResponse\n    medusa: GenerationResponse\n    speedup: float\n\nclass MedusaLlamaCppManager:\n    \"\"\"\n    Manager class that combines llama.cpp with Medusa for speculative decoding.\n    This class loads the model, sets up Medusa buffers, and provides text generation.\n    \"\"\"\n    def __init__(\n        self,\n        model_path: str,\n        medusa_num_heads: int = 4,\n        n_ctx: int = 2048,\n        n_batch: int = 512,\n        n_threads: int = 8\n    ):\n        self.logger = logging.getLogger(\"MedusaLlamaCppManager\")\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.medusa_num_heads = medusa_num_heads\n        self.model_lock = Lock()  # For thread safety\n\n        # Check if model exists\n        if not os.path.exists(model_path):\n            self.logger.error(f\"Model file not found at {model_path}\")\n            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n\n        # Load compiled model using llama.cpp backend\n        self.logger.info(f\"Loading model from {model_path}...\")\n        self.llama_model = Llama(\n            model_path=model_path,\n            n_ctx=n_ctx,\n            n_batch=n_batch,\n            n_threads=n_threads,\n            n_gpu_layers=-1,  # Use all GPU layers\n            verbose=True  # For debugging\n        )\n        self.logger.info(f\"Loaded GGUF model from {model_path}\")\n\n        # Initialize Medusa components\n        self.medusa_buffers = self._initialize_medusa_buffers()\n        self.logger.info(f\"Initialized Medusa buffers with {medusa_num_heads} heads\")\n\n        # Calculate baseline speed for later comparison\n        self._baseline_tokens_per_second = self._calculate_baseline_speed()\n        self.logger.info(f\"Baseline generation speed: {self._baseline_tokens_per_second:.2f} tokens/second\")\n\n    def _initialize_medusa_buffers(self) -> Dict:\n        \"\"\"Initialize Medusa buffers for speculative decoding.\"\"\"\n        # Create buffers for Medusa\n        tree_indices = torch.zeros((self.medusa_num_heads, 2), dtype=torch.long)\n        medusa_attn_mask = torch.ones((self.medusa_num_heads + 1, self.medusa_num_heads + 1), dtype=torch.bool)\n        medusa_attn_mask = torch.triu(medusa_attn_mask, diagonal=1)\n        medusa_position_ids = torch.arange(self.medusa_num_heads, dtype=torch.long)\n\n        return {\n            \"tree_indices\": tree_indices,\n            \"medusa_attn_mask\": medusa_attn_mask,\n            \"medusa_position_ids\": medusa_position_ids,\n            \"retrieve_indices\": None  # Will be set during generation\n        }\n\n    def _calculate_baseline_speed(self) -> float:\n        \"\"\"Calculate baseline generation speed without Medusa.\"\"\"\n        prompt = \"Once upon a time\"\n        start_time = time.time()\n\n        response = self.llama_model(prompt, max_tokens=20, temperature=0.7)\n\n        if response and 'choices' in response:\n            generated_text = response['choices'][0]['text']\n            tokens = len(generated_text.split())\n            elapsed_time = time.time() - start_time\n\n            if elapsed_time > 0 and tokens > 0:\n                tokens_per_second = tokens / elapsed_time\n                self.logger.info(f\"Baseline generation speed: {tokens_per_second:.2f} tokens/second\")\n                return tokens_per_second\n\n        # Default value if calculation fails\n        return 5.0\n\n    def generate_normal(\n        self,\n        prompt: str,\n        max_length: int = 512,\n        temperature: float = 0.7\n    ) -> Dict:\n        \"\"\"Generate text using standard approach (no Medusa)\"\"\"\n        with self.model_lock:  # Ensure thread safety\n            start_time = time.time()\n            \n            # For better reliability, use straightforward generation\n            self.logger.info(f\"Normal generation with prompt: {prompt[:50]}...\")\n            \n            try:\n                # Generate text using llama.cpp directly\n                response = self.llama_model(\n                    prompt,\n                    max_tokens=min(max_length, 100),  # Cap max tokens for reliability\n                    temperature=temperature,\n                    echo=False\n                )\n                \n                if not response or 'choices' not in response:\n                    self.logger.error(\"No response from model\")\n                    return {\n                        \"text\": \"Error: No response from model\",\n                        \"generation_time\": time.time() - start_time,\n                        \"tokens_generated\": 0,\n                        \"tokens_per_second\": 0,\n                        \"speedup_factor\": 0\n                    }\n                \n                generated_text = response['choices'][0]['text']\n                tokens_generated = len(generated_text.split())\n                \n                elapsed_time = time.time() - start_time\n                tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n                \n                self.logger.info(f\"Normal generation: {tokens_generated} tokens in {elapsed_time:.2f} seconds\")\n                self.logger.info(f\"Normal generation speed: {tokens_per_second:.2f} tokens/second\")\n                \n                return {\n                    \"text\": generated_text,\n                    \"generation_time\": elapsed_time,\n                    \"tokens_generated\": tokens_generated,\n                    \"tokens_per_second\": tokens_per_second,\n                    \"speedup_factor\": 1.0  # No speedup for normal generation\n                }\n                \n            except Exception as e:\n                self.logger.error(f\"Error in normal generation: {str(e)}\")\n                return {\n                    \"text\": f\"Error: {str(e)}\",\n                    \"generation_time\": time.time() - start_time,\n                    \"tokens_generated\": 0,\n                    \"tokens_per_second\": 0,\n                    \"speedup_factor\": 0\n                }\n\n    def generate_medusa(\n        self,\n        prompt: str,\n        max_length: int = 512,\n        temperature: float = 0.7,\n        posterior_threshold: float = 0.09,\n        posterior_alpha: float = 0.3\n    ) -> Dict:\n        \"\"\"Generate text using Medusa speculative decoding\"\"\"\n        with self.model_lock:  # Ensure thread safety\n            start_time = time.time()\n            \n            self.logger.info(f\"Medusa generation with prompt: {prompt[:50]}...\")\n            \n            try:\n                input_text = prompt\n                generated_text = \"\"\n                tokens_generated = 0\n                draft_tokens_generated = 0\n                accepted_tokens = 0\n                \n                # Add safeguard \n                max_iterations = min(max_length, 100)\n                \n                for _ in range(max_iterations):\n                    # Generate base token and draft tokens\n                    base_token, drafts = self._generate_drafts(input_text, temperature)\n                    \n                    if not base_token:\n                        self.logger.warning(\"No base token generated, ending generation\")\n                        break\n                    \n                    # Count drafts\n                    draft_tokens_generated += len(drafts) + 1  # Base + drafts\n                    \n                    # Verify drafts\n                    accept_count, accepted_tokens_list = self._verify_drafts(\n                        input_text, \n                        [base_token] + drafts, \n                        temperature, \n                        posterior_threshold,\n                        posterior_alpha\n                    )\n                    \n                    # Update accepted count\n                    accepted_tokens += accept_count\n                    \n                    # Use accepted tokens or fallback to base token\n                    if accept_count > 0:\n                        token_text = ''.join(accepted_tokens_list)\n                        input_text += token_text\n                        generated_text += token_text\n                        tokens_generated += accept_count\n                    else:\n                        input_text += base_token\n                        generated_text += base_token\n                        tokens_generated += 1\n                    \n                    # Log progress periodically\n                    if tokens_generated % 20 == 0:\n                        self.logger.info(f\"Generated {tokens_generated} tokens so far\")\n                \n                elapsed_time = time.time() - start_time\n                tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n                acceptance_rate = (accepted_tokens / draft_tokens_generated * 100) if draft_tokens_generated > 0 else 0\n                \n                self.logger.info(f\"Medusa generation: {tokens_generated} tokens in {elapsed_time:.2f} seconds\")\n                self.logger.info(f\"Medusa speed: {tokens_per_second:.2f} tokens/second\")\n                self.logger.info(f\"Acceptance rate: {acceptance_rate:.2f}%\")\n                \n                return {\n                    \"text\": generated_text,\n                    \"generation_time\": elapsed_time,\n                    \"tokens_generated\": tokens_generated,\n                    \"tokens_per_second\": tokens_per_second,\n                    \"speedup_factor\": tokens_per_second / self._baseline_tokens_per_second,\n                    \"acceptance_rate\": acceptance_rate\n                }\n                \n            except Exception as e:\n                self.logger.error(f\"Error in Medusa generation: {str(e)}\")\n                return {\n                    \"text\": f\"Error: {str(e)}\",\n                    \"generation_time\": time.time() - start_time,\n                    \"tokens_generated\": 0,\n                    \"tokens_per_second\": 0,\n                    \"speedup_factor\": 0,\n                    \"acceptance_rate\": 0\n                }\n\n    def _generate_drafts(self, context: str, temperature: float) -> tuple:\n        \"\"\"Generate base and draft tokens following the Medusa pattern\"\"\"\n        try:\n            # Get the base token\n            base_response = self.llama_model(\n                context,\n                max_tokens=1,\n                temperature=temperature,\n                echo=False\n            )\n            \n            if not base_response or 'choices' not in base_response:\n                return \"\", []\n                \n            base_token = base_response['choices'][0]['text']\n            \n            # Generate draft tokens in tree structure (continuation of base token)\n            drafts = []\n            current_context = context + base_token\n            \n            # Generate drafts following tree structure\n            for i in range(min(self.medusa_num_heads - 1, 3)):  # Limit to 3 drafts max for performance\n                try:\n                    draft_response = self.llama_model(\n                        current_context,\n                        max_tokens=1,\n                        temperature=temperature,\n                        echo=False\n                    )\n                    \n                    if draft_response and 'choices' in draft_response:\n                        draft_token = draft_response['choices'][0]['text']\n                        drafts.append(draft_token)\n                        current_context += draft_token\n                    else:\n                        break\n                except Exception as e:\n                    self.logger.error(f\"Error generating draft {i}: {str(e)}\")\n                    break\n            \n            return base_token, drafts\n            \n        except Exception as e:\n            self.logger.error(f\"Error in draft generation: {str(e)}\")\n            return \"\", []\n\n    def _verify_drafts(\n        self,\n        context: str,\n        drafts: List[str],\n        temperature: float,\n        threshold: float,\n        alpha: float\n    ) -> tuple:\n        \"\"\"Verify draft tokens and return accepted ones\"\"\"\n        if not drafts:\n            return 0, []\n            \n        accepted_drafts = []\n        current_context = context\n        \n        for i, draft in enumerate(drafts):\n            try:\n                # For the base token (first draft), always accept\n                if i == 0:\n                    accepted_drafts.append(draft)\n                    current_context += draft\n                    continue\n                \n                # For subsequent tokens, verify\n                verify_response = self.llama_model(\n                    current_context,\n                    max_tokens=1,\n                    temperature=0,  # Use zero temperature for deterministic output\n                    echo=False\n                )\n                \n                # Simple verification: if the model generates the same token, it's good\n                if verify_response and 'choices' in verify_response:\n                    predicted = verify_response['choices'][0]['text']\n                    \n                    # Calculate simple score (1.0 if perfect match, 0.0 otherwise)\n                    score = 1.0 if predicted == draft else 0.0\n                    \n                    # Apply threshold\n                    if score >= threshold:\n                        accepted_drafts.append(draft)\n                        current_context += draft\n                    else:\n                        break\n                else:\n                    break\n                    \n            except Exception as e:\n                self.logger.error(f\"Error in draft verification: {str(e)}\")\n                break\n        \n        return len(accepted_drafts), accepted_drafts\n\n# Global variables for model manager\nmodel_manager = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Lifespan context for startup and shutdown actions.\"\"\"\n    global model_manager\n\n    logger.info(\"Initializing model manager...\")\n    try:\n        model_path = \"/kaggle/input/vicuna-1/gguf/default/1/vicuna-7b-v1.3-F16_KM.gguf\"\n        model_manager = MedusaLlamaCppManager(\n            model_path=model_path,\n            medusa_num_heads=4,\n            n_ctx=2048,\n            n_batch=512,\n            n_threads=8\n        )\n        logger.info(\"Model manager initialized successfully.\")\n    except Exception as e:\n        logger.error(f\"Error initializing model: {str(e)}\")\n        raise\n\n    try:\n        # Set up ngrok tunnel for external access\n        ngrok_tunnel = ngrok.connect(8000)\n        logger.info(f\"Ngrok tunnel established at: {ngrok_tunnel.public_url}\")\n        print(f\"Public URL: {ngrok_tunnel.public_url}\")\n    except Exception as e:\n        logger.error(f\"Failed to establish ngrok tunnel: {str(e)}\")\n\n    yield\n\n    # Shutdown procedures can be added here\n    logger.info(\"Shutting down server and cleaning up resources.\")\n\n# Initialize FastAPI app with lifespan context\napp = FastAPI(\n    title=\"Medusa LLM Service\",\n    description=\"Language model service with both standard and Medusa speculative decoding\",\n    lifespan=lifespan\n)\n\n# Normal generation endpoint\n@app.post(\"/generate/normal\", response_model=GenerationResponse)\nasync def generate_normal(request: GenerationRequest):\n    \"\"\"Generate text using normal decoding (without Medusa)\"\"\"\n    try:\n        if model_manager is None:\n            raise HTTPException(status_code=503, detail=\"Model not initialized\")\n        \n        logger.info(f\"Processing normal generation request: {request.prompt[:50]}...\")\n        \n        # Use asyncio.to_thread to avoid blocking the event loop\n        result = await asyncio.to_thread(\n            model_manager.generate_normal,\n            prompt=request.prompt,\n            max_length=min(request.max_length, 100),\n            temperature=request.temperature\n        )\n        \n        if \"error\" in result:\n            raise HTTPException(status_code=500, detail=result[\"error\"])\n            \n        return GenerationResponse(\n            text=result[\"text\"],\n            generation_time=result[\"generation_time\"],\n            tokens_generated=result[\"tokens_generated\"],\n            tokens_per_second=result[\"tokens_per_second\"],\n            speedup_factor=result[\"speedup_factor\"]\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error in normal generation endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Medusa generation endpoint\n@app.post(\"/generate/medusa\", response_model=GenerationResponse)\nasync def generate_medusa(request: GenerationRequest):\n    \"\"\"Generate text using Medusa speculative decoding\"\"\"\n    try:\n        if model_manager is None:\n            raise HTTPException(status_code=503, detail=\"Model not initialized\")\n        \n        logger.info(f\"Processing Medusa generation request: {request.prompt[:50]}...\")\n        \n        # Use asyncio.to_thread to avoid blocking the event loop\n        result = await asyncio.to_thread(\n            model_manager.generate_medusa,\n            prompt=request.prompt,\n            max_length=min(request.max_length, 100),\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n        \n        if \"error\" in result:\n            raise HTTPException(status_code=500, detail=result[\"error\"])\n            \n        return GenerationResponse(\n            text=result[\"text\"],\n            generation_time=result[\"generation_time\"],\n            tokens_generated=result[\"tokens_generated\"],\n            tokens_per_second=result[\"tokens_per_second\"],\n            speedup_factor=result[\"speedup_factor\"],\n            acceptance_rate=result[\"acceptance_rate\"]\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error in Medusa generation endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Comparison endpoint - runs both methods and compares\n@app.post(\"/compare\", response_model=ComparisonResponse)\nasync def compare_generation_methods(request: GenerationRequest):\n    \"\"\"Compare normal and Medusa generation methods\"\"\"\n    try:\n        if model_manager is None:\n            raise HTTPException(status_code=503, detail=\"Model not initialized\")\n        \n        logger.info(f\"Running comparison with prompt: {request.prompt[:50]}...\")\n        \n        # Run normal generation\n        normal_result = await asyncio.to_thread(\n            model_manager.generate_normal,\n            prompt=request.prompt,\n            max_length=min(request.max_length, 100),\n            temperature=request.temperature\n        )\n        \n        # Run Medusa generation\n        medusa_result = await asyncio.to_thread(\n            model_manager.generate_medusa,\n            prompt=request.prompt,\n            max_length=min(request.max_length, 100),\n            temperature=request.temperature,\n            posterior_threshold=request.posterior_threshold,\n            posterior_alpha=request.posterior_alpha\n        )\n        \n        # Calculate speedup\n        normal_speed = normal_result[\"tokens_per_second\"]\n        medusa_speed = medusa_result[\"tokens_per_second\"]\n        speedup = medusa_speed / normal_speed if normal_speed > 0 else 0\n        \n        # Create response\n        normal_response = GenerationResponse(\n            text=normal_result[\"text\"],\n            generation_time=normal_result[\"generation_time\"],\n            tokens_generated=normal_result[\"tokens_generated\"],\n            tokens_per_second=normal_result[\"tokens_per_second\"],\n            speedup_factor=normal_result[\"speedup_factor\"]\n        )\n        \n        medusa_response = GenerationResponse(\n            text=medusa_result[\"text\"],\n            generation_time=medusa_result[\"generation_time\"],\n            tokens_generated=medusa_result[\"tokens_generated\"],\n            tokens_per_second=medusa_result[\"tokens_per_second\"],\n            speedup_factor=medusa_result[\"speedup_factor\"],\n            acceptance_rate=medusa_result[\"acceptance_rate\"]\n        )\n        \n        return ComparisonResponse(\n            normal=normal_response,\n            medusa=medusa_response,\n            speedup=speedup\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error in comparison endpoint: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Health check endpoint\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Check if the service is healthy\"\"\"\n    # Also check if model is loaded\n    model_status = \"loaded\" if model_manager is not None else \"not loaded\"\n    return {\n        \"status\": \"healthy\", \n        \"model\": model_status,\n        \"device\": model_manager.device if model_manager else \"unknown\"\n    }\n\n# Function to start the server\nasync def start_server():\n    \"\"\"Start the FastAPI server\"\"\"\n    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n    server = uvicorn.Server(config)\n    await server.serve()\n\n# Entry point for running the file directly\nif __name__ == \"__main__\":\n    try:\n        # Check if running in a notebook environment\n        if 'google.colab' in globals() or 'kaggle_secrets' in globals():\n            # Running in notebook (Colab or Kaggle)\n            nest_asyncio.apply()\n            asyncio.run(start_server())\n        else:\n            # Standard Python environment\n            import uvicorn\n            uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n    except Exception as e:\n        logger.error(f\"Error starting server: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:11:32.685376Z","iopub.execute_input":"2025-03-28T11:11:32.693415Z"}},"outputs":[{"name":"stderr","text":"INFO:     Started server process [1398]\nINFO:     Waiting for application startup.\nllama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /kaggle/input/vicuna-1/gguf/default/1/vicuna-7b-v1.3-F16_KM.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Vicuna 7b v1.3\nllama_model_loader: - kv   3:                            general.version str              = v1.3\nllama_model_loader: - kv   4:                           general.basename str              = vicuna\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                          llama.block_count u32              = 32\nllama_model_loader: - kv   7:                       llama.context_length u32              = 2048\nllama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  12:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  13:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\nllama_model_loader: - kv  25:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 2048\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 2048\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \nllm_load_print_meta: general.name     = Vicuna 7b v1.3\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.14 MiB\nllm_load_tensors:        CPU buffer size =  3891.24 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'general.file_type': '15', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.block_count': '32', 'tokenizer.ggml.padding_token_id': '0', 'general.basename': 'vicuna', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '2048', 'general.name': 'Vicuna 7b v1.3', 'llama.rope.dimension_count': '128', 'general.version': 'v1.3', 'general.type': 'model', 'general.size_label': '7B', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.vocab_size': '32000'}\nUsing fallback chat format: llama-2\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       1.56 ms /    20 runs   (    0.08 ms per token, 12795.91 tokens per second)\nllama_print_timings: prompt eval time =    1950.02 ms /     5 tokens (  390.00 ms per token,     2.56 tokens per second)\nllama_print_timings:        eval time =   14967.81 ms /    19 runs   (  787.78 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =   16939.71 ms /    24 tokens\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"Public URL: https://dadf-35-194-149-242.ngrok-free.app\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.86 ms /   265 runs   (    0.06 ms per token, 15721.40 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  158592.01 ms /   265 runs   (  598.46 ms per token,     1.67 tokens per second)\nllama_print_timings:       total time =  158953.44 ms /   265 tokens\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       3.59 ms /    50 runs   (    0.07 ms per token, 13939.22 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   37401.19 ms /    50 runs   (  748.02 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =   37453.74 ms /    50 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     763.14 ms /     1 runs   (  763.14 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     763.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     747.41 ms /     1 runs   (  747.41 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     747.88 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     766.96 ms /     1 runs   (  766.96 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     767.46 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     656.49 ms /     1 runs   (  656.49 ms per token,     1.52 tokens per second)\nllama_print_timings:       total time =     657.64 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.42 ms /     1 runs   (  789.42 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     789.79 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     735.27 ms /     1 runs   (  735.27 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     736.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.84 ms /     1 runs   (  774.84 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.20 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     716.74 ms /     1 runs   (  716.74 ms per token,     1.40 tokens per second)\nllama_print_timings:       total time =     716.76 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     799.14 ms /     1 runs   (  799.14 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     800.00 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.37 ms /     1 runs   (  764.37 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     764.74 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     721.86 ms /     1 runs   (  721.86 ms per token,     1.39 tokens per second)\nllama_print_timings:       total time =     722.38 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22727.27 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     753.40 ms /     1 runs   (  753.40 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     753.40 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     670.50 ms /     1 runs   (  670.50 ms per token,     1.49 tokens per second)\nllama_print_timings:       total time =     671.27 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     751.97 ms /     1 runs   (  751.97 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     752.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     709.83 ms /     1 runs   (  709.83 ms per token,     1.41 tokens per second)\nllama_print_timings:       total time =     709.86 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     932.00 ms /     1 runs   (  932.00 ms per token,     1.07 tokens per second)\nllama_print_timings:       total time =     932.54 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20833.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     908.43 ms /     1 runs   (  908.43 ms per token,     1.10 tokens per second)\nllama_print_timings:       total time =     908.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     791.11 ms /     1 runs   (  791.11 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     791.77 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     714.90 ms /     1 runs   (  714.90 ms per token,     1.40 tokens per second)\nllama_print_timings:       total time =     715.47 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 26315.79 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     677.85 ms /     1 runs   (  677.85 ms per token,     1.48 tokens per second)\nllama_print_timings:       total time =     677.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.24 ms /     1 runs   (  772.24 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     772.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.20 ms /     1 runs   (  762.20 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.57 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     739.42 ms /     1 runs   (  739.42 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     740.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     758.20 ms /     1 runs   (  758.20 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     758.93 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     690.82 ms /     1 runs   (  690.82 ms per token,     1.45 tokens per second)\nllama_print_timings:       total time =     690.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.84 ms /     1 runs   (  789.84 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     790.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     725.90 ms /     1 runs   (  725.90 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     726.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     725.70 ms /     1 runs   (  725.70 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     726.17 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.64 ms /     1 runs   (  789.64 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     790.34 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     757.08 ms /     1 runs   (  757.08 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     757.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     722.35 ms /     1 runs   (  722.35 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     722.47 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     739.54 ms /     1 runs   (  739.54 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     739.86 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     828.68 ms /     1 runs   (  828.68 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     829.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10989.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     747.77 ms /     1 runs   (  747.77 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     748.38 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     794.74 ms /     1 runs   (  794.74 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     710.19 ms /     1 runs   (  710.19 ms per token,     1.41 tokens per second)\nllama_print_timings:       total time =     713.20 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     745.59 ms /     1 runs   (  745.59 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.37 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10869.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     780.74 ms /     1 runs   (  780.74 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.29 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     746.36 ms /     1 runs   (  746.36 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     733.67 ms /     1 runs   (  733.67 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     733.95 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     725.91 ms /     1 runs   (  725.91 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     726.71 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     724.60 ms /     1 runs   (  724.60 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     725.30 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     715.32 ms /     1 runs   (  715.32 ms per token,     1.40 tokens per second)\nllama_print_timings:       total time =     717.47 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     726.12 ms /     1 runs   (  726.12 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     726.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      42.45 ms /   682 runs   (    0.06 ms per token, 16064.45 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  423094.31 ms /   682 runs   (  620.37 ms per token,     1.61 tokens per second)\nllama_print_timings:       total time =  424511.41 ms /   682 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18518.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     751.07 ms /     1 runs   (  751.07 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     751.40 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     749.30 ms /     1 runs   (  749.30 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     750.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     773.81 ms /     1 runs   (  773.81 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     774.63 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.49 ms /     1 runs   (  772.49 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     773.98 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     716.53 ms /     1 runs   (  716.53 ms per token,     1.40 tokens per second)\nllama_print_timings:       total time =     716.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22222.22 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     752.84 ms /     1 runs   (  752.84 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     753.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     732.38 ms /     1 runs   (  732.38 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     732.63 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     709.24 ms /     1 runs   (  709.24 ms per token,     1.41 tokens per second)\nllama_print_timings:       total time =     709.17 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     674.93 ms /     1 runs   (  674.93 ms per token,     1.48 tokens per second)\nllama_print_timings:       total time =     675.29 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     740.10 ms /     1 runs   (  740.10 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     740.20 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     748.54 ms /     1 runs   (  748.54 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     749.34 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     682.37 ms /     1 runs   (  682.37 ms per token,     1.47 tokens per second)\nllama_print_timings:       total time =     683.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     736.11 ms /     1 runs   (  736.11 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     738.07 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18518.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     754.76 ms /     1 runs   (  754.76 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     755.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     910.48 ms /     1 runs   (  910.48 ms per token,     1.10 tokens per second)\nllama_print_timings:       total time =     910.99 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     892.16 ms /     1 runs   (  892.16 ms per token,     1.12 tokens per second)\nllama_print_timings:       total time =     892.47 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23255.81 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     672.06 ms /     1 runs   (  672.06 ms per token,     1.49 tokens per second)\nllama_print_timings:       total time =     672.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     752.79 ms /     1 runs   (  752.79 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     753.61 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 28571.43 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     755.35 ms /     1 runs   (  755.35 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     755.49 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     694.89 ms /     1 runs   (  694.89 ms per token,     1.44 tokens per second)\nllama_print_timings:       total time =     695.50 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     735.40 ms /     1 runs   (  735.40 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     737.46 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20408.16 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     733.94 ms /     1 runs   (  733.94 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     734.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     708.94 ms /     1 runs   (  708.94 ms per token,     1.41 tokens per second)\nllama_print_timings:       total time =     709.64 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     740.80 ms /     1 runs   (  740.80 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.17 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     748.15 ms /     1 runs   (  748.15 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     748.44 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     795.57 ms /     1 runs   (  795.57 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     796.22 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     802.41 ms /     1 runs   (  802.41 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     803.03 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     693.21 ms /     1 runs   (  693.21 ms per token,     1.44 tokens per second)\nllama_print_timings:       total time =     693.95 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     705.35 ms /     1 runs   (  705.35 ms per token,     1.42 tokens per second)\nllama_print_timings:       total time =     706.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     697.72 ms /     1 runs   (  697.72 ms per token,     1.43 tokens per second)\nllama_print_timings:       total time =     698.14 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.71 ms /     1 runs   (  762.71 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     763.19 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     782.75 ms /     1 runs   (  782.75 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     785.60 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     751.25 ms /     1 runs   (  751.25 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     751.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     729.01 ms /     1 runs   (  729.01 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     729.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     739.20 ms /     1 runs   (  739.20 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     740.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     727.71 ms /     1 runs   (  727.71 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     727.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     670.60 ms /     1 runs   (  670.60 ms per token,     1.49 tokens per second)\nllama_print_timings:       total time =     671.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     692.01 ms /     1 runs   (  692.01 ms per token,     1.45 tokens per second)\nllama_print_timings:       total time =     692.29 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.96 ms /     1 runs   (  772.96 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     773.63 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     779.91 ms /     1 runs   (  779.91 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     780.53 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18518.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     770.78 ms /     1 runs   (  770.78 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     771.38 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     736.66 ms /     1 runs   (  736.66 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     739.82 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     735.35 ms /     1 runs   (  735.35 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     735.73 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.82 ms /     1 runs   (  764.82 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     764.95 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     761.87 ms /     1 runs   (  761.87 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.21 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     745.39 ms /     1 runs   (  745.39 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     662.98 ms /     1 runs   (  662.98 ms per token,     1.51 tokens per second)\nllama_print_timings:       total time =     663.19 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     745.48 ms /     1 runs   (  745.48 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     747.51 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.72 ms /     1 runs   (  760.72 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     696.93 ms /     1 runs   (  696.93 ms per token,     1.43 tokens per second)\nllama_print_timings:       total time =     697.44 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     704.02 ms /     1 runs   (  704.02 ms per token,     1.42 tokens per second)\nllama_print_timings:       total time =     704.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10989.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     740.12 ms /     1 runs   (  740.12 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     748.53 ms /     1 runs   (  748.53 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     750.94 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     780.89 ms /     1 runs   (  780.89 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.99 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.11 ms /     1 runs   (  797.11 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     798.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     841.15 ms /     1 runs   (  841.15 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time =     841.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8547.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     885.52 ms /     1 runs   (  885.52 ms per token,     1.13 tokens per second)\nllama_print_timings:       total time =     886.32 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     958.76 ms /     1 runs   (  958.76 ms per token,     1.04 tokens per second)\nllama_print_timings:       total time =     959.35 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     961.26 ms /     1 runs   (  961.26 ms per token,     1.04 tokens per second)\nllama_print_timings:       total time =     961.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.11 ms /     1 runs   (  796.11 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     796.94 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     819.44 ms /     1 runs   (  819.44 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     821.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     781.74 ms /     1 runs   (  781.74 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     782.52 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     785.41 ms /     1 runs   (  785.41 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.79 ms /     1 runs   (  768.79 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     769.77 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     754.29 ms /     1 runs   (  754.29 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     754.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.79 ms /     1 runs   (  768.79 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     769.38 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     839.60 ms /     1 runs   (  839.60 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time =     840.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     788.31 ms /     1 runs   (  788.31 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     867.98 ms /     1 runs   (  867.98 ms per token,     1.15 tokens per second)\nllama_print_timings:       total time =     868.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     807.73 ms /     1 runs   (  807.73 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     808.42 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12345.68 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     818.33 ms /     1 runs   (  818.33 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     819.12 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     769.90 ms /     1 runs   (  769.90 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     770.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     840.38 ms /     1 runs   (  840.38 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time =     840.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     749.56 ms /     1 runs   (  749.56 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     750.73 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     783.27 ms /     1 runs   (  783.27 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     783.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     765.80 ms /     1 runs   (  765.80 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     766.47 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     802.51 ms /     1 runs   (  802.51 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     803.07 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     788.12 ms /     1 runs   (  788.12 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     765.33 ms /     1 runs   (  765.33 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     766.29 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.82 ms /     1 runs   (  774.82 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.74 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     869.44 ms /     1 runs   (  869.44 ms per token,     1.15 tokens per second)\nllama_print_timings:       total time =     870.19 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10869.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     791.28 ms /     1 runs   (  791.28 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     791.64 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     781.48 ms /     1 runs   (  781.48 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     782.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     755.94 ms /     1 runs   (  755.94 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     757.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.95 ms /     1 runs   (  772.95 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     773.64 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 24390.24 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     780.88 ms /     1 runs   (  780.88 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.30 ms /     1 runs   (  760.30 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     761.12 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     766.87 ms /     1 runs   (  766.87 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     767.23 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10416.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.44 ms /     1 runs   (  796.44 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     797.45 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     779.74 ms /     1 runs   (  779.74 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     780.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8264.46 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     773.93 ms /     1 runs   (  773.93 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.02 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.09 ms /     1 runs   (  762.09 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.37 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11494.25 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     766.50 ms /     1 runs   (  766.50 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     767.50 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 27777.78 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.30 ms /     1 runs   (  789.30 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     789.98 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     776.80 ms /     1 runs   (  776.80 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     779.25 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     814.56 ms /     1 runs   (  814.56 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     815.36 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     825.60 ms /     1 runs   (  825.60 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     826.72 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     982.41 ms /     1 runs   (  982.41 ms per token,     1.02 tokens per second)\nllama_print_timings:       total time =     983.46 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     977.38 ms /     1 runs   (  977.38 ms per token,     1.02 tokens per second)\nllama_print_timings:       total time =     978.66 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     826.49 ms /     1 runs   (  826.49 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     826.72 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.03 ms /     1 runs   (  768.03 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     768.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     803.03 ms /     1 runs   (  803.03 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     803.68 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     751.22 ms /     1 runs   (  751.22 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     752.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     786.80 ms /     1 runs   (  786.80 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.50 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     818.61 ms /     1 runs   (  818.61 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     823.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     756.42 ms /     1 runs   (  756.42 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     757.38 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10989.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     779.53 ms /     1 runs   (  779.53 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     780.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22222.22 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     717.17 ms /     1 runs   (  717.17 ms per token,     1.39 tokens per second)\nllama_print_timings:       total time =     717.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.75 ms /     1 runs   (  760.75 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     761.30 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18518.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     794.20 ms /     1 runs   (  794.20 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     794.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     743.07 ms /     1 runs   (  743.07 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     743.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     812.30 ms /     1 runs   (  812.30 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     812.94 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11111.11 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     807.69 ms /     1 runs   (  807.69 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     808.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     791.17 ms /     1 runs   (  791.17 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     791.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.14 ms /     1 runs   (  768.14 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     770.53 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     783.51 ms /     1 runs   (  783.51 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     783.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8403.36 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     807.18 ms /     1 runs   (  807.18 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     808.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     761.54 ms /     1 runs   (  761.54 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.49 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  8928.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     749.81 ms /     1 runs   (  749.81 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     750.92 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     822.05 ms /     1 runs   (  822.05 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     822.80 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22222.22 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     753.45 ms /     1 runs   (  753.45 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     753.54 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     788.07 ms /     1 runs   (  788.07 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.98 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     795.12 ms /     1 runs   (  795.12 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     778.40 ms /     1 runs   (  778.40 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     779.45 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.53 ms /     1 runs   (  772.53 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     773.65 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     732.65 ms /     1 runs   (  732.65 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     733.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     728.03 ms /     1 runs   (  728.03 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     728.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.66 ms /     1 runs   (  774.66 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.74 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     739.67 ms /     1 runs   (  739.67 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.25 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.18 ms /     1 runs   (  760.18 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     763.93 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     833.38 ms /     1 runs   (  833.38 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     834.18 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     786.72 ms /     1 runs   (  786.72 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     787.20 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12345.68 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     769.97 ms /     1 runs   (  769.97 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     771.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     794.57 ms /     1 runs   (  794.57 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     808.33 ms /     1 runs   (  808.33 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     809.20 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     805.04 ms /     1 runs   (  805.04 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     807.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     795.62 ms /     1 runs   (  795.62 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.89 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     805.15 ms /     1 runs   (  805.15 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     805.49 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     934.65 ms /     1 runs   (  934.65 ms per token,     1.07 tokens per second)\nllama_print_timings:       total time =     935.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =    1001.78 ms /     1 runs   ( 1001.78 ms per token,     1.00 tokens per second)\nllama_print_timings:       total time =    1002.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     863.94 ms /     1 runs   (  863.94 ms per token,     1.16 tokens per second)\nllama_print_timings:       total time =     866.23 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     802.96 ms /     1 runs   (  802.96 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     803.64 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     845.60 ms /     1 runs   (  845.60 ms per token,     1.18 tokens per second)\nllama_print_timings:       total time =     846.18 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11363.64 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     738.77 ms /     1 runs   (  738.77 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     739.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      16.64 ms /   264 runs   (    0.06 ms per token, 15869.20 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  200994.14 ms /   264 runs   (  761.34 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =  201360.41 ms /   264 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     786.83 ms /     1 runs   (  786.83 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     787.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     807.13 ms /     1 runs   (  807.13 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     807.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     756.02 ms /     1 runs   (  756.02 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     757.12 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.99 ms /     1 runs   (  797.99 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     798.52 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.79 ms /     1 runs   (  796.79 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     797.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     819.96 ms /     1 runs   (  819.96 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     822.39 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     751.14 ms /     1 runs   (  751.14 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     751.36 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     780.18 ms /     1 runs   (  780.18 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.25 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  8928.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     798.26 ms /     1 runs   (  798.26 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     799.40 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     778.76 ms /     1 runs   (  778.76 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     779.60 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.85 ms /     1 runs   (  797.85 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     798.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     766.78 ms /     1 runs   (  766.78 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     766.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     769.97 ms /     1 runs   (  769.97 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     770.96 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     800.11 ms /     1 runs   (  800.11 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     800.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     839.52 ms /     1 runs   (  839.52 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time =     840.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     750.55 ms /     1 runs   (  750.55 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     751.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     828.01 ms /     1 runs   (  828.01 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     830.30 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.99 ms /     1 runs   (  793.99 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     796.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     827.51 ms /     1 runs   (  827.51 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     828.97 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.75 ms /     1 runs   (  764.75 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.93 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     718.82 ms /     1 runs   (  718.82 ms per token,     1.39 tokens per second)\nllama_print_timings:       total time =     721.35 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.49 ms /     1 runs   (  793.49 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     793.89 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     802.97 ms /     1 runs   (  802.97 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     804.64 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     763.75 ms /     1 runs   (  763.75 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     764.86 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     821.07 ms /     1 runs   (  821.07 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     821.79 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     776.63 ms /     1 runs   (  776.63 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     777.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     808.60 ms /     1 runs   (  808.60 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     809.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     828.02 ms /     1 runs   (  828.02 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     829.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.10 ms /     1 runs   (  797.10 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     797.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     749.36 ms /     1 runs   (  749.36 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     749.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     752.64 ms /     1 runs   (  752.64 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     753.07 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     758.49 ms /     1 runs   (  758.49 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     758.90 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     800.19 ms /     1 runs   (  800.19 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     800.35 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     822.65 ms /     1 runs   (  822.65 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     823.46 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     744.35 ms /     1 runs   (  744.35 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     747.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.21 ms /     1 runs   (    0.21 ms per token,  4807.69 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     786.63 ms /     1 runs   (  786.63 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     787.85 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     819.69 ms /     1 runs   (  819.69 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     820.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.03 ms /     1 runs   (  760.03 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     760.65 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23255.81 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     907.64 ms /     1 runs   (  907.64 ms per token,     1.10 tokens per second)\nllama_print_timings:       total time =     908.85 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     997.13 ms /     1 runs   (  997.13 ms per token,     1.00 tokens per second)\nllama_print_timings:       total time =     998.30 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     936.01 ms /     1 runs   (  936.01 ms per token,     1.07 tokens per second)\nllama_print_timings:       total time =     936.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     812.15 ms /     1 runs   (  812.15 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     812.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     753.36 ms /     1 runs   (  753.36 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     755.21 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     804.89 ms /     1 runs   (  804.89 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     805.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     779.08 ms /     1 runs   (  779.08 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     780.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     845.42 ms /     1 runs   (  845.42 ms per token,     1.18 tokens per second)\nllama_print_timings:       total time =     845.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     740.79 ms /     1 runs   (  740.79 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.42 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     799.99 ms /     1 runs   (  799.99 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     800.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 28571.43 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     805.82 ms /     1 runs   (  805.82 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     808.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     761.35 ms /     1 runs   (  761.35 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     808.74 ms /     1 runs   (  808.74 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     809.82 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     834.88 ms /     1 runs   (  834.88 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     835.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.45 ms /     1 runs   (  797.45 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     798.28 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.03 ms /     1 runs   (  789.03 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     790.22 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 27027.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     776.44 ms /     1 runs   (  776.44 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     777.23 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.50 ms /     1 runs   (  762.50 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     764.82 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 27777.78 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     792.02 ms /     1 runs   (  792.02 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     794.27 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     740.02 ms /     1 runs   (  740.02 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     740.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22727.27 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     773.85 ms /     1 runs   (  773.85 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     774.82 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     814.15 ms /     1 runs   (  814.15 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     814.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 27027.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     783.09 ms /     1 runs   (  783.09 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     785.12 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10869.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     792.90 ms /     1 runs   (  792.90 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     794.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     800.70 ms /     1 runs   (  800.70 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     801.49 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     765.18 ms /     1 runs   (  765.18 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.73 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     876.17 ms /     1 runs   (  876.17 ms per token,     1.14 tokens per second)\nllama_print_timings:       total time =     876.92 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8474.58 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     821.58 ms /     1 runs   (  821.58 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     822.87 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     812.14 ms /     1 runs   (  812.14 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     814.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     773.74 ms /     1 runs   (  773.74 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     774.86 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     777.31 ms /     1 runs   (  777.31 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     777.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     783.11 ms /     1 runs   (  783.11 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     784.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     834.80 ms /     1 runs   (  834.80 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     835.87 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     829.06 ms /     1 runs   (  829.06 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     829.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     787.35 ms /     1 runs   (  787.35 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.91 ms /     1 runs   (  789.91 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     790.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     821.33 ms /     1 runs   (  821.33 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     822.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     779.46 ms /     1 runs   (  779.46 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     780.27 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     841.69 ms /     1 runs   (  841.69 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time =     842.88 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     799.08 ms /     1 runs   (  799.08 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     800.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     920.82 ms /     1 runs   (  920.82 ms per token,     1.09 tokens per second)\nllama_print_timings:       total time =     921.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =    1007.40 ms /     1 runs   ( 1007.40 ms per token,     0.99 tokens per second)\nllama_print_timings:       total time =    1008.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     874.89 ms /     1 runs   (  874.89 ms per token,     1.14 tokens per second)\nllama_print_timings:       total time =     877.32 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     778.37 ms /     1 runs   (  778.37 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     780.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     822.38 ms /     1 runs   (  822.38 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     824.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     736.80 ms /     1 runs   (  736.80 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     737.53 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12345.68 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     811.21 ms /     1 runs   (  811.21 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     813.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     792.25 ms /     1 runs   (  792.25 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     792.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     690.35 ms /     1 runs   (  690.35 ms per token,     1.45 tokens per second)\nllama_print_timings:       total time =     691.45 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     791.05 ms /     1 runs   (  791.05 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     791.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     889.69 ms /     1 runs   (  889.69 ms per token,     1.12 tokens per second)\nllama_print_timings:       total time =     892.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20833.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     825.60 ms /     1 runs   (  825.60 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     827.65 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     821.12 ms /     1 runs   (  821.12 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     821.67 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     784.30 ms /     1 runs   (  784.30 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     785.17 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     811.01 ms /     1 runs   (  811.01 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     812.71 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     821.11 ms /     1 runs   (  821.11 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     826.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.56 ms /     1 runs   (  796.56 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     797.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8196.72 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.21 ms /     1 runs   (  796.21 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     797.50 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     806.54 ms /     1 runs   (  806.54 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     807.57 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     810.64 ms /     1 runs   (  810.64 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     811.30 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.45 ms /     1 runs   (  789.45 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     790.71 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10752.69 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     787.25 ms /     1 runs   (  787.25 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     787.82 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     801.14 ms /     1 runs   (  801.14 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     801.28 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     778.92 ms /     1 runs   (  778.92 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     780.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11494.25 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     803.26 ms /     1 runs   (  803.26 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     804.50 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     809.23 ms /     1 runs   (  809.23 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     810.52 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.42 ms /     1 runs   (  793.42 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     794.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     804.75 ms /     1 runs   (  804.75 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     805.60 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.41 ms /     1 runs   (  790.41 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     791.79 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     773.99 ms /     1 runs   (  773.99 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.00 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     814.85 ms /     1 runs   (  814.85 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     815.44 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.54 ms /     1 runs   (  790.54 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     790.77 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     792.26 ms /     1 runs   (  792.26 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     793.52 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.82 ms /     1 runs   (  790.82 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     792.86 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20408.16 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     801.64 ms /     1 runs   (  801.64 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     801.88 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     777.15 ms /     1 runs   (  777.15 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     778.52 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     747.99 ms /     1 runs   (  747.99 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     749.00 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18518.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     827.39 ms /     1 runs   (  827.39 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     827.99 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     782.13 ms /     1 runs   (  782.13 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     783.02 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     831.07 ms /     1 runs   (  831.07 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     832.49 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     973.02 ms /     1 runs   (  973.02 ms per token,     1.03 tokens per second)\nllama_print_timings:       total time =     973.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     967.46 ms /     1 runs   (  967.46 ms per token,     1.03 tokens per second)\nllama_print_timings:       total time =     968.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     893.26 ms /     1 runs   (  893.26 ms per token,     1.12 tokens per second)\nllama_print_timings:       total time =     893.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     799.44 ms /     1 runs   (  799.44 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     800.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.98 ms /     1 runs   (  760.98 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     787.59 ms /     1 runs   (  787.59 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10309.28 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     842.76 ms /     1 runs   (  842.76 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time =     844.03 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     801.34 ms /     1 runs   (  801.34 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     802.90 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     778.07 ms /     1 runs   (  778.07 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     780.00 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     806.62 ms /     1 runs   (  806.62 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     807.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10638.30 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     804.38 ms /     1 runs   (  804.38 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     805.66 ms /     1 tokens\n","output_type":"stream"},{"name":"stdout","text":"INFO:     14.102.161.98:0 - \"POST /compare HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      17.17 ms /   263 runs   (    0.07 ms per token, 15316.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  180009.76 ms /   263 runs   (  684.45 ms per token,     1.46 tokens per second)\nllama_print_timings:       total time =  180356.88 ms /   263 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      13.00 ms /   217 runs   (    0.06 ms per token, 16687.17 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  125286.57 ms /   217 runs   (  577.36 ms per token,     1.73 tokens per second)\nllama_print_timings:       total time =  125555.52 ms /   217 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      43.80 ms /   681 runs   (    0.06 ms per token, 15548.30 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  467053.76 ms /   681 runs   (  685.84 ms per token,     1.46 tokens per second)\nllama_print_timings:       total time =  468456.63 ms /   681 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     706.00 ms /     1 runs   (  706.00 ms per token,     1.42 tokens per second)\nllama_print_timings:       total time =     706.68 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     699.70 ms /     1 runs   (  699.70 ms per token,     1.43 tokens per second)\nllama_print_timings:       total time =     703.25 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     663.88 ms /     1 runs   (  663.88 ms per token,     1.51 tokens per second)\nllama_print_timings:       total time =     664.65 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     604.46 ms /     1 runs   (  604.46 ms per token,     1.65 tokens per second)\nllama_print_timings:       total time =     605.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      22.45 ms /   327 runs   (    0.07 ms per token, 14563.11 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  191326.83 ms /   327 runs   (  585.10 ms per token,     1.71 tokens per second)\nllama_print_timings:       total time =  191796.32 ms /   327 tokens\nLlama.generate: prefix-match hit\n","output_type":"stream"},{"name":"stdout","text":"INFO:     14.102.161.98:0 - \"POST /compare HTTP/1.1\" 422 Unprocessable Entity\nINFO:     14.102.161.98:0 - \"POST /compare HTTP/1.1\" 422 Unprocessable Entity\n","output_type":"stream"},{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      20.89 ms /   326 runs   (    0.06 ms per token, 15604.06 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  195076.38 ms /   326 runs   (  598.39 ms per token,     1.67 tokens per second)\nllama_print_timings:       total time =  195558.81 ms /   326 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.08 ms /     1 runs   (  790.08 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     791.23 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11494.25 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     836.69 ms /     1 runs   (  836.69 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     838.00 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  9009.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     827.12 ms /     1 runs   (  827.12 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     828.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     759.52 ms /     1 runs   (  759.52 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     760.25 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       3.67 ms /    50 runs   (    0.07 ms per token, 13609.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   40325.43 ms /    50 runs   (  806.51 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =   40378.13 ms /    50 tokens\n","output_type":"stream"},{"name":"stdout","text":"INFO:     14.102.161.98:0 - \"POST /generate/normal HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      44.72 ms /   680 runs   (    0.07 ms per token, 15207.08 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  416398.15 ms /   680 runs   (  612.35 ms per token,     1.63 tokens per second)\nllama_print_timings:       total time =  417804.25 ms /   680 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      20.64 ms /   325 runs   (    0.06 ms per token, 15742.31 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  193180.42 ms /   325 runs   (  594.40 ms per token,     1.68 tokens per second)\nllama_print_timings:       total time =  193665.82 ms /   325 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       8.72 ms /   129 runs   (    0.07 ms per token, 14788.49 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   71523.19 ms /   129 runs   (  554.44 ms per token,     1.80 tokens per second)\nllama_print_timings:       total time =   71663.26 ms /   129 tokens\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.02 ms /     1 runs   (  764.02 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  9174.31 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     784.20 ms /     1 runs   (  784.20 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     784.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     728.60 ms /     1 runs   (  728.60 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     729.50 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  8928.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     707.20 ms /     1 runs   (  707.20 ms per token,     1.41 tokens per second)\nllama_print_timings:       total time =     708.02 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     756.54 ms /     1 runs   (  756.54 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     757.37 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     833.96 ms /     1 runs   (  833.96 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     834.80 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10416.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     974.95 ms /     1 runs   (  974.95 ms per token,     1.03 tokens per second)\nllama_print_timings:       total time =     975.82 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     963.98 ms /     1 runs   (  963.98 ms per token,     1.04 tokens per second)\nllama_print_timings:       total time =     964.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     862.85 ms /     1 runs   (  862.85 ms per token,     1.16 tokens per second)\nllama_print_timings:       total time =     863.21 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     753.45 ms /     1 runs   (  753.45 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     753.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     782.27 ms /     1 runs   (  782.27 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     782.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     781.03 ms /     1 runs   (  781.03 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.18 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     775.12 ms /     1 runs   (  775.12 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.28 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     767.02 ms /     1 runs   (  767.02 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     767.64 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     750.95 ms /     1 runs   (  750.95 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     750.95 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     783.99 ms /     1 runs   (  783.99 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     784.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     741.62 ms /     1 runs   (  741.62 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.97 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     730.88 ms /     1 runs   (  730.88 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     730.80 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     750.32 ms /     1 runs   (  750.32 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     750.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     755.35 ms /     1 runs   (  755.35 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     755.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     754.05 ms /     1 runs   (  754.05 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     755.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     784.70 ms /     1 runs   (  784.70 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     786.53 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     782.87 ms /     1 runs   (  782.87 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     784.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     780.86 ms /     1 runs   (  780.86 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     750.11 ms /     1 runs   (  750.11 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     750.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12345.68 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     746.14 ms /     1 runs   (  746.14 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.28 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     807.16 ms /     1 runs   (  807.16 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     807.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  9174.31 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     738.91 ms /     1 runs   (  738.91 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     739.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     741.43 ms /     1 runs   (  741.43 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     721.36 ms /     1 runs   (  721.36 ms per token,     1.39 tokens per second)\nllama_print_timings:       total time =     723.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.44 ms /     1 runs   (  797.44 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     798.28 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     775.99 ms /     1 runs   (  775.99 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     776.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     770.44 ms /     1 runs   (  770.44 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     771.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     660.28 ms /     1 runs   (  660.28 ms per token,     1.51 tokens per second)\nllama_print_timings:       total time =     660.74 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     784.69 ms /     1 runs   (  784.69 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     784.78 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.81 ms /     1 runs   (  774.81 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     777.26 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8403.36 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     743.47 ms /     1 runs   (  743.47 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     744.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10752.69 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     753.63 ms /     1 runs   (  753.63 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     754.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     709.34 ms /     1 runs   (  709.34 ms per token,     1.41 tokens per second)\nllama_print_timings:       total time =     710.93 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     773.76 ms /     1 runs   (  773.76 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     774.52 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     739.41 ms /     1 runs   (  739.41 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     739.79 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     808.05 ms /     1 runs   (  808.05 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     809.82 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     820.86 ms /     1 runs   (  820.86 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     822.27 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     747.97 ms /     1 runs   (  747.97 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     748.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.21 ms /     1 runs   (  764.21 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     764.86 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     783.55 ms /     1 runs   (  783.55 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     784.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     717.66 ms /     1 runs   (  717.66 ms per token,     1.39 tokens per second)\nllama_print_timings:       total time =     717.82 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     901.48 ms /     1 runs   (  901.48 ms per token,     1.11 tokens per second)\nllama_print_timings:       total time =     902.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22727.27 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     886.94 ms /     1 runs   (  886.94 ms per token,     1.13 tokens per second)\nllama_print_timings:       total time =     889.18 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     968.98 ms /     1 runs   (  968.98 ms per token,     1.03 tokens per second)\nllama_print_timings:       total time =     969.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.65 ms /     1 runs   (  760.65 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     760.78 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.42 ms /     1 runs   (  772.42 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     772.94 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 24390.24 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     808.59 ms /     1 runs   (  808.59 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     809.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     776.00 ms /     1 runs   (  776.00 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     776.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     816.83 ms /     1 runs   (  816.83 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     817.72 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     794.18 ms /     1 runs   (  794.18 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     794.97 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     802.71 ms /     1 runs   (  802.71 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     802.93 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     803.13 ms /     1 runs   (  803.13 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     803.71 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.28 ms /     1 runs   (  764.28 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     764.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     806.68 ms /     1 runs   (  806.68 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     807.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     791.38 ms /     1 runs   (  791.38 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     791.76 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     755.53 ms /     1 runs   (  755.53 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     755.68 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     741.10 ms /     1 runs   (  741.10 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     777.85 ms /     1 runs   (  777.85 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     778.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.08 ms /     1 runs   (  790.08 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     790.37 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10526.32 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.40 ms /     1 runs   (  797.40 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     798.30 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     710.75 ms /     1 runs   (  710.75 ms per token,     1.41 tokens per second)\nllama_print_timings:       total time =     711.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.34 ms /     1 runs   (  762.34 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     763.19 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     721.16 ms /     1 runs   (  721.16 ms per token,     1.39 tokens per second)\nllama_print_timings:       total time =     721.89 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     746.92 ms /     1 runs   (  746.92 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     747.32 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     767.90 ms /     1 runs   (  767.90 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     769.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 24390.24 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.44 ms /     1 runs   (  762.44 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     807.43 ms /     1 runs   (  807.43 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     808.12 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     753.60 ms /     1 runs   (  753.60 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     754.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10869.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     725.09 ms /     1 runs   (  725.09 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     730.26 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18518.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.17 ms /     1 runs   (  762.17 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     716.76 ms /     1 runs   (  716.76 ms per token,     1.40 tokens per second)\nllama_print_timings:       total time =     717.46 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     767.03 ms /     1 runs   (  767.03 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     767.40 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     769.43 ms /     1 runs   (  769.43 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     771.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.72 ms /     1 runs   (  774.72 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.26 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.06 ms /     1 runs   (  762.06 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.40 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     739.61 ms /     1 runs   (  739.61 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     740.29 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23809.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     791.23 ms /     1 runs   (  791.23 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     791.25 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     748.51 ms /     1 runs   (  748.51 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     749.18 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     754.06 ms /     1 runs   (  754.06 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     756.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.04 ms /     1 runs   (  790.04 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     791.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22222.22 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     725.72 ms /     1 runs   (  725.72 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     726.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     742.44 ms /     1 runs   (  742.44 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     743.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.63 ms /     1 runs   (  764.63 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.38 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.38 ms /     1 runs   (    0.38 ms per token,  2631.58 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     913.80 ms /     1 runs   (  913.80 ms per token,     1.09 tokens per second)\nllama_print_timings:       total time =     915.32 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     877.18 ms /     1 runs   (  877.18 ms per token,     1.14 tokens per second)\nllama_print_timings:       total time =     877.34 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     867.01 ms /     1 runs   (  867.01 ms per token,     1.15 tokens per second)\nllama_print_timings:       total time =     867.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     763.00 ms /     1 runs   (  763.00 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     763.23 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.20 ms /     1 runs   (  764.20 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.20 ms /     1 runs   (  774.20 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     777.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     769.57 ms /     1 runs   (  769.57 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     769.93 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     781.02 ms /     1 runs   (  781.02 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.87 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20833.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     810.47 ms /     1 runs   (  810.47 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     811.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     775.48 ms /     1 runs   (  775.48 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     776.04 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.05 ms /     1 runs   (  793.05 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.04 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     712.14 ms /     1 runs   (  712.14 ms per token,     1.40 tokens per second)\nllama_print_timings:       total time =     712.42 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20833.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.07 ms /     1 runs   (  768.07 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     768.09 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.16 ms /     1 runs   (  762.16 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     763.21 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     756.29 ms /     1 runs   (  756.29 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     756.77 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11363.64 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.19 ms /     1 runs   (  790.19 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     791.20 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10638.30 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     767.14 ms /     1 runs   (  767.14 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     767.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     788.60 ms /     1 runs   (  788.60 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.98 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     767.92 ms /     1 runs   (  767.92 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     768.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     676.17 ms /     1 runs   (  676.17 ms per token,     1.48 tokens per second)\nllama_print_timings:       total time =     676.67 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     747.41 ms /     1 runs   (  747.41 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     748.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     836.74 ms /     1 runs   (  836.74 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     837.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     800.91 ms /     1 runs   (  800.91 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     801.90 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.50 ms /     1 runs   (  789.50 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     790.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     730.15 ms /     1 runs   (  730.15 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     730.71 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     756.17 ms /     1 runs   (  756.17 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     756.46 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.36 ms /     1 runs   (  772.36 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     772.51 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       8.22 ms /   128 runs   (    0.06 ms per token, 15567.99 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   97678.18 ms /   128 runs   (  763.11 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =   97812.54 ms /   128 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     745.24 ms /     1 runs   (  745.24 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.42 ms /     1 runs   (  768.42 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     768.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     753.60 ms /     1 runs   (  753.60 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     753.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 26315.79 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     740.17 ms /     1 runs   (  740.17 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     740.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     773.01 ms /     1 runs   (  773.01 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     774.14 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     761.12 ms /     1 runs   (  761.12 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.97 ms /     1 runs   (  762.97 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     763.94 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     851.29 ms /     1 runs   (  851.29 ms per token,     1.17 tokens per second)\nllama_print_timings:       total time =     852.63 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     766.03 ms /     1 runs   (  766.03 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     767.77 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.96 ms /     1 runs   (  797.96 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     798.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10989.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     706.01 ms /     1 runs   (  706.01 ms per token,     1.42 tokens per second)\nllama_print_timings:       total time =     707.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     748.75 ms /     1 runs   (  748.75 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     749.74 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     722.18 ms /     1 runs   (  722.18 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     722.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.07 ms /     1 runs   (  797.07 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     797.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     821.84 ms /     1 runs   (  821.84 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     822.93 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =    1035.12 ms /     1 runs   ( 1035.12 ms per token,     0.97 tokens per second)\nllama_print_timings:       total time =    1035.72 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     919.63 ms /     1 runs   (  919.63 ms per token,     1.09 tokens per second)\nllama_print_timings:       total time =     920.46 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     847.74 ms /     1 runs   (  847.74 ms per token,     1.18 tokens per second)\nllama_print_timings:       total time =     849.19 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     775.86 ms /     1 runs   (  775.86 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     776.07 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     815.99 ms /     1 runs   (  815.99 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     816.31 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.15 ms /     1 runs   (  764.15 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     770.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     749.21 ms /     1 runs   (  749.21 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     749.48 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10752.69 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     682.34 ms /     1 runs   (  682.34 ms per token,     1.47 tokens per second)\nllama_print_timings:       total time =     683.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token,  9803.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.60 ms /     1 runs   (  790.60 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     791.74 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     733.83 ms /     1 runs   (  733.83 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     735.27 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20833.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     710.39 ms /     1 runs   (  710.39 ms per token,     1.41 tokens per second)\nllama_print_timings:       total time =     711.50 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     758.18 ms /     1 runs   (  758.18 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     758.47 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     799.75 ms /     1 runs   (  799.75 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     799.95 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.09 ms /     1 runs   (  764.09 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.28 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 26315.79 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.10 ms /     1 runs   (  793.10 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     793.88 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.86 ms /     1 runs   (  760.86 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     761.60 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     724.75 ms /     1 runs   (  724.75 ms per token,     1.38 tokens per second)\nllama_print_timings:       total time =     725.18 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     851.23 ms /     1 runs   (  851.23 ms per token,     1.17 tokens per second)\nllama_print_timings:       total time =     851.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.81 ms /     1 runs   (  760.81 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     761.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     688.54 ms /     1 runs   (  688.54 ms per token,     1.45 tokens per second)\nllama_print_timings:       total time =     689.85 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     801.91 ms /     1 runs   (  801.91 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     802.23 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12500.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.23 ms /     1 runs   (  790.23 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     791.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     801.88 ms /     1 runs   (  801.88 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     802.87 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     741.45 ms /     1 runs   (  741.45 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     742.12 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     787.03 ms /     1 runs   (  787.03 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.29 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     802.77 ms /     1 runs   (  802.77 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     803.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     808.47 ms /     1 runs   (  808.47 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     810.61 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     830.22 ms /     1 runs   (  830.22 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     831.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     798.79 ms /     1 runs   (  798.79 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     799.44 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     799.54 ms /     1 runs   (  799.54 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     800.12 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     771.67 ms /     1 runs   (  771.67 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     772.29 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     742.75 ms /     1 runs   (  742.75 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     743.54 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20408.16 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     730.76 ms /     1 runs   (  730.76 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     731.37 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     786.33 ms /     1 runs   (  786.33 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     786.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     777.36 ms /     1 runs   (  777.36 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     778.49 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20833.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     771.78 ms /     1 runs   (  771.78 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     773.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20408.16 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.70 ms /     1 runs   (  793.70 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     798.18 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 24390.24 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     734.14 ms /     1 runs   (  734.14 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     734.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  9090.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     820.25 ms /     1 runs   (  820.25 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     821.37 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     730.93 ms /     1 runs   (  730.93 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     731.63 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22727.27 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.32 ms /     1 runs   (  793.32 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     794.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     953.65 ms /     1 runs   (  953.65 ms per token,     1.05 tokens per second)\nllama_print_timings:       total time =     954.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     912.47 ms /     1 runs   (  912.47 ms per token,     1.10 tokens per second)\nllama_print_timings:       total time =     917.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     813.55 ms /     1 runs   (  813.55 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     817.19 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     787.47 ms /     1 runs   (  787.47 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.02 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     765.89 ms /     1 runs   (  765.89 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     766.89 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.41 ms /     1 runs   (  796.41 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     796.78 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     820.88 ms /     1 runs   (  820.88 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     821.66 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.30 ms /     1 runs   (  764.30 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     766.34 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     802.43 ms /     1 runs   (  802.43 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     803.60 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     848.15 ms /     1 runs   (  848.15 ms per token,     1.18 tokens per second)\nllama_print_timings:       total time =     850.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     770.91 ms /     1 runs   (  770.91 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     772.03 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     815.16 ms /     1 runs   (  815.16 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     816.02 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     770.97 ms /     1 runs   (  770.97 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     771.28 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     800.77 ms /     1 runs   (  800.77 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     801.74 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     741.00 ms /     1 runs   (  741.00 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.76 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.16 ms /     1 runs   (  764.16 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     766.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     737.42 ms /     1 runs   (  737.42 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     740.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       1.40 ms /     1 runs   (    1.40 ms per token,   713.78 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     758.60 ms /     1 runs   (  758.60 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     760.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     786.52 ms /     1 runs   (  786.52 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     787.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     794.74 ms /     1 runs   (  794.74 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.96 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11494.25 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     763.74 ms /     1 runs   (  763.74 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     764.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     781.17 ms /     1 runs   (  781.17 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     784.51 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     784.26 ms /     1 runs   (  784.26 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     785.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     749.96 ms /     1 runs   (  749.96 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     751.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.39 ms /     1 runs   (  793.39 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     793.73 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     756.72 ms /     1 runs   (  756.72 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     757.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     821.27 ms /     1 runs   (  821.27 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     824.20 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     777.93 ms /     1 runs   (  777.93 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     778.94 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12345.68 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     750.26 ms /     1 runs   (  750.26 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     750.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10309.28 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.50 ms /     1 runs   (  764.50 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.80 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22222.22 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     806.75 ms /     1 runs   (  806.75 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     807.14 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     769.30 ms /     1 runs   (  769.30 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     769.85 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     757.23 ms /     1 runs   (  757.23 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     758.30 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     735.32 ms /     1 runs   (  735.32 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     736.42 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10309.28 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.47 ms /     1 runs   (  774.47 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.39 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     740.98 ms /     1 runs   (  740.98 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     741.96 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     786.10 ms /     1 runs   (  786.10 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     787.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.22 ms /     1 runs   (  772.22 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     772.99 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     771.53 ms /     1 runs   (  771.53 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     772.47 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     803.07 ms /     1 runs   (  803.07 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     803.67 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     831.51 ms /     1 runs   (  831.51 ms per token,     1.20 tokens per second)\nllama_print_timings:       total time =     831.99 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     951.79 ms /     1 runs   (  951.79 ms per token,     1.05 tokens per second)\nllama_print_timings:       total time =     952.22 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     969.93 ms /     1 runs   (  969.93 ms per token,     1.03 tokens per second)\nllama_print_timings:       total time =     971.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     810.03 ms /     1 runs   (  810.03 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     811.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     770.04 ms /     1 runs   (  770.04 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     772.09 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     815.01 ms /     1 runs   (  815.01 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     815.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     748.26 ms /     1 runs   (  748.26 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     749.51 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20408.16 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     735.79 ms /     1 runs   (  735.79 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     736.26 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11111.11 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.99 ms /     1 runs   (  772.99 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     773.79 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     770.85 ms /     1 runs   (  770.85 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     771.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     766.27 ms /     1 runs   (  766.27 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     767.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18518.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.62 ms /     1 runs   (  762.62 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     763.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     775.06 ms /     1 runs   (  775.06 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     758.44 ms /     1 runs   (  758.44 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     759.76 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 18181.82 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.07 ms /     1 runs   (  789.07 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     790.09 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     781.17 ms /     1 runs   (  781.17 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.62 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     745.71 ms /     1 runs   (  745.71 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.34 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     766.05 ms /     1 runs   (  766.05 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     767.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     795.56 ms /     1 runs   (  795.56 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     797.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20000.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     754.67 ms /     1 runs   (  754.67 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     755.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19230.77 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     745.67 ms /     1 runs   (  745.67 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     801.34 ms /     1 runs   (  801.34 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     802.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     785.62 ms /     1 runs   (  785.62 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     786.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     759.88 ms /     1 runs   (  759.88 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     761.80 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.86 ms /     1 runs   (  764.86 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.92 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12345.68 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     771.94 ms /     1 runs   (  771.94 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     772.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     748.43 ms /     1 runs   (  748.43 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     748.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     744.83 ms /     1 runs   (  744.83 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     747.86 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     699.32 ms /     1 runs   (  699.32 ms per token,     1.43 tokens per second)\nllama_print_timings:       total time =     702.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     782.11 ms /     1 runs   (  782.11 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     783.14 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  9433.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     747.08 ms /     1 runs   (  747.08 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     748.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     749.82 ms /     1 runs   (  749.82 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     751.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10526.32 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     780.26 ms /     1 runs   (  780.26 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     782.14 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.97 ms /     1 runs   (  760.97 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     801.24 ms /     1 runs   (  801.24 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     801.51 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     817.73 ms /     1 runs   (  817.73 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     818.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     856.47 ms /     1 runs   (  856.47 ms per token,     1.17 tokens per second)\nllama_print_timings:       total time =     857.27 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     782.25 ms /     1 runs   (  782.25 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     782.75 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11764.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     790.65 ms /     1 runs   (  790.65 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     791.34 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.27 ms /     1 runs   (  760.27 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     761.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     744.48 ms /     1 runs   (  744.48 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.20 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     814.72 ms /     1 runs   (  814.72 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     815.19 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =    1001.45 ms /     1 runs   ( 1001.45 ms per token,     1.00 tokens per second)\nllama_print_timings:       total time =    1002.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     962.37 ms /     1 runs   (  962.37 ms per token,     1.04 tokens per second)\nllama_print_timings:       total time =     963.32 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     800.75 ms /     1 runs   (  800.75 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     802.40 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10416.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     794.13 ms /     1 runs   (  794.13 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     796.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.93 ms /     1 runs   (  768.93 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     770.04 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     817.45 ms /     1 runs   (  817.45 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     818.14 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.02 ms /     1 runs   (  796.02 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     796.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     776.65 ms /     1 runs   (  776.65 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     777.49 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     805.62 ms /     1 runs   (  805.62 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     806.98 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     755.73 ms /     1 runs   (  755.73 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     756.80 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     749.15 ms /     1 runs   (  749.15 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     749.81 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     764.44 ms /     1 runs   (  764.44 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     765.10 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16949.15 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     747.32 ms /     1 runs   (  747.32 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     748.44 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     765.70 ms /     1 runs   (  765.70 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     767.25 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     770.68 ms /     1 runs   (  770.68 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     773.79 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     742.36 ms /     1 runs   (  742.36 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     743.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     784.88 ms /     1 runs   (  784.88 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     785.39 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.36 ms /     1 runs   (  774.36 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     778.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     757.15 ms /     1 runs   (  757.15 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     757.63 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     783.08 ms /     1 runs   (  783.08 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     784.22 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     769.37 ms /     1 runs   (  769.37 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     770.44 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     794.18 ms /     1 runs   (  794.18 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     796.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     727.57 ms /     1 runs   (  727.57 ms per token,     1.37 tokens per second)\nllama_print_timings:       total time =     728.27 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.55 ms /     1 runs   (  768.55 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     769.04 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.98 ms /     1 runs   (  772.98 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     773.86 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     760.44 ms /     1 runs   (  760.44 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     761.32 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     770.22 ms /     1 runs   (  770.22 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     771.73 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20408.16 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     745.36 ms /     1 runs   (  745.36 ms per token,     1.34 tokens per second)\nllama_print_timings:       total time =     746.49 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17241.38 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     778.75 ms /     1 runs   (  778.75 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     779.17 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     736.33 ms /     1 runs   (  736.33 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     737.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     752.00 ms /     1 runs   (  752.00 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     753.30 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     794.84 ms /     1 runs   (  794.84 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.70 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     817.50 ms /     1 runs   (  817.50 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     818.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16393.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     798.45 ms /     1 runs   (  798.45 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     799.61 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token,  9900.99 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     792.74 ms /     1 runs   (  792.74 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     794.02 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     779.12 ms /     1 runs   (  779.12 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     780.72 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     810.03 ms /     1 runs   (  810.03 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     811.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     781.50 ms /     1 runs   (  781.50 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     781.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 20833.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     776.95 ms /     1 runs   (  776.95 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     777.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     820.33 ms /     1 runs   (  820.33 ms per token,     1.22 tokens per second)\nllama_print_timings:       total time =     821.01 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     768.36 ms /     1 runs   (  768.36 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     769.80 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.11 ms /     1 runs   (    0.11 ms per token,  8849.56 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     989.70 ms /     1 runs   (  989.70 ms per token,     1.01 tokens per second)\nllama_print_timings:       total time =     992.05 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     930.34 ms /     1 runs   (  930.34 ms per token,     1.07 tokens per second)\nllama_print_timings:       total time =     931.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15384.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     863.43 ms /     1 runs   (  863.43 ms per token,     1.16 tokens per second)\nllama_print_timings:       total time =     867.33 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22222.22 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     815.16 ms /     1 runs   (  815.16 ms per token,     1.23 tokens per second)\nllama_print_timings:       total time =     815.57 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     804.84 ms /     1 runs   (  804.84 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     805.63 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     759.66 ms /     1 runs   (  759.66 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     760.60 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     776.59 ms /     1 runs   (  776.59 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     777.52 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     753.87 ms /     1 runs   (  753.87 ms per token,     1.33 tokens per second)\nllama_print_timings:       total time =     756.74 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 11904.76 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     769.44 ms /     1 runs   (  769.44 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     771.43 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     781.33 ms /     1 runs   (  781.33 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =     782.95 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     761.62 ms /     1 runs   (  761.62 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     762.46 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     762.14 ms /     1 runs   (  762.14 ms per token,     1.31 tokens per second)\nllama_print_timings:       total time =     763.00 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     791.23 ms /     1 runs   (  791.23 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     793.58 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     793.48 ms /     1 runs   (  793.48 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.66 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     759.36 ms /     1 runs   (  759.36 ms per token,     1.32 tokens per second)\nllama_print_timings:       total time =     760.87 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     823.27 ms /     1 runs   (  823.27 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     824.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21739.13 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     809.11 ms /     1 runs   (  809.11 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     810.26 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.43 ms /     1 runs   (  796.43 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     796.88 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     774.15 ms /     1 runs   (  774.15 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     775.37 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     791.39 ms /     1 runs   (  791.39 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     792.42 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     787.48 ms /     1 runs   (  787.48 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     788.96 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14925.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     772.31 ms /     1 runs   (  772.31 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     773.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12345.68 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     792.84 ms /     1 runs   (  792.84 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     793.90 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     792.12 ms /     1 runs   (  792.12 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     793.09 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     735.43 ms /     1 runs   (  735.43 ms per token,     1.36 tokens per second)\nllama_print_timings:       total time =     736.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16129.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     795.23 ms /     1 runs   (  795.23 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     795.84 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     796.13 ms /     1 runs   (  796.13 ms per token,     1.26 tokens per second)\nllama_print_timings:       total time =     797.22 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     766.38 ms /     1 runs   (  766.38 ms per token,     1.30 tokens per second)\nllama_print_timings:       total time =     767.97 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     829.32 ms /     1 runs   (  829.32 ms per token,     1.21 tokens per second)\nllama_print_timings:       total time =     830.22 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17857.14 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     808.92 ms /     1 runs   (  808.92 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     809.51 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.05 ms /     1 runs   (  789.05 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     791.17 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14492.75 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     797.30 ms /     1 runs   (  797.30 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     799.08 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     807.35 ms /     1 runs   (  807.35 ms per token,     1.24 tokens per second)\nllama_print_timings:       total time =     808.61 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11235.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     775.84 ms /     1 runs   (  775.84 ms per token,     1.29 tokens per second)\nllama_print_timings:       total time =     778.11 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     800.20 ms /     1 runs   (  800.20 ms per token,     1.25 tokens per second)\nllama_print_timings:       total time =     801.16 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1950.08 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15625.00 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     789.07 ms /     1 runs   (  789.07 ms per token,     1.27 tokens per second)\nllama_print_timings:       total time =     789.92 ms /     1 tokens\n","output_type":"stream"},{"name":"stdout","text":"INFO:     14.102.161.98:0 - \"POST /generate/medusa HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      13.64 ms /   219 runs   (    0.06 ms per token, 16054.54 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  171585.69 ms /   219 runs   (  783.50 ms per token,     1.28 tokens per second)\nllama_print_timings:       total time =  171866.48 ms /   219 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 17543.86 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     552.66 ms /     1 runs   (  552.66 ms per token,     1.81 tokens per second)\nllama_print_timings:       total time =     553.91 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     529.36 ms /     1 runs   (  529.36 ms per token,     1.89 tokens per second)\nllama_print_timings:       total time =     529.79 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     534.58 ms /     1 runs   (  534.58 ms per token,     1.87 tokens per second)\nllama_print_timings:       total time =     534.90 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10869.57 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     534.95 ms /     1 runs   (  534.95 ms per token,     1.87 tokens per second)\nllama_print_timings:       total time =     535.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      44.70 ms /   679 runs   (    0.07 ms per token, 15189.48 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  470302.35 ms /   679 runs   (  692.64 ms per token,     1.44 tokens per second)\nllama_print_timings:       total time =  471724.43 ms /   679 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      13.12 ms /   218 runs   (    0.06 ms per token, 16622.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  122332.80 ms /   218 runs   (  561.16 ms per token,     1.78 tokens per second)\nllama_print_timings:       total time =  122585.31 ms /   218 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      12.91 ms /   217 runs   (    0.06 ms per token, 16812.58 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  121391.46 ms /   217 runs   (  559.41 ms per token,     1.79 tokens per second)\nllama_print_timings:       total time =  121650.08 ms /   217 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      13.81 ms /   216 runs   (    0.06 ms per token, 15639.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  122801.16 ms /   216 runs   (  568.52 ms per token,     1.76 tokens per second)\nllama_print_timings:       total time =  123078.00 ms /   216 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      43.97 ms /   678 runs   (    0.06 ms per token, 15418.55 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  395097.86 ms /   678 runs   (  582.74 ms per token,     1.72 tokens per second)\nllama_print_timings:       total time =  396449.04 ms /   678 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      12.74 ms /   215 runs   (    0.06 ms per token, 16873.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  121584.51 ms /   215 runs   (  565.51 ms per token,     1.77 tokens per second)\nllama_print_timings:       total time =  121826.94 ms /   215 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 21276.60 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     538.06 ms /     1 runs   (  538.06 ms per token,     1.86 tokens per second)\nllama_print_timings:       total time =     538.83 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     540.47 ms /     1 runs   (  540.47 ms per token,     1.85 tokens per second)\nllama_print_timings:       total time =     542.88 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 15873.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     616.74 ms /     1 runs   (  616.74 ms per token,     1.62 tokens per second)\nllama_print_timings:       total time =     616.85 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 22727.27 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     738.11 ms /     1 runs   (  738.11 ms per token,     1.35 tokens per second)\nllama_print_timings:       total time =     738.56 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      12.41 ms /   214 runs   (    0.06 ms per token, 17239.99 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  118466.57 ms /   214 runs   (  553.58 ms per token,     1.81 tokens per second)\nllama_print_timings:       total time =  118706.39 ms /   214 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      13.08 ms /   213 runs   (    0.06 ms per token, 16286.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  120326.60 ms /   213 runs   (  564.91 ms per token,     1.77 tokens per second)\nllama_print_timings:       total time =  120583.95 ms /   213 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      39.72 ms /   677 runs   (    0.06 ms per token, 17042.59 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  391892.75 ms /   677 runs   (  578.87 ms per token,     1.73 tokens per second)\nllama_print_timings:       total time =  393180.24 ms /   677 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8196.72 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     552.62 ms /     1 runs   (  552.62 ms per token,     1.81 tokens per second)\nllama_print_timings:       total time =     553.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     526.82 ms /     1 runs   (  526.82 ms per token,     1.90 tokens per second)\nllama_print_timings:       total time =     528.00 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11494.25 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     544.44 ms /     1 runs   (  544.44 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =     545.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     572.87 ms /     1 runs   (  572.87 ms per token,     1.75 tokens per second)\nllama_print_timings:       total time =     574.32 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      13.40 ms /   212 runs   (    0.06 ms per token, 15825.62 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  119720.43 ms /   212 runs   (  564.72 ms per token,     1.77 tokens per second)\nllama_print_timings:       total time =  119953.06 ms /   212 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      15.16 ms /   211 runs   (    0.07 ms per token, 13917.29 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  118470.61 ms /   211 runs   (  561.47 ms per token,     1.78 tokens per second)\nllama_print_timings:       total time =  118714.58 ms /   211 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23809.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     548.58 ms /     1 runs   (  548.58 ms per token,     1.82 tokens per second)\nllama_print_timings:       total time =     548.99 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23809.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     535.19 ms /     1 runs   (  535.19 ms per token,     1.87 tokens per second)\nllama_print_timings:       total time =     536.06 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     585.98 ms /     1 runs   (  585.98 ms per token,     1.71 tokens per second)\nllama_print_timings:       total time =     586.22 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     564.78 ms /     1 runs   (  564.78 ms per token,     1.77 tokens per second)\nllama_print_timings:       total time =     565.69 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =      12.14 ms /   210 runs   (    0.06 ms per token, 17291.07 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  114572.99 ms /   210 runs   (  545.59 ms per token,     1.83 tokens per second)\nllama_print_timings:       total time =  114806.35 ms /   210 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.58 ms /    96 runs   (    0.06 ms per token, 17201.22 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   51482.48 ms /    96 runs   (  536.28 ms per token,     1.86 tokens per second)\nllama_print_timings:       total time =   51577.30 ms /    96 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      40.25 ms /   676 runs   (    0.06 ms per token, 16796.28 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  384634.31 ms /   676 runs   (  568.99 ms per token,     1.76 tokens per second)\nllama_print_timings:       total time =  385906.74 ms /   676 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.25 ms /    94 runs   (    0.06 ms per token, 17901.35 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   50111.14 ms /    94 runs   (  533.10 ms per token,     1.88 tokens per second)\nllama_print_timings:       total time =   50196.17 ms /    94 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       4.88 ms /    93 runs   (    0.05 ms per token, 19076.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   48713.78 ms /    93 runs   (  523.80 ms per token,     1.91 tokens per second)\nllama_print_timings:       total time =   48787.20 ms /    93 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 19607.84 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     518.59 ms /     1 runs   (  518.59 ms per token,     1.93 tokens per second)\nllama_print_timings:       total time =     520.12 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     525.32 ms /     1 runs   (  525.32 ms per token,     1.90 tokens per second)\nllama_print_timings:       total time =     527.36 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 26315.79 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     525.45 ms /     1 runs   (  525.45 ms per token,     1.90 tokens per second)\nllama_print_timings:       total time =     525.76 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 27777.78 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     517.24 ms /     1 runs   (  517.24 ms per token,     1.93 tokens per second)\nllama_print_timings:       total time =     517.41 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       4.91 ms /    92 runs   (    0.05 ms per token, 18748.73 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   48576.61 ms /    92 runs   (  528.01 ms per token,     1.89 tokens per second)\nllama_print_timings:       total time =   48655.44 ms /    92 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.13 ms /    91 runs   (    0.06 ms per token, 17735.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   48005.21 ms /    91 runs   (  527.53 ms per token,     1.90 tokens per second)\nllama_print_timings:       total time =   48092.01 ms /    91 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.68 ms /    90 runs   (    0.06 ms per token, 15853.44 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   48438.81 ms /    90 runs   (  538.21 ms per token,     1.86 tokens per second)\nllama_print_timings:       total time =   48522.31 ms /    90 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.33 ms /    89 runs   (    0.06 ms per token, 16710.48 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   47680.50 ms /    89 runs   (  535.74 ms per token,     1.87 tokens per second)\nllama_print_timings:       total time =   47758.98 ms /    89 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     523.22 ms /     1 runs   (  523.22 ms per token,     1.91 tokens per second)\nllama_print_timings:       total time =     523.60 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     542.34 ms /     1 runs   (  542.34 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =     543.24 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     534.04 ms /     1 runs   (  534.04 ms per token,     1.87 tokens per second)\nllama_print_timings:       total time =     535.14 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     522.98 ms /     1 runs   (  522.98 ms per token,     1.91 tokens per second)\nllama_print_timings:       total time =     524.59 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.38 ms /    88 runs   (    0.06 ms per token, 16362.96 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   48619.46 ms /    88 runs   (  552.49 ms per token,     1.81 tokens per second)\nllama_print_timings:       total time =   48703.24 ms /    88 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.16 ms /    87 runs   (    0.06 ms per token, 16847.41 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   47391.86 ms /    87 runs   (  544.73 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =   47468.98 ms /    87 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      39.71 ms /   675 runs   (    0.06 ms per token, 16999.95 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  378731.65 ms /   675 runs   (  561.08 ms per token,     1.78 tokens per second)\nllama_print_timings:       total time =  379983.62 ms /   675 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.45 ms /    86 runs   (    0.06 ms per token, 15788.51 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   47030.07 ms /    86 runs   (  546.86 ms per token,     1.83 tokens per second)\nllama_print_timings:       total time =   47113.27 ms /    86 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.12 ms /    85 runs   (    0.06 ms per token, 16585.37 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   45811.46 ms /    85 runs   (  538.96 ms per token,     1.86 tokens per second)\nllama_print_timings:       total time =   45889.49 ms /    85 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10416.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     508.71 ms /     1 runs   (  508.71 ms per token,     1.97 tokens per second)\nllama_print_timings:       total time =     509.67 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 15151.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     531.53 ms /     1 runs   (  531.53 ms per token,     1.88 tokens per second)\nllama_print_timings:       total time =     532.15 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23809.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     534.17 ms /     1 runs   (  534.17 ms per token,     1.87 tokens per second)\nllama_print_timings:       total time =     534.36 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.04 ms /     1 runs   (    0.04 ms per token, 23255.81 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     510.74 ms /     1 runs   (  510.74 ms per token,     1.96 tokens per second)\nllama_print_timings:       total time =     511.55 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       4.81 ms /    84 runs   (    0.06 ms per token, 17459.99 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   45600.37 ms /    84 runs   (  542.86 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =   45679.94 ms /    84 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       4.70 ms /    83 runs   (    0.06 ms per token, 17663.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   45085.86 ms /    83 runs   (  543.20 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =   45157.16 ms /    83 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.06 ms /    82 runs   (    0.06 ms per token, 16208.74 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   44657.44 ms /    82 runs   (  544.60 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =   44735.88 ms /    82 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       4.91 ms /    81 runs   (    0.06 ms per token, 16493.59 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   44402.93 ms /    81 runs   (  548.18 ms per token,     1.82 tokens per second)\nllama_print_timings:       total time =   44477.28 ms /    81 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14285.71 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     525.63 ms /     1 runs   (  525.63 ms per token,     1.90 tokens per second)\nllama_print_timings:       total time =     526.60 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12048.19 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     539.73 ms /     1 runs   (  539.73 ms per token,     1.85 tokens per second)\nllama_print_timings:       total time =     540.36 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 13157.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     532.86 ms /     1 runs   (  532.86 ms per token,     1.88 tokens per second)\nllama_print_timings:       total time =     533.45 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8064.52 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     547.91 ms /     1 runs   (  547.91 ms per token,     1.83 tokens per second)\nllama_print_timings:       total time =     550.89 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       4.86 ms /    82 runs   (    0.06 ms per token, 16882.85 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   44461.78 ms /    82 runs   (  542.22 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =   44540.61 ms /    82 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       5.13 ms /    81 runs   (    0.06 ms per token, 15774.10 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   44258.84 ms /    81 runs   (  546.41 ms per token,     1.83 tokens per second)\nllama_print_timings:       total time =   44335.39 ms /    81 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       6.10 ms /    80 runs   (    0.08 ms per token, 13123.36 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   42873.56 ms /    80 runs   (  535.92 ms per token,     1.87 tokens per second)\nllama_print_timings:       total time =   42949.83 ms /    80 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1256.18 ms\nllama_print_timings:      sample time =      40.46 ms /   674 runs   (    0.06 ms per token, 16659.25 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =  381652.68 ms /   674 runs   (  566.25 ms per token,     1.77 tokens per second)\nllama_print_timings:       total time =  382911.99 ms /   674 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       4.83 ms /    79 runs   (    0.06 ms per token, 16352.72 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   42716.24 ms /    79 runs   (  540.71 ms per token,     1.85 tokens per second)\nllama_print_timings:       total time =   42785.88 ms /    79 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12195.12 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     500.60 ms /     1 runs   (  500.60 ms per token,     2.00 tokens per second)\nllama_print_timings:       total time =     502.73 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.06 ms /     1 runs   (    0.06 ms per token, 16666.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     544.37 ms /     1 runs   (  544.37 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =     545.02 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.05 ms /     1 runs   (    0.05 ms per token, 18867.92 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     504.25 ms /     1 runs   (  504.25 ms per token,     1.98 tokens per second)\nllama_print_timings:       total time =     505.17 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =     546.55 ms /     1 runs   (  546.55 ms per token,     1.83 tokens per second)\nllama_print_timings:       total time =     547.13 ms /     1 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       3.57 ms /    61 runs   (    0.06 ms per token, 17101.21 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   33097.03 ms /    61 runs   (  542.57 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =   33150.03 ms /    61 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       3.56 ms /    60 runs   (    0.06 ms per token, 16835.02 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   32539.63 ms /    60 runs   (  542.33 ms per token,     1.84 tokens per second)\nllama_print_timings:       total time =   32591.96 ms /    60 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     886.03 ms\nllama_print_timings:      sample time =       3.52 ms /    59 runs   (    0.06 ms per token, 16770.89 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\nllama_print_timings:        eval time =   32235.25 ms /    59 runs   (  546.36 ms per token,     1.83 tokens per second)\nllama_print_timings:       total time =   32290.54 ms /    59 tokens\nLlama.generate: prefix-match hit\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### output tested\n\n#### test1\n\n**Request**\n```\ncurl -X POST \"https://dadf-35-194-149-242.ngrok-free.app/compare\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Once upon a time\",\n    \"max_length\": 50\n  }'\n```\n**Response**\n```\n{\n  \"normal\": {\n    \"text\": \", a long, long time ago, in a small village nestled in the foothills of the majestic Himalayas, there lived a young girl named Mala. Mala was a curious and adventurous child, always eager\",\n    \"generation_time\": 37.465,\n    \"tokens_generated\": 34,\n    \"tokens_per_second\": 0.9075,\n    \"speedup_factor\": 1.0,\n    \"acceptance_rate\": null\n  },\n  \"medusa\": {\n    \"text\": \", in a far-off land, there was a kind and gentle woman who lived in a small village. She was known for her kindness, her wisdom, and her love of nature. She spent her days tending to her garden, helping her neighbors, and teaching the children of the village.\\nThe woman was deeply spiritual, and she spent many hours in prayer and meditation, seeking guidance from the divine. She believed that the natural world was a reflection of the divine, and that every plant, animal, and stone had a spirit that needed to be respected and honored.\\nOne day, as the woman was walking in the forest, she came\",\n    \"generation_time\": 249.927,\n    \"tokens_generated\": 136,\n    \"tokens_per_second\": 0.5442,\n    \"speedup_factor\": 0.5123,\n    \"acceptance_rate\": 68.0\n  },\n  \"speedup\": 0.5996\n}\n```\n![image.png](attachment:3afcc22c-1b33-42f7-86eb-a4183867ae82.png)\n\n\n\n#### Test 2\n**Request** \n```\ncurl -X POST \"https://dadf-35-194-149-242.ngrok-free.app/generate/normal\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Once upon a time\",\n    \"max_length\": 50\n  }'\n\n```\n**Response**\n```\n{\n  \"text\": \", a young girl named Lily was born in a small village. She lived a happy life with her parents and siblings, and she loved to help her mother in the kitchen. Lily' 123Movies Movies\",\n  \"generation_time\": 40.3927,\n  \"tokens_generated\": 35,\n  \"tokens_per_second\": 0.8665,\n  \"speedup_factor\": 1.0,\n  \"acceptance_rate\": null\n}\n```\n![image.png](attachment:4214feaf-bb85-43a5-bf1f-4b7d8c8a51f9.png)\n\n\n\n#### Test 3\n\n**Request**\n```\ncurl -X POST \"https://dadf-35-194-149-242.ngrok-free.app/generate/medusa\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Once upon a time\",\n    \"max_length\": 50\n  }'\n\n```\n\n**response**\n\n```\n{\n  \"text\": \", in a small village, there lived a kind, old man named John. John was loved by everyone in the village because he was always ready to help those in need. He had a large garden where he grew all kinds of fruits and vegetables. John was known for his delicious apple pies and fruit preserves. People from far and wide came to buy his produce and taste his pies and preserves.\\nOne day, a group of travelers passed through the village. They were on their way to a distant land and were hungry. They asked John if he had any food to sell. John told them that he had plenty of food, but he only sold it to people who could pay him in gold or silver coins. The travelers had no coins, but they\",\n  \"generation_time\": 263.546,\n  \"tokens_generated\": 163,\n  \"tokens_per_second\": 0.6185,\n  \"speedup_factor\": 0.5823,\n  \"acceptance_rate\": 81.5\n}\n```\n\n![image.png](attachment:60069889-eb48-4920-adce-bb585adbccb0.png)\n","metadata":{},"attachments":{"3afcc22c-1b33-42f7-86eb-a4183867ae82.png":{"image/png":"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"},"4214feaf-bb85-43a5-bf1f-4b7d8c8a51f9.png":{"image/png":"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"},"60069889-eb48-4920-adce-bb585adbccb0.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"### Testing stretegies documentation","metadata":{"id":"fskVoyMEFq7_"}},{"cell_type":"markdown","source":"Here are some curl commands to test FastAPI service with Medusa:\n\n### 1. Health Check\n\n```bash\ncurl -X GET \"https://f696-104-196-160-201.ngrok-free.app/health\"\n```\n\n### 2. Basic Text Generation\n\n```bash\ncurl -X POST \"https://f696-104-196-160-201.ngrok-free.app/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Once upon a time\",\n    \"max_length\": 100,\n    \"temperature\": 0.7,\n  }'\n```\n\n### 3. Benchmark Comparison (with and without Medusa)\n\n```bash\ncurl -X POST \"https://f696-104-196-160-201.ngrok-free.app/compare\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Once upon a time\",\n    \"max_length\": 50,\n    \"temperature\": 0.7,\n  }'\n```\n\n### 4. Testing with Different Parameters\n\n```bash\n# Lower temperature for more deterministic output\ncurl -X POST \"https://f696-104-196-160-201.ngrok-free.app/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Write a poem about\",\n    \"max_length\": 100,\n    \"temperature\": 0.3,\n  }'\n\n# Higher threshold for more speculative acceptance\ncurl -X POST \"https://f696-104-196-160-201.ngrok-free.app/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Explain quantum physics\",\n    \"max_length\": 150,\n    \"temperature\": 0.7,\n  }'\n```\n\n### 5. Stress Testing with a Complex Prompt\n\n```bash\ncurl -X POST \"https://798e-104-196-160-201.ngrok-free.app/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Write a detailed technical explanation of how speculative decoding works in large language models, including the advantages and potential challenges.\",\n    \"max_length\": 200,\n    \"temperature\": 0.7,\n  }'","metadata":{"id":"VG9rAEatFwBJ"}},{"cell_type":"code","source":"!ngrok authtoken 2rf1753VsPYXTOVl62iLwS2dITs_5XcrdEGFYarW57qMykxj6\n","metadata":{"id":"pn415ae9Tiap","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1c011ef-d1ff-45bb-c217-31387479951e","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:16:35.284883Z","iopub.execute_input":"2025-03-28T10:16:35.285235Z","iopub.status.idle":"2025-03-28T10:16:35.578244Z","shell.execute_reply.started":"2025-03-28T10:16:35.285204Z","shell.execute_reply":"2025-03-28T10:16:35.577145Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!ps aux | grep ngrok\n","metadata":{"id":"-WRhMXn5bmGh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kill -9 9675\n","metadata":{"id":"Q2RT2O8Qb2dY","trusted":true},"outputs":[],"execution_count":null}]}